\documentclass{ximera}
\input{../../../preamble.tex}

\author{Zack Reed}
%borrowed from chrichton ogle Linear
\title{Elementary Matrices: Row Reduction, Matrix Decomposition, and Finding the Inverse of a Matrix}
\begin{document}
\begin{abstract}



\end{abstract}
\maketitle

%THE MAIN IDEA IS THAT WE CONSTRUCT THE INVERSE OF A MATRIX BY USING ELEMENTARY MATRICES, AND SO THAT'S THE POINT OF REDUCED ROW ECHELON FORM AND STUFF. 
%ALSO SET UP AN RREF IN GEOGEBRA WITH "Reduced" function as part of it, just so they can 
%See the full process, then use rref in maltab and stuff for higher dimensions.
 
\section*{A return to elementary matrices}

As we saw in the final example of \href{https://ximera.osu.edu/appliedlinearalgebra/c4ChapterFour/learningActivities/m4LearningActivities/elementaryMatrices/elementaryMatrices}{the last section}, each of the elementary matrices have easily-discernible inverses. 

\begin{itemize}

  \item The \textbf{row swapping matrix} $E_{ij}$ can be inverted by returning the rows to their original positions. For a single row swap, this simply means re-swapping the already-swapped rows, so $E_{ij}$ is its own inverse. 
  
  $E_{12}$ is the matrix

  \begin{equation*}
    \begin{bmatrix}
      0 & 1 & 0 \\
      1 & 0 & 0 \\
      0 & 0 & 1
    \end{bmatrix}
  \end{equation*}.

  To return rows $1$ and $2$ to their original positions, we swap them again, so $E_{12}*E_{12}=I$.

  \item The \textbf{scaling matrix} $E_{i}(\lambda)$ is inverted by again scaling the same row, but by the multiplicative inverse of $\lambda$, aka $1/\lambda$. This un-does the original scaling to return space to its original scale.
  
  $E_{2}(3)$ would be the matrix

  \begin{equation*}
    \begin{bmatrix}
      1 & 0 & 0 \\
      0 & 3 & 0 \\
      0 & 0 & 1
    \end{bmatrix}.
  \end{equation*}

  $E_{2}(1/3)$ would be the inverse of $E_{2}(3)$, so $E_{2}(1/3)*E_{2}(3)=I$.

  \item The \textbf{row addition matrix} $E_{ij}(\lambda)$ is inverted by subtracting off the previously added row, returning row $i$ to its original state. So $E_{ij}(\lambda)^{-1}=E_{ij}(-\lambda)$.
  
  $E_{3,2}(2)$ performed on a $3\times 3$ matrix would be the matrix

  \begin{equation*}
    \begin{bmatrix}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 2 & 1
    \end{bmatrix}.
  \end{equation*}

  $E_{3,2}(-2)$,

  \begin{equation*}
    \begin{bmatrix}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 2 & 1
    \end{bmatrix}
  \end{equation*}

  would be the inverse.

\end{itemize}

\subsection*{Inverses of general matrices}

We focus on elementary matrices because of the inherent connection between elementary row operations and left-multiplication by elementary matrices. 

\begin{example}
Recall from our work with Gaussian Elimination that performing a row operation on a matrix is the same as left-multiplying by the associated elementary matrix.

If we start with the matrix $A=\begin{bmatrix}
  1&0&3\\0&2&1\\-1&1&0
\end{bmatrix}$, and want to add row $1$ to row $3$, we can perform this by left-multiplying $A$ by $E_{3,2}(1)$ to get

$E_{3,2}(1)*A=\begin{bmatrix}
1&0&3\\0&2&1\\\answer{0}&\answer{3}&\answer{4}
\end{bmatrix}$.

Let's continue in this fashion, as if we were enacting Gaussian Elimination, and see what happens.

Scaling row $2$ by $1/2$ is done by $E_{2}(1/2)$, so we get

$$E_{2}(1/2)*E_{3,2}(1)*A=\begin{bmatrix}
  1&0&3\\0&\answer{1}&\answer{1/2}\\\answer{0}&\answer{3}&\answer{4}
  \end{bmatrix}.$$

Subtracting $3$ times row $2$ from row $3$ is done by $E_{3,2}(-3)$, to get

$$E_{3,2}(-3)*E_{2}(1/2)*E_{3,2}(1)*A=\begin{bmatrix}
  1&0&3\\0&\answer{1}&\answer{1/2}\\\answer{0}&\answer{0}&\answer{1/2}
  \end{bmatrix}.$$

Doubling row $3$ is done by $E_{3}(2)$, which yields

$$E_{3}(2)*E_{3,2}(-3)*E_{2}(1/2)*E_{3,2}(1)*A=\begin{bmatrix}
  1&0&3\\0&\answer{1}&\answer{1/2}\\\answer{0}&\answer{0}&\answer{1}
  \end{bmatrix}.$$

Finally, if we subtract $1/2$ times row $3$ from row $2$, and subtract $3$ times row $3$ from row $1$, then we get

$$E_{1,3}(-3)*E_{2,3}(-1/2)*E_{3}(2)*E_{3,2}(-3)*E_{2}(1/2)*E_{3,2}(1)*A=\begin{bmatrix}
  1&0&\answer{0}\\0&\answer{1}&\answer{0}\\\answer{0}&\answer{0}&\answer{1}
  \end{bmatrix}.$$

Notice that on the right side, we have $I$, the identity matrix! So, the product $E_{1,3}(-3)*E_{2,3}(-1/2)*E_{3}(2)*E_{3,2}(-3)*E_{2}(1/2)*E_{3,2}(1)*A=I$, which means $E_{1,3}(-3)*E_{2,3}(-1/2)*E_{3}(2)*E_{3,2}(-3)*E_{2}(1/2)*E_{3,2}(1)$ is the inverse matrix of $A$!

Multiplying these elementary matrices together yeilds the more succinct form of $A^{-1}$.

$A^{-1}=\begin{bmatrix}
  \answer{-.2}&\answer{.6}&-1.2\\\answer{-.2}&\answer{.6}&\answer{-.2}\\.4&\answer{-.2}&\answer{.4}
\end{bmatrix}$ (Use decimals instead of fractions.)

\end{example}

In fact, the previous example gives us a more general process for finding inverses of general matrices, or even of determining whether matrices exist! 

The logic goes something like this: 

\begin{enumerate}
\item If a matrix $A$ is reducible to $I$ by a sequence of elementary row operations, then those row operations can be represented by the product of elementary matrices $\Pi_{k}E_k$ ($E_k$ denotes the elementary matrices and $\Pi$ is the symbol for mass multiplication much like $\Sigma$ is the symbol for mass summation)
\item Enacting row operations on $A$ is the same as left-multiplying $A$ by $\Pi_{k}E_k$, so $\Pi_{k}E_k*A=I$
\item This means that $\Pi_{k}E_k$ satisfies the definition of inverse for $A$, so $A^{-1}=\Pi_{k}E_k$
\end{enumerate}

We actually get a stronger result, that if a matrix $A$ is \emph{not} reducible to $I$ by elementary row operations, it is not invertible. These are summarized in the following theorem:

\begin{theorem}\name{Inverses: Products of elementary matrices}\label{th:prod-elementary}
  A matrix $A$ is invertible if and
  only if it is reducible to $I$ by elementary row operations.

  If this is possible, and $\Pi_kE_k$ is the product of the corresponding elementary matrices, then $A^{-1}=\Pi_kE_k$.
\end{theorem}

This also gives us a quick way of determining the inverse matrix itself, using row operations.

\begin{remark}
  If we use the fact that inverses multiply to the identity, then we get the equation $A*A^{-1}=I$. If we then multiply both sides of the equation by $\Pi_kE_k$, we get $\Pi_kE_k*A*A^{-1}=\Pi_kE_k$, and so $A^{-1}=\Pi_kE_k$. This is more than just a simple re-stating of the definition $A^{-1}=\Pi_kE_k$, it also gives us a way to use row operations to explicitly solve for $A^{-1}$. 
  
  Recall from Gaussian Eliminiation that we solved for the solution $\vec{x}$ to $A\vec{x}=\vec{b}$ by performing row operations on the augmented matrix $A|\vec{b}$ (add on $\vec{b}$ as a final column of $A$) until we got the identity so that the final augmented matrix $I|\vec{x}$ was a solution to the system (if it existed).

  We will perform the same process to find $A^{-1}$, starting from the augmented matrix $A|I$. Each row operation performed on the left side of the augmented matrix (reducing $A$ to the identity) will also be applied on the right side, which will progressively build $A^{-1}$ by left multiplying elementary matrices to $I$.
\end{remark}

This gives us the following algorithm. 

\begin{theorem}
  The following algorithm will determine whether a matrix $A$ is invertible, and if it is then it will also yield the inverse matrix $A^{-1}$.
  \begin{enumerate}
    \item Augment $A$ with the identity matrix, $A|I$.
    \item Find the \texttt{rref} of the augmented matrix. The left side of the augmented matrix will be $\texttt{rref(A)}$.
    \item If $\texttt{rref(A)}=I$, then the right side of the final augmented matrix is $A^{-1}$, if $\texttt{rref(A)}\neq I$, then $A$ is not invertible.
  \end{enumerate}
\end{theorem}

START HERE!

\begin{example}{Product of elementary matrices}{prod-elementary}
  Let $A = \startmat{rrr}
    0 &  1 & 0 \\
    1 &  1 & 0 \\
    0 & -2 & 1
  \stopmat$.
  Write $A$ as a product of elementary matrices.
\end{example}

\begin{solution}
  Following the process of Theorem~\ref{thm:prod-elementary}, we first
  row-reduce $A$ to its {\rref}, recording each row operation as an
  elementary matrix.
  \begin{equation*}
    \startmat{rrr}
      0 & 1 & 0 \\
      1 & 1 & 0 \\
      0 & -2 & 1 \\
    \stopmat
    \quad\stackrel{R_1\rowswap R_2}{\roweq}\quad
    \startmat{rrr}
      1 & 1 & 0 \\
      0 & 1 & 0 \\
      0 & -2 & 1 \\
    \stopmat
    \quad\mbox{with elementary matrix}\quad
    E_1 ~=~ \startmat{rrr}
      0 & 1 & 0 \\
      1 & 0 & 0 \\
      0 & 0 & 1
    \stopmat,
  \end{equation*}
  \begin{equation*}
    \startmat{rrr}
      1 & 1 & 0 \\
      0 & 1 & 0 \\
      0 & -2 & 1 \\
    \stopmat
    \quad\stackrel{R_1\rowop R_1-R_2}{\roweq}\quad
    \startmat{rrr}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & -2 & 1 \\
    \stopmat
    \quad\mbox{with elementary matrix}\quad
    E_2 ~=~  \startmat{rrr}
      1 & -1 & 0 \\
      0 & 1 & 0 \\
      0 & 0 & 1
    \stopmat,
  \end{equation*}
  \begin{equation*}
    \startmat{rrr}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & -2 & 1 \\
    \stopmat
    \quad\stackrel{R_3\rowop R_3+2R_2}{\roweq}\quad
    \startmat{rrr}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 0 & 1 \\
    \stopmat
    \quad\mbox{with elementary matrix}\quad
    E_3 ~=~ \startmat{rrr}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 2 & 1 \\
    \stopmat.
  \end{equation*}
  Notice that the {\rref} of $A$ is $I$. Hence $I = UA$ where
  $U=E_3E_2E_1$. It follows that
  $A = U^{-1} = E_1^{-1}E_2^{-1}E_3^{-1}$, and so we have succeeded in
  writing $A$ as a product of elementary matrices
  \begin{equation*}
    A
    ~=~ E_1^{-1}E_2^{-1}E_3^{-1}
    ~=~
    \startmat{rrr}
      0 & 1 & 0 \\
      1 & 0 & 0 \\
      0 & 0 & 1
    \stopmat
    \startmat{rrr}
      1 & 1 & 0 \\
      0 & 1 & 0 \\
      0 & 0 & 1
    \stopmat
    \startmat{rrr}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & -2 & 1
    \stopmat.
  \end{equation*}
  In particular, it follows that $A$ is invertible.
\end{solution}

\subsection*{Elementary matrices and row operations}

Recall from Definition~\ref{def:row-operations} that there are three
kinds of elementary row operations%
\index{matrix!row operation}%
\index{matrix!elementary row operation}%
\index{row operation}%
\index{elementary row operation} on matrices:
\begin{enumerate}
\item Switch two rows.
\item Multiply a row by a non-zero number.
\item Add a multiple of one row to another row.
\end{enumerate}
The purpose of this section is to show that each of these row
operations corresponds to a special type of invertible matrix called
an \textbf{elementary matrix}%
\index{elementary matrix}%
\index{matrix!elementary matrix}.

\begin{example}{Elementary matrix for switching two rows}{elementary-matrix-1}
  Let
  \begin{equation*}
    E ~=~ \startmat{lll}
      1 & 0 & 0 \\
      0 & 0 & 1 \\
      0 & 1 & 0 \\
    \stopmat.
  \end{equation*}
  What is the effect of multiplying $E$ by an arbitrary $3\times
  n$-matrix $A$?
\end{example}

\begin{solution}
  Consider an arbitrary $3\times n$-matrix
  \begin{equation*}
    A ~=~ \startmat{cccc}
      a_{11} & a_{12} & \cdots & a_{1n} \\
      a_{21} & a_{22} & \cdots & a_{2n} \\
      a_{31} & a_{32} & \cdots & a_{3n} \\
    \stopmat.
  \end{equation*}
  We compute the product $EA$ by the row method:
  \begin{equation*}
    EA ~=~ \startmat{lll}
      1 & 0 & 0 \\
      0 & 0 & 1 \\
      0 & 1 & 0 \\
    \stopmat
    \startmat{cccc}
      a_{11} & a_{12} & \cdots & a_{1n} \\
      a_{21} & a_{22} & \cdots & a_{2n} \\
      a_{31} & a_{32} & \cdots & a_{3n} \\
    \stopmat
    ~=~
    \startmat{cccc}
      a_{11} & a_{12} & \cdots & a_{1n} \\
      a_{31} & a_{32} & \cdots & a_{3n} \\
      a_{21} & a_{22} & \cdots & a_{2n} \\
    \stopmat.
  \end{equation*}
  So the effect of multiplying $A$ by $E$ on the left is exactly the
  same as switching rows 2 and 3. We say that $E$ is the
  \textbf{elementary matrix for switching rows 2 and 3}.
\end{solution}

\begin{example}{Elementary matrix for multiplying a row by a non-zero number}{elementary-matrix-2}
  Let
  \begin{equation*}
    E ~=~ \startmat{lll}
      1 & 0 & 0 \\
      0 & k & 0 \\
      0 & 0 & 1 \\
    \stopmat.
  \end{equation*}
  What is the effect of multiplying $E$ by an arbitrary $3\times
  n$-matrix $A$?
\end{example}

\begin{solution}
  We compute the product $EA$ by the row method:
  \begin{equation*}
    EA ~=~ \startmat{lll}
      1 & 0 & 0 \\
      0 & k & 0 \\
      0 & 0 & 1 \\
    \stopmat
    \startmat{cccc}
      a_{11} & a_{12} & \cdots & a_{1n} \\
      a_{21} & a_{22} & \cdots & a_{2n} \\
      a_{31} & a_{32} & \cdots & a_{3n} \\
    \stopmat
    ~=~
    \startmat{rrrr}
      a_{11} & a_{12} & \cdots & a_{1n} \\
      ka_{21} & ka_{22} & \cdots & ka_{2n} \\
      a_{31} & a_{32} & \cdots & a_{3n} \\
    \stopmat.
  \end{equation*}
  So the effect of multiplying $A$ by $E$ on the left is exactly the
  same as multiplying row 2 by the scalar $k$. We say that $E$ is the
  \textbf{elementary matrix for multiplying row 2 by $k$}.
\end{solution}

\begin{example}{Elementary matrix for adding a multiple of one row to another row}{elementary-matrix-3}
  Let
  \begin{equation*}
    E ~=~ \startmat{lll}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & k & 1 \\
    \stopmat.
  \end{equation*}
  What is the effect of multiplying $E$ by an arbitrary $3\times
  n$-matrix $A$?
\end{example}

\begin{solution}
  Once again we compute the product $EA$:
  \begin{equation*}
    EA ~=~ \startmat{lll}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & k & 1 \\
    \stopmat
    \startmat{cccc}
      a_{11} & a_{12} & \cdots & a_{1n} \\
      a_{21} & a_{22} & \cdots & a_{2n} \\
      a_{31} & a_{32} & \cdots & a_{3n} \\
    \stopmat
    ~=~
    \startmat{cccc}
      a_{11} & a_{12} & \cdots & a_{1n} \\
      a_{21} & a_{22} & \cdots & a_{2n} \\
      a_{31}+ka_{21} & a_{32}+ka_{22} & \cdots & a_{3n}+ka_{2n} \\
    \stopmat.
  \end{equation*}
  So the effect of multiplying $A$ by $E$ on the left is exactly the
  same as adding $k$ times row 2 to row 3. We say that $E$ is the
  \textbf{elementary matrix for adding $k$ times row 2 to row 3}.
\end{solution}

As these examples show, performing each type of elementary row
operation is the same as multiplying (on the left) by a certain
invertible matrix. Thesse matrices are called the \textbf{elementary
  matrices}%
\index{elementary matrix}%
\index{matrix!elementary matrix}. In the
above examples, we have only considered $3\times 3$-elementary
matrices, but they exist for other sizes too. The following definition
makes this precise. It also shows how to calculate the elementary
matrix corresponding to any elementary row operation.

\begin{definition}{Elementary matrices and row operations}{elementary-matrices-and-row-operations}
  Let $E$ be an $n\times n$-matrix. Then $E$ is an \textbf{elementary
    matrix}%
  \index{elementary matrix}%
  \index{matrix!elementary matrix}
  if it is the result of applying one elementary row operation to the
  $n\times n$ identity matrix.
\end{definition}

\begin{example}{Finding an elementary matrix}{finding-elementary-matrix}
  Consider the elementary row operation of adding $5$ times row 3 to
  row 1 of a $4\times n$-matrix. Find the elementary matrix $E$
  corresponding to this row operation.
\end{example}

\begin{solution}
  Following Definition~\ref{def:elementary-matrices-and-row-operations}, all
  we have to do is apply the desired row operation to the
  $4\times 4$-identity matrix:
  \begin{equation*}
    \startmat{rrrr}
      1 & 0 & 0 & 0 \\
      0 & 1 & 0 & 0 \\
      0 & 0 & 1 & 0 \\
      0 & 0 & 0 & 1 \\
    \stopmat
    \quad
    \stackrel{R_1\rowop R_1+5R_3}{\roweq}
    \quad
    \startmat{rrrr}
      1 & 0 & 5 & 0 \\
      0 & 1 & 0 & 0 \\
      0 & 0 & 1 & 0 \\
      0 & 0 & 0 & 1 \\
    \stopmat
    ~=~ E.
  \end{equation*}
\end{solution}

We can double-check that multiplying $E$ by any $4\times n$-matrix
does indeed have the desired effect:
\begin{equation*}
  \startmat{rrrr}
    1 & 0 & 5 & 0 \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1 \\
  \stopmat
  \startmat{cccc}
    a_{11} & a_{12} & \cdots & a_{1n} \\
    a_{21} & a_{22} & \cdots & a_{2n} \\
    a_{31} & a_{32} & \cdots & a_{3n} \\
    a_{41} & a_{42} & \cdots & a_{4n} \\
  \stopmat
  ~=~
  \startmat{cccc}
    a_{11}+5a_{31} & a_{12}+5a_{32} & \cdots & a_{1n}+5a_{3n} \\
    a_{21} & a_{22} & \cdots & a_{2n} \\
    a_{31} & a_{32} & \cdots & a_{3n} \\
    a_{41} & a_{42} & \cdots & a_{4n} \\
  \stopmat.
\end{equation*}
The fact that this always works is the content of the following
theorem.

\begin{theorem}{Multiplication by an elementary matrix and row operations}{multiplication-by-elementary-matrix}
  Performing any of the three elementary row operations on a matrix $A$ is the
  same as taking the product $EA$, where $E$ is the elementary matrix
  obtained by applying the desired row operation to the identity
  matrix.
\end{theorem}



\end{document}