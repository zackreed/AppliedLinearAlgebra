\documentclass{ximera}
\input{../../../preamble.tex}

\author{Zack Reed}
%borrowed from chrichton ogle Linear
\title{Elementary Matrices: Row Reduction, Matrix Decomposition, and Finding the Inverse of a Matrix}
\begin{document}
\begin{abstract}



\end{abstract}
\maketitle

%THE MAIN IDEA IS THAT WE CONSTRUCT THE INVERSE OF A MATRIX BY USING ELEMENTARY MATRICES, AND SO THAT'S THE POINT OF REDUCED ROW ECHELON FORM AND STUFF. 
%ALSO SET UP AN RREF IN GEOGEBRA WITH "Reduced" function as part of it, just so they can 
%See the full process, then use rref in maltab and stuff for higher dimensions.
 
\section*{A return to elementary matrices}

As we saw in the final example of \href{https://ximera.osu.edu/appliedlinearalgebra/c4ChapterFour/learningActivities/m4LearningActivities/elementaryMatrices/elementaryMatrices}{the last section}, each of the elementary matrices have easily-discernible inverses. 

\begin{itemize}

  \item The \textbf{row swapping matrix} $E_{ij}$ can be inverted by returning the rows to their original positions. For a single row swap, this simply means re-swapping the already-swapped rows, so $E_{ij}$ is its own inverse. 
  
  $E_{12}$ is the matrix

  \begin{equation*}
    \begin{bmatrix}
      0 & 1 & 0 \\
      1 & 0 & 0 \\
      0 & 0 & 1
    \end{bmatrix}
  \end{equation*}.

  To return rows $1$ and $2$ to their original positions, we swap them again, so $E_{12}*E_{12}=I$.

  \item The \textbf{scaling matrix} $E_{i}(\lambda)$ is inverted by again scaling the same row, but by the multiplicative inverse of $\lambda$, aka $1/\lambda$. This un-does the original scaling to return space to its original scale.
  
  $E_{2}(3)$ would be the matrix

  \begin{equation*}
    \begin{bmatrix}
      1 & 0 & 0 \\
      0 & 3 & 0 \\
      0 & 0 & 1
    \end{bmatrix}.
  \end{equation*}

  $E_{2}(1/3)$ would be the inverse of $E_{2}(3)$, so $E_{2}(1/3)*E_{2}(3)=I$.

  \item The \textbf{row addition matrix} $E_{ij}(\lambda)$ is inverted by subtracting off the previously added row, returning row $i$ to its original state. So $E_{ij}(\lambda)^{-1}=E_{ij}(-\lambda)$.
  
  $E_{3,2}(2)$ performed on a $3\times 3$ matrix would be the matrix

  \begin{equation*}
    \begin{bmatrix}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 2 & 1
    \end{bmatrix}.
  \end{equation*}

  $E_{3,2}(-2)$,

  \begin{equation*}
    \begin{bmatrix}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 2 & 1
    \end{bmatrix}
  \end{equation*}

  would be the inverse.

\end{itemize}

\subsection*{Inverses of general matrices}

We focus on elementary matrices because of the inherent connection between elementary row operations and left-multiplication by elementary matrices. 

\begin{example}
Recall from our work with Gaussian Elimination that performing a row operation on a matrix is the same as left-multiplying by the associated elementary matrix.

If we start with the matrix $A=\begin{bmatrix}
  1&0&3\\0&2&1\\-1&1&0
\end{bmatrix}$, and want to add row $1$ to row $3$, we can perform this by left-multiplying $A$ by $E_{3,2}(1)$ to get

$E_{3,2}(1)*A=\begin{bmatrix}
1&0&3\\0&2&1\\\answer{0}&\answer{3}&\answer{4}
\end{bmatrix}$.

Let's continue in this fashion, as if we were enacting Gaussian Elimination, and see what happens.

Scaling row $2$ by $1/2$ is done by $E_{2}(1/2)$, so we get

$$E_{2}(1/2)*E_{3,2}(1)*A=\begin{bmatrix}
  1&0&3\\0&\answer{1}&\answer{1/2}\\\answer{0}&\answer{3}&\answer{4}
  \end{bmatrix}.$$

Subtracting $3$ times row $2$ from row $3$ is done by $E_{3,2}(-3)$, to get

$$E_{3,2}(-3)*E_{2}(1/2)*E_{3,2}(1)*A=\begin{bmatrix}
  1&0&3\\0&\answer{1}&\answer{1/2}\\\answer{0}&\answer{0}&\answer{1/2}
  \end{bmatrix}.$$

Doubling row $3$ is done by $E_{3}(2)$, which yields

$$E_{3}(2)*E_{3,2}(-3)*E_{2}(1/2)*E_{3,2}(1)*A=\begin{bmatrix}
  1&0&3\\0&\answer{1}&\answer{1/2}\\\answer{0}&\answer{0}&\answer{1}
  \end{bmatrix}.$$

Finally, if we subtract $1/2$ times row $3$ from row $2$, and subtract $3$ times row $3$ from row $1$, then we get

$$E_{1,3}(-3)*E_{2,3}(-1/2)*E_{3}(2)*E_{3,2}(-3)*E_{2}(1/2)*E_{3,2}(1)*A=\begin{bmatrix}
  1&0&\answer{0}\\0&\answer{1}&\answer{0}\\\answer{0}&\answer{0}&\answer{1}
  \end{bmatrix}.$$

Notice that on the right side, we have $I$, the identity matrix! So, the product $E_{1,3}(-3)*E_{2,3}(-1/2)*E_{3}(2)*E_{3,2}(-3)*E_{2}(1/2)*E_{3,2}(1)*A=I$, which means $E_{1,3}(-3)*E_{2,3}(-1/2)*E_{3}(2)*E_{3,2}(-3)*E_{2}(1/2)*E_{3,2}(1)$ is the inverse matrix of $A$!

Multiplying these elementary matrices together yeilds the more succinct form of $A^{-1}$.

$A^{-1}=\begin{bmatrix}
  \answer{-.2}&\answer{.6}&-1.2\\\answer{-.2}&\answer{.6}&\answer{-.2}\\.4&\answer{-.2}&\answer{.4}
\end{bmatrix}$ (Use decimals instead of fractions.)

\end{example}

In fact, the previous example gives us a more general process for finding inverses of general matrices, or even of determining whether matrices exist! 

The logic goes something like this: 

\begin{enumerate}
\item If a matrix $A$ is reducible to $I$ by a sequence of elementary row operations, then those row operations can be represented by the product of elementary matrices $\Pi_{k}E_k$ ($E_k$ denotes the elementary matrices and $\Pi$ is the symbol for mass multiplication much like $\Sigma$ is the symbol for mass summation)
\item Enacting row operations on $A$ is the same as left-multiplying $A$ by $\Pi_{k}E_k$, so $\Pi_{k}E_k*A=I$
\item This means that $\Pi_{k}E_k$ satisfies the definition of inverse for $A$, so $A^{-1}=\Pi_{k}E_k$
\end{enumerate}

We actually get a stronger result, that if a matrix $A$ is \emph{not} reducible to $I$ by elementary row operations, it is not invertible. These are summarized in the following theorem:

\begin{theorem}\name{Inverses: Products of elementary matrices}\label{th:prod-elementary}
  A matrix $A$ is invertible if and
  only if it is reducible to $I$ by elementary row operations.

  If this is possible, and $\Pi_kE_k$ is the product of the corresponding elementary matrices, then $A^{-1}=\Pi_kE_k$.
\end{theorem}

This also gives us a quick way of determining the inverse matrix itself, using row operations.

\begin{remark}
  If we use the fact that inverses multiply to the identity, then we get the equation $A*A^{-1}=I$. If we then multiply both sides of the equation by $\Pi_kE_k$, we get $\Pi_kE_k*A*A^{-1}=\Pi_kE_k$, and so $A^{-1}=\Pi_kE_k$. This is more than just a simple re-stating of the definition $A^{-1}=\Pi_kE_k$, it also gives us a way to use row operations to explicitly solve for $A^{-1}$. 
  
  Recall from Gaussian Eliminiation that we solved for the solution $\vec{x}$ to $A\vec{x}=\vec{b}$ by performing row operations on the augmented matrix $A|\vec{b}$ (add on $\vec{b}$ as a final column of $A$) until we got the identity so that the final augmented matrix $I|\vec{x}$ was a solution to the system (if it existed).

  We will perform the same process to find $A^{-1}$, starting from the augmented matrix $A|I$. Each row operation performed on the left side of the augmented matrix (reducing $A$ to the identity) will also be applied on the right side, which will progressively build $A^{-1}$ by left multiplying elementary matrices to $I$.
\end{remark}

This gives us the following algorithm. 

\begin{theorem}\label{thm:prod-elementary}
  The following algorithm will determine whether a matrix $A$ is invertible, and if it is then it will also yield the inverse matrix $A^{-1}$.
  \begin{enumerate}
    \item Augment $A$ with the identity matrix, $A|I$.
    \item Find the \texttt{rref} of the augmented matrix. The left side of the augmented matrix will be $\texttt{rref(A)}$.
    \item If $\texttt{rref(A)}=I$, then the right side of the final augmented matrix is $A^{-1}$, if $\texttt{rref(A)}\neq I$, then $A$ is not invertible.
  \end{enumerate}
\end{theorem}

Let's try this process a few times and verify that this gives us inverse matrices! We'll also verify that if we don't get the identity matrix from row operations, then the matrix $A$ is not invertible.

\begin{example}\name{Product of elementary matrices}\label{ex:prod-elementary}

  Let $A = \startmat{rrr}
    0 &  1 & 0 \\
    1 &  1 & 0 \\
    0 & -2 & 1
  \stopmat$.
  Write $A^{-1}$ as a product of elementary matrices.
\end{example}

\begin{solution}
  Following the process of Theorem~\ref{thm:prod-elementary}, we first
  row-reduce $A$ to its {\rref}, recording each row operation as an
  elementary matrix.
  \begin{equation*}
    \startmat{rrr}
      0 & 1 & 0 \\
      1 & 1 & 0 \\
      0 & -2 & 1 \\
    \stopmat
    \quad\stackrel{E_{12}}{\rightarrow}\quad
    \startmat{rrr}
      1 & 1 & 0 \\
      0 & 1 & 0 \\
      0 & -2 & 1 \\
    \stopmat,
  \end{equation*}
  \begin{equation*}
    \startmat{rrr}
      1 & 1 & 0 \\
      0 & 1 & 0 \\
      0 & -2 & 1 \\
    \stopmat
    \quad\stackrel{E_{12}(-1)}{\rightarrow}\quad
    \startmat{rrr}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & -2 & 1 \\
    \stopmat,
  \end{equation*}
  \begin{equation*}
    \startmat{rrr}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & -2 & 1 \\
    \stopmat
    \quad\stackrel{E_{32}(2)}{\roweq}\quad
    \startmat{rrr}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 0 & 1 \\
    \stopmat.
  \end{equation*}
  Notice that the {\rref} of $A$ is $I$. Hence $E_{32}(2)*E_{12}(-1)*E_{12}*A=I$ where
  $U=E_3E_2E_1$. It follows that
  $A^{-1} = E_{32}(2)*E_{12}(-1)*E_{12}$, and so we have succeeded in
  writing $A^{-1}$ as a product of elementary matrices
  \begin{equation*}
    A^{-1}
    ~=~ E_{32}(2)*E_{12}(-1)*E_{12}
    ~=~
    \startmat{rrr}
      \answer{-1}&\answer{1}&0\\
      1&\answer{0}&\answer{0}\\
      \answer{2}&\answer{0}&\answer{1}
    \stopmat.
  \end{equation*}
\end{solution}



\begin{exploration}
  For the following matrices $A_i$, use $rref(A_i|I)$ to determine whether they are invertible. In each case, verify the invertibility (or lack of invertibility) by the product $M*A$, where $M$ is the right side of $rref(A_i|I)$.

  \begin{enumerate}
  \item $A_1=
    \begin{bmatrix} 
      2 & 1 & 3 \\ 
      0 & 1 & 4 \\ 
      5 & 6 & 0 
      \end{bmatrix}$

  \item $A_2=\begin{bmatrix} 
    1 & 2 & 3 \\ 
    4 & 5 & 6 \\ 
    7 & 8 & 9 
    \end{bmatrix}$
  \item $A_3=\begin{bmatrix} 
    2 & 5 \\ 
    3 & 1 \\ 
    4 & 0 \\ 
    -1 & 7 \end{bmatrix}$
  \end{enumerate}
  
  \begin{enumerate}
    \item The right side of $rref(A_1|I)$ is the $3x3$ matrix $M = \begin{bmatrix} 
      \answer{0.5581} & \answer{-0.4186} & \answer{-0.0233} \\ 
      \answer{-0.4651} & \answer{0.3488} & \answer{0.1860} \\ 
      \answer{0.1163} & \answer{0.1628} & \answer{-0.0465} 
      \end{bmatrix}$ (rounded to four decimals).

    The product $M*A_1=   \startmat{rrr}
    \answer{1} & \answer{0} & \answer{0} \\ 
    \answer{0} & \answer{1} & \answer{0} \\ 
    \answer{0} & \answer{0} & \answer{1} 
  \stopmat$. Hence, $A$  \wordChoice{
    \choice[correct]{is}
    \choice{is not}
    } invertible.
    \item The right side of $rref(A_2|I)$ is the $3x3$ matrix $M = \begin{bmatrix} 
      \answer{0} & \answer{-2.6667} & \answer{1.6667} \\ 
      \answer{0} & \answer{2.3333} & \answer{-1.3333} \\ 
      \answer{1} & \answer{-2} & \answer{1} 
      \end{bmatrix}$ (rounded to four decimals).
      
    The product $M*A_2=   \startmat{rrr}
    \answer{1} & \answer{0} & \answer{-1} \\ 
    \answer{0} & \answer{1} & \answer{2} \\ 
    \answer{0} & \answer{0} & \answer{0} 
  \stopmat$. Hence, $A$  \wordChoice{
    \choice{is}
    \choice[correct]{is not}
    } invertible.
    \item $A_3$ becomes more tricky. Because of the dimension of $A_3$, we need to agument $A_3$ with a $4x4$ identity matrix even though there are only two vectors in the columns of $A_3$. 
    
    Doing so and then taking $rref(A_3|I)$ gives $M = \begin{bmatrix} 
      0 & 0 & 0.2500 & 0 \\ 
      0 & 0 & 0.0357 & 0.1429 \\ 
      1 & 0 & -0.6786 & -0.7143 \\ 
      0 & 1 & -0.7857 & -0.1429 
      \end{bmatrix}$ (rounded to four decimals).
      
    The product $M*A_3=   \startmat{rrr}
    \answer{1} & \answer{0}  \\ 
    \answer{0} & \answer{1}  \\ 
    \answer{0} & \answer{0} \\
    \answer{0} & \answer{0}
  \stopmat$, which is not quite the identity matrix. Hence, $A$  \wordChoice{
    \choice{is}
    \choice[correct]{is not}
    } invertible.
  \end{enumerate}

\end{exploration}

\begin{remark}
  The example $A_3$ speaks of a more general fact. Because we require that both $A*A^{-1}=I$ and $A^{-1}*A=I$, this can only be satisfied if $A$ and $A^{-1}$ are square matrices, since the dimensions of matrix multiplication will not allow both left and right multiplication if $A$ and $A^{-1}$ are not square. 

  This leads to a notion of \emph{left} and \emph{right} invertibility, which we will not dwell on beyond this point.
\end{remark}

\begin{definition}\name{Left and Right Invertibility}
  A matrix $A$ is left invertible if there is a matrix $M$ such that $M*A=I$, and is right invertible if there is a matrix $N$ such that $A*N=I$. 
  
  A matrix is invertible if and only if it is both left and right invertible and $M=N$. This is only possible if $A$ is square.
\end{definition}

\begin{remark}
  You also might have noticed that in both examples $A_1$ and $A_2$, the products $M*A_1$ and $M*A_2$ were equal to the left side of the reduced row echelon augmented matrix. 

  This is a necessary consequence of how we built the matrices $M$ by row operations on the identity matrix, and the connections between augmented matrices and matrix equality, however it should be stated nonetheless and these connections are not always obvious by a first glance.
\end{remark}

We'll now continue to find new ways of determining whether matrices are invertible, and will find new ways of calculating the inverses of matrices.

\end{document}