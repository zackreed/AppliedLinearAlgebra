\documentclass{ximera}
\input{../../../preamble.tex}

\author{Zack Reed}
%borrowed from selinger linear algebra
\title{Inverses}
\begin{document}
\begin{abstract}

    In this learning activity, you will be 
\end{abstract}
\maketitle
 
\section*{Composition and Inverses of Linear Transformations}
 
\subsection*{Composition of Linear Transformations}
\begin{definition}\label{def:compoflintrans} Let $U$, $V$ and $W$ be vector spaces, and let $T:U\rightarrow V$ and $S:V\rightarrow W$ be linear transformations.  The \dfn{composition} of $S$ and $T$ is the transformation $S\circ T:U\rightarrow W$ given by
$$(S\circ T)(\vec{u})=S(T(\vec{u}))$$
\end{definition}
 
\begin{example}\label{ex:transcomp} Define $$T:\RR^2\rightarrow \RR^2 \quad\text{by}\quad T\left(\begin{bmatrix}u_1\\u_2\end{bmatrix}\right)=\begin{bmatrix}u_1+u_2\\3u_1+3u_2\end{bmatrix}$$
 
$$S:\RR^2\rightarrow \RR^2 \quad\text{by}\quad S\left(\begin{bmatrix}v_1\\v_2\end{bmatrix}\right)=\begin{bmatrix}3v_1-v_2\\-3v_1+v_2\end{bmatrix}$$
(You should be able to verify that both transformations are linear.)  Examine the effect of $S\circ T$ on vectors of $\RR^2$.
\begin{explanation}
From the computational standpoint, the situation is simple.
\begin{align*}
(S\circ T)\left(\begin{bmatrix}u_1\\u_2\end{bmatrix}\right)&=S\left(T\left(\begin{bmatrix}u_1\\u_2\end{bmatrix}\right)\right)=S\left(\begin{bmatrix}u_1+u_2\\3u_1+3u_2\end{bmatrix}\right)\\
&=\begin{bmatrix}3(u_1+u_2)-(3u_1+3u_2)\\-3(u_1+u_2)+(3u_1+3u_2)\end{bmatrix}\\
&=\vec{0}
\end{align*}
This means that $S\circ T$ maps all vectors of $\RR^2$ to $\vec{0}$. 
 
In addition to the computational approach, it is also useful to visualize what happens geometrically.
 
First, observe that $$T\left(\begin{bmatrix}u_1\\u_2\end{bmatrix}\right)=\begin{bmatrix}u_1+u_2\\3u_1+3u_2\end{bmatrix}=(u_1+u_2)\begin{bmatrix}1\\3\end{bmatrix}$$  Therefore the image of any vector of $\RR^2$ under $T$ lies on the line determined by the vector $\begin{bmatrix}1\\3\end{bmatrix}$. 
 
Even though $S$ is defined on all of $\RR^2$, we are only interested in the action of $S$ on vectors along the line determined by $\begin{bmatrix}1\\3\end{bmatrix}$.  Our computations showed that all such vectors map to $\vec{0}$.
 
The actions of individual transformations, as well as the composite transformation are shown below.
 
\begin{center}
\begin{tikzpicture}[scale=0.7]
 
\fill[blue!40] (-1,-1) rectangle (3,3);
\draw[thin,gray!40] (-1,-1) grid (3,3);
  \draw[<->] (-1,0)--(3,0);
  \draw[<->] (0,-1)--(0,3);
 
  \draw[thin,gray!40] (4,-1) grid (8,3);
  \draw[<->] (4,0)--(8,0);
  \draw[<->] (5,-1)--(5,3);
  \draw[line width=2pt,blue!40](14/3,-1)--(6,3) node[anchor=south west]{};
   
  \draw[thin,gray!40] (9,-1) grid (13,3);
  \draw[<->] (9,0)--(13,0);
  \draw[<->] (10,-1)--(10,3);
   
  \fill[blue!40!white] (10,0) circle (0.2cm);
  \draw [->,line width=1pt,-stealth]  (2,1)to[out=60, in=120](4.5, 1);
   \node[] at (3.4, 2)   (b) {$T$};
\draw [->,line width=1pt,-stealth] (7,1)to[out=60, in=120](9.5, 1);
\node[] at (8.4, 2)   (b) {$S$};
\draw [->,line width=1pt,-stealth] (2,3.5)to[out=30, in=150](11, 3.5);
\node[] at (6.5, 5.3)   (b) {$S\circ T$};
\end{tikzpicture}
\end{center}
 
\end{explanation}
\end{example}
 
\begin{theorem}\label{th:complinear}
The composition of two linear transformations is linear.
\end{theorem}
\begin{proof}
Let $T:U\rightarrow V$ and $S:V\rightarrow W$ be linear transformations. We will show that $S\circ T$ is linear.  For all vectors $\vec{u}_1$ and $\vec{u}_2$ of $U$ and scalars $a$ and $b$ we have:
\begin{align*}
(S\circ T)(a\vec{u}_1+b\vec{u}_2)&=S(T(a\vec{u}_1+b\vec{u}_2))\\
&=S(aT(\vec{u}_1)+bT(\vec{u}_2))\\
&=aS(T(\vec{u}_1))+bS(T(\vec{u}_2))\\
&=a(S\circ T)(\vec{u}_1)+b(S\circ T)(\vec{u}_2)
\end{align*}
\end{proof}

 
\begin{theorem}\label{th:compoflintransass} Composition of linear transformations is associative.  In other words, for linear transformations  $T$, $S$ and $R$


 
    \begin{center}
        \begin{tikzpicture}
        %\node{
        \begin{tikzcd}
        U\rar{T}\arrow[black, bend right]{rrr}[black,swap]{R\circ S\circ T}  & V \rar{S}  & W \rar{R} & Z
        \end{tikzcd}
        %};
        \end{tikzpicture}
    \end{center}
 
We have $(R\circ S)\circ T=R\circ (S\circ T)$.
\end{theorem}



\begin{proof} For all $\vec{u}$ in $U$ we have:
\begin{align*}
((R\circ S)\circ T)(\vec{u})&=(R\circ S)(T(\vec{u}))=R(S(T(\vec{u})))\\
&=R((S\circ T)(\vec{u}))=(R\circ (S\circ T))(\vec{u})
\end{align*}
\end{proof}
 
\subsection*{Composition and Matrix Multiplication}
 
In this section we will consider linear transformations of $\RR^n$ and their standard matrices. 
 
\begin{theorem}\label{th:standardmatcompoflintrans}
Let $T:\RR^n\rightarrow \RR^m$ and $S:\RR^m\rightarrow \RR^p$ be linear transformations with standard matrices $M_T$ and $M_S$, respectively.  Then the composite transformation $S\circ T:\RR^n\rightarrow \RR^p$ has a standard matrix given by the product $M_SM_T$.
\end{theorem}
\begin{proof}
For all $\vec{v}$ in $\RR^n$ we have:
$$(S\circ T)(\vec{v})=S(T(\vec{v}))=S(M_T\vec{v})=M_S(M_T\vec{v})=(M_SM_T)\vec{v}$$
\end{proof}

 
\begin{example}\label{ex:standardmatofcomp}
In Example \ref{ex:transcomp}, we discussed a composite transformation $S\circ T:\RR^2\rightarrow \RR^2$
given by:
$$T\left(\begin{bmatrix}u_1\\u_2\end{bmatrix}\right)=\begin{bmatrix}u_1+u_2\\3u_1+3u_2\end{bmatrix}\quad \text{and} \quad
S\left(\begin{bmatrix}v_1\\v_2\end{bmatrix}\right)=\begin{bmatrix}3v_1-v_2\\-3v_1+v_2\end{bmatrix}$$
Express $S\circ T$ as a matrix transformation.
\begin{explanation}
The standard matrix for $T:\RR^2\rightarrow \RR^2$ is $$\begin{bmatrix}1&1\\3&3\end{bmatrix}$$ and the standard matrix for $S:\RR^2\rightarrow \RR^2$ is $$\begin{bmatrix}3&-1\\-3&1\end{bmatrix}$$
The standard matrix for $S\circ T$ is the product
$$\begin{bmatrix}3&-1\\-3&1\end{bmatrix}\begin{bmatrix}1&1\\3&3\end{bmatrix}=\begin{bmatrix}0&0\\0&0\end{bmatrix}$$
\end{explanation}
\end{example}

 
We conclude this section by revisiting the associative property of matrix multiplication.  At the time matrix multiplication was introduced, we skipped the cumbersome proof that for appropriately sized matrices $A$, $B$ and $C$, we have $(AB)C=A(BC)$. (See Theorem \ref{th:propertiesofmatrixmultiplication}.)  We are now in a position to prove this result with ease. 
 
Every matrix induces a linear transformation.  The product of two matrices can be interpreted as a composition of transformations.  Since function composition is associative, so is matrix multiplication. We formalize this observation as a theorem.
 
\begin{theorem}[Associativity of Matrix Multiplication] \label{th:associativematrixmult}  Let $A$, $B$ and $C$ be matrices of appropriate dimensions so that the product $(AB)C$ is defined.  Then
$$(AB)C=A(BC)$$
\end{theorem}

 
\subsection*{Inverses of Linear Transformations}

 
\begin{exploration}\label{ep:inverse} Define a linear transformation $T:\RR^2\rightarrow \RR^2$ by $T(\vec{v})=2\vec{v}$.  In other words, $T$ doubles every vector in $\RR^2$.  Now define $S:\RR^2\rightarrow \RR^2$ by $S(\vec{v})=\frac{1}{2}\vec{v}$.  What happens when we compose these two transformations?

$$(S\circ T)(\vec{v})=S(T(\vec{v}))=S(2\vec{v})=\left(\frac{1}{2}\right)(2)\vec{v}=\vec{v}$$

$$(T\circ S)(\vec{v})=T(S(\vec{v}))=T(\frac{1}{2}\vec{v})=(2)\left(\frac{1}{2}\right)\vec{v}=\vec{v}$$
Both composite transformations return the original vector $\vec{v}$. 

In other words, $S\circ T$ and $T\circ S$ are just the identity maps.  We say that $S$ is an \dfn{inverse} of $T$, and $T$ is an inverse of $S$.
\end{exploration}



 
\begin{definition}\label{def:inverseoflintrans} Let $V$ and $W$ be vector spaces, and let $T:V\rightarrow W$ be a linear transformation.  A transformation $S:W\rightarrow V$ that satisfies $S\circ T=\id_V$ and $T\circ S=\id_W$ is called an \dfn{inverse} of $T$. If $T$ has an inverse, $T$ is called \dfn{invertible}.
\end{definition}
 
\begin{example}\label{ex:inverseverify} Let $T:\RR^2\rightarrow \RR^2$ be a transformation defined by $T\left(\begin{bmatrix}x\\y\end{bmatrix}\right)=\begin{bmatrix}x+y\\x-y\end{bmatrix}$. (How would you verify that $T$ is linear?)  Show that $S:\RR^2\rightarrow \RR^2$ given by $S\left(\begin{bmatrix}x\\y\end{bmatrix}\right)=\begin{bmatrix}0.5x+0.5y\\0.5x-0.5y\end{bmatrix}$ is an inverse of $T$.
\begin{explanation}
We will show that $S\circ T=\id_{\RR^2}$. 
\begin{align*}
(S\circ T)\left(\begin{bmatrix}x\\y\end{bmatrix}\right)&=S\left(T\left(\begin{bmatrix}x\\y\end{bmatrix}\right)\right)=S\left(\begin{bmatrix}x+y\\x-y\end{bmatrix}\right)\\
&=\begin{bmatrix}0.5(x+y)+0.5(x-y)\\0.5(x+y)-0.5(x-y)\end{bmatrix}
=\begin{bmatrix}x\\y\end{bmatrix}
\end{align*}
We leave it to the reader to verify that $T\circ S=\id_{\RR^2}$.
 
\end{explanation}
\end{example}

 
\subsection*{Linearity of Inverses of Linear Transformations}
 
Definition \ref{def:inverseoflintrans} does not specifically require an inverse $S$ of a linear transformation $T$ to be linear, but it turns out that the requirement that $S\circ T=\id_V$ and $T\circ S=\id_W$ is sufficient to guarantee that $S$ is linear. 
 
\begin{theorem}\label{th:inverseislinear} Suppose $T:V\rightarrow W$ is an invertible linear transformation.  Let $S$ be an inverse of $T$.  Then $S$ is  linear.
\end{theorem}
 
 
\subsection*{Linear Transformations of $\RR^n$ and the Standard Matrix of the Inverse Transformation}
 
Every linear transformation $T:\RR^n\rightarrow\RR^m$ is a matrix transformation. (See Theorem \ref{th:matlin}.)  If $T$ has an inverse $S$, then by Theorem \ref{th:inverseislinear}, $S$ is also a matrix transformation.  Let  $M_T$ and $M_S$ denote the standard matrices of $T$ and $S$, respectively.  We see that $S\circ T=\id_{\RR^n}$ and $T\circ S=\id_{\RR^m}$ if and only if $M_SM_T=I_{n}$ and $M_TM_S=I_{m}$.  In other words, $T$ and $S$ are inverse transformations if and only if $M_T$ and $M_S$ are matrix inverses.
 
Note that if $S$ is an inverse of $T$, then $M_T$ and $M_S$ are square matrices, and $n=m$.

 
\begin{theorem}\label{th:existunique} Let $T:\RR^n\rightarrow \RR^n$ be a linear transformation, and let $M$ be the standard matrix of $T$.
  \begin{enumerate}
  \item \label{item:exists} (Existence of Inverses.)  $T$ is invertible if and only if $M$ is invertible.  If $T$ is invertible, then the inverse is induced by $M^{-1}$.
  \item \label{item:unique} (Uniqueness of Inverses.)  If $S$ is an inverse of $T$, then $S$ is unique.
  \end{enumerate}
\end{theorem}
\begin{proof}
Part \ref{item:exists} follows directly from the preceding discussion.  Part \ref{item:unique} follows from uniqueness of matrix inverses. (Theorem \ref{th:matinverseunique}.)
\end{proof}


 
Please note that Theorem \ref{th:existunique} is only applicable in the context of linear transformations of $\RR^n$ and their standard matrices.  The following example provides us with motivation to investigate inverses further, which we will do in \href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/LTR-0035/main}{Existence of the Inverse of a Linear Transformation}.
 
\begin{exploration}\label{init:subtosub}
Let $$V=\text{span}\left(\begin{bmatrix}1\\0\\0\end{bmatrix}, \begin{bmatrix}1\\1\\1\end{bmatrix}\right)$$
Define a linear transformation $$T:V\rightarrow \RR^2$$
by $$T\left(\begin{bmatrix}1\\0\\0\end{bmatrix}\right)=\begin{bmatrix}1\\1\end{bmatrix}\quad \text{and} \quad T\left(\begin{bmatrix}1\\1\\1\end{bmatrix}\right)=\begin{bmatrix}0\\1\end{bmatrix}$$
Observe that $\left\{\begin{bmatrix}1\\0\\0\end{bmatrix}, \begin{bmatrix}1\\1\\1\end{bmatrix}\right\}$ is a basis of $V$ (why?). The information about the images of the basis vectors is sufficient to define a linear transformation.  This is because every vector $\vec{v}$ in $V$ can be expressed as a linear combination of the basis elements.  The image, $T(\vec{v})$, can be found by applying the linearity properties.
At this point we know what transformation $T$ does, but it is still unclear what the matrix of this linear transformation is.
 
Geometrically speaking, the domain of $T$ is a plane in $\RR^3$ and its codomain is $\RR^2$. 
 
Does $T$ have an inverse?  We are not in a position to answer this question right now because Theorem \ref{th:existunique} does not apply to this situation.
\end{exploration}
Exploration \ref{init:subtosub} highlights the necessity of further discussion of inverses and of matrices associated with linear transformations.  In Example \ref{ex:subtosub}, we will prove that $T$ has an inverse, and in Example \ref{ex:inversematrixoftransform}, we will address the issue of matrices associated with $T$ and its inverse.
 
 
\section*{Practice Problems}
\begin{problem}\label{prob:geometryoflintrans1}
Let $T:\RR^2\rightarrow \RR^2$ and $S:\RR^2\rightarrow \RR^2$ be linear transformations with standard matrices
$$M_T=\begin{bmatrix}2&-4\\1&2\end{bmatrix}\quad\text{and}\quad M_S=\begin{bmatrix}1&-1\\2&1\end{bmatrix}$$
respectively.  Describe the actions of $T$, $S$, and $S\circ T$ geometrically, as in Example \ref{ex:transcomp}.
\end{problem}
\begin{problem}\label{prob:geometryoflintrans2}
Let $T:\RR^3\rightarrow \RR^2$ and $S:\RR^2\rightarrow \RR^2$ be linear transformations with standard matrices
$$M_T=\begin{bmatrix}1&0&-1\\2&1&0\end{bmatrix}\quad\text{and}\quad M_S=\begin{bmatrix}-1&2\\1&-2\end{bmatrix}$$
respectively.  Describe the actions of $T$, $S$, and $S\circ T$ geometrically, as in Example \ref{ex:transcomp}.
\end{problem}
\begin{problem}\label{prob:completeinverseverify}
Complete the Explanation of Example \ref{ex:inverseverify} by verifying that $T\circ S=\id_{\RR^2}$.
\end{problem}
\begin{problem}\label{prob:candidateforinv}
Let $T:\RR^2\rightarrow \RR^2$ be a linear transformation given by
$$T\left(\begin{bmatrix}x\\y\end{bmatrix}\right)=\begin{bmatrix}2x-5y\\-x+3y\end{bmatrix}$$
Propose a candidate for the inverse of $T$ and verify your choice using Definition \ref{def:inverseoflintrans}.
 
$$T^{-1}\left(\begin{bmatrix}x\\y\end{bmatrix}\right)=\begin{bmatrix}\answer{3}x+\answer{5}y\\x+\answer{2}y\end{bmatrix}$$
\end{problem}
\begin{problem}\label{prob:noinversetrans}Explain why linear transformation $T:\RR^2\rightarrow\RR^2$ given by $$T\left(\begin{bmatrix}x\\y\end{bmatrix}\right)=\begin{bmatrix}2x+2y\\-3x-3y\end{bmatrix}$$ does not have an inverse.
\end{problem}
\begin{problem}\label{prob:inverseislinear}
Prove Theorem \ref{th:inverseislinear}. 
\end{problem}
\begin{problem}\label{prob:compofinvisinvofcomp} Suppose $T:U\rightarrow V$ and $S:V\rightarrow W$ are linear transformations with inverses $T'$ and $S'$ respectively.  Prove that $T'\circ S'$ is the inverse of $S\circ T$.
\end{problem}



\subsection{Right and left inverses}

So far, we have only talked about the inverses of square matrices. But
what about matrices that are not square? Can they be invertible? It
turns out that non-square matrices can never be invertible. However,
they can have left inverses or right inverses.


\begin{definition}{Left and right inverses}{left-and-right-inverse}
  Let $A$ be an $m\times n$-matrix and $B$ an $n\times m$-matrix.  We
  say that $B$ is a \textbf{left inverse}%
  \index{inverse!left inverse}%
  \index{left inverse}%
  \index{matrix!left inverse}%
  \index{matrix!inverse!left inverse} of $A$ if
  \begin{equation*}
    BA=I.
  \end{equation*}
  We say that $B$ is a \textbf{right inverse}%
  \index{inverse!right inverse}%
  \index{right inverse}%
  \index{matrix!right inverse}%
  \index{matrix!inverse!right inverse} of $A$ if
  \begin{equation*}
    AB=I.
  \end{equation*}
  If $A$ has a left inverse, we also say that $A$ is
  \textbf{left invertible}. Similarly, if $A$ has a right inverse, we
  say that $A$ is \textbf{right invertible}.
\end{definition}

\begin{example}{Right inverse}{right-inverse}
  Let
  \begin{equation*}
    A = \startmat{rrr}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
    \stopmat
    \quad\mbox{and}\quad
    B = \startmat{rr}
      1 & 0 \\
      0 & 1 \\
      0 & 0 \\
    \stopmat.
  \end{equation*}
  Show that $B$ is a right inverse, but not a left inverse, of $A$.
\end{example}



\begin{solution}
  We compute
  \begin{equation*}
    AB
    ~=~ \startmat{rrr}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
    \stopmat
    \startmat{rr}
      1 & 0 \\
      0 & 1 \\
      0 & 0 \\
    \stopmat
    ~=~ \startmat{rrr}
      1 & 0 \\
      0 & 1 \\
    \stopmat
    ~=~ I,
  \end{equation*}
  \begin{equation*}
    BA
    ~=~ \startmat{rr}
      1 & 0 \\
      0 & 1 \\
      0 & 0 \\
    \stopmat
    \startmat{rrr}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
    \stopmat
    ~=~ \startmat{rrr}
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 0 & 0 \\
    \stopmat
    ~\neq~ I.
  \end{equation*}
  Therefore, $B$ is a right inverse, but not a left inverse, of $A$.
\end{solution}

Recall from Definition~\ref{def:invertible-matrix} that $B$ is called
an \textbf{inverse}%
\index{inverse!of a matrix}%
\index{matrix!inverse} of $A$ if it is both a left inverse and a right
inverse. A crucial fact is that invertible matrices are always square.

\begin{theorem}{Invertible matrices are square}{invertible-square}
  Let $A$ be an $m\times n$-matrix.
  \begin{itemize}
  \item If $A$ is left invertible, then $m\geq n$.
  \item If $A$ is right invertible, then $m\leq n$.
  \item If $A$ is invertible, then $m=n$.
  \end{itemize}
  In particular, only square matrices can be invertible.
\end{theorem}

\begin{proof}
  To prove the first claim, assume that $A$ is left invertible, i.e.,
  assume that $BA=I$ for some $n\times m$-matrix $B$. We must show
  that $m\geq n$. Assume, for the sake of obtaining a contradiction,
  that this is not the case, i.e., that $m<n$. Then the matrix $A$ has
  more columns than rows. It follows that the homogeneous system of
  equations $A\vect{x}=\vect{0}$ has a non-trivial solution; let
  $\vect{x}$ be such a solution. We obtain a contradiction by a
  similar method as in
  Example~\ref{exa:non-invertible-matrix}. Namely, we have
  \begin{equation*}
    \vect{x} ~=~ I\,\vect{x} ~=~ (BA)\vect{x} ~=~ B(A\vect{x}) ~=~ B\vect{0} ~=~
    \vect{0},
  \end{equation*}
  contradicting the fact that $\vect{x}$ was non-trivial.  Since we
  got a contradiction from the assumption that $m<n$, it follows that
  $m\geq n$.

  The second claim is proved similarly, but exchanging the roles of
  $A$ and $B$.  The third claim follows directly from the first two
  claims, because every invertible matrix is both left and right
  invertible.
\end{proof}

Of course, not all square matrices are invertible. In particular, zero
matrices are not invertible, along with many other square matrices.

\end{document}