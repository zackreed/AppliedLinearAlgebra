\documentclass{ximera}
\input{../../../preamble.tex}

\author{Zack Reed}
%borrowed from selinger linear algebra
\title{Learning Activity: Investigating The Properties of Images and Kernels of Linear Transformations}\license{CC BY-NC-SA 4.0}

\begin{document}
\begin{abstract}
    In this learning activity, you will learn about the image and kernel sets of linear transformations. These are important for determining properties of matrices, such as when they are invertible or not.
\end{abstract}
\maketitle


\section*{Image and Kernel of a Linear Transformation}

 
 
In the previous activity we observed that the image of the linear transformation $T$ is the same as the span of the columns of the associated matrix $A$. We also saw that we could exactly describe the images of linear transformations, as well as their kernels (aka the null space of the associated matrix).

We're going to press further into exploring the spaces related to linear transformations' domains and co-domains, and get some simply stated and at times useful results.

First, let's formally talk about the image of a transformation as the span of the column vectors from the associated matrix. This is called the \emph{column space} of the matrix.

\begin{definition}{Column Space}

The column space of the matrix $A$ is the span of its column vectors.

Since the column space directly determines the image of the associated transformation $T$, we can state that:

$$\mbox{im}(T)=\mbox{col}(A)$$
\end{definition}

Moreover, since $\mbox{im}(T)$ and $\mbox{col}(A)$ are the same space, they have the same dimension. We have a special word for the dimension of the column space, called the \emph{rank} of the matrix. 

\begin{definition}{Rank}

  The rank of a matrix $A$ is the dimension of its column space. You might also describe this as the rank of the transformation $T$, but this is less common. 

  As such, we have

$$\mbox{dim}(\mbox{im}(T))=\mbox{dim}(\mbox{col}(A))=\mbox{rank}(A)$$
\end{definition}
 
\begin{example}\label{ex:image2}
Let $T:\RR^5\rightarrow \RR^4$ be a linear transformation with standard matrix $$A=\begin{bmatrix}1 & 2 & 2 &-1 & 0\\-1 & 3 & 1 & 0 & -1\\3 & 0 & 0 & 3 & 6\\ 1 & -1 & 1 & -2 & -1\end{bmatrix}$$
Find $\mbox{im}(T)$ and $\mbox{rank}(A)$.

\begin{explanation}
As before, the image of $T$ is the column space of $A$, so we have 

$$\mbox{im}(T)=\mbox{span}\left(\begin{bmatrix}1\\-1\\3\\1\end{bmatrix}, \begin{bmatrix}2\\3\\0\\-1\end{bmatrix}, \begin{bmatrix}2\\1\\0\\1\end{bmatrix}, \begin{bmatrix}-1\\0\\3\\-2\end{bmatrix}, \begin{bmatrix}0\\-1\\6\\-1\end{bmatrix}\right)=\mbox{col}(A)$$

To better describe this, and see which vectors might be linear combinations of others, let's find \texttt{rref(A)}.
 
$$\text{rref}(A)=\begin{bmatrix} 1 & 0 & 0 & 1 & 2\\0 & 1 & 0 & 1 & 1\\0 & 0 & 1 & -2 & -2\\ 0 & 0 & 0 & 0 & 0 \end{bmatrix}$$
 
We can see that, more descriptively, the image is just the span of the first three vectors.

$$\mbox{im}(T)=\mbox{span}\left(\begin{bmatrix}1\\-1\\3\\1\end{bmatrix}, \begin{bmatrix}2\\3\\0\\-1\end{bmatrix}, \begin{bmatrix}2\\1\\0\\1\end{bmatrix}\right)$$
 
This also lets us determine $\mbox{rank}(A)$, which is $\answer{3}$. You might also notice that there were $\answer{3}$ pivot columns in \texttt{rref(A)}. This is no coincidence, and in fact you can quickly determine the rank of any matrix by counting the number of pivot columns in its \rref.
\end{explanation}
\end{example}
 
In the spirit of ``having many names for the same thing'', we have the following simple equation that shows agreement between the image of transformations and the column space of matrices.
 
\begin{formula}\label{form:rankTrankA}
$$\mbox{rank}(A) = \mbox{dim}(\mbox{col}(A))=\mbox{dim}(\mbox{im}(T))=\mbox{rank}(T)$$
\end{formula}
 
%Note that since  $\mbox{rank}(A) = \mbox{dim}(\mbox{col}(A))=\mbox{dim}(\mbox{im}(T))$, our definitions of the rank of a linear transformation and the rank of its associated standard matrix are in agreement.
 
\subsection*{Nullity and the Kernel of a Linear Transformation}
 
Before, we discussed how the \emph{kernel} of a linear transformation is all vectors that map to $\vec{0}$, that is, $\vec{v}$ such that $T(\vec{v})=\vec{0}$. For the sake of again using different verbiage for the same thing, it is worth stating that the kernel of a transformation is the null space of its matrix.

$$\mbox{ker}(T)=\mbox{null}(A).$$
 
\begin{example}\label{ex:kernel} Let $T:\RR^5\rightarrow \RR^4$ be a linear transformation with standard matrix $$A=\begin{bmatrix}1 & 2 & 2 &-1 & 0\\-1 & 3 & 1 & 0 & -1\\3 & 0 & 0 & 3 & 6\\ 1 & -1 & 1 & -2 & -1\end{bmatrix}$$
\begin{enumerate}
\item \label{item:kernelT}
Find $\mbox{ker}(T)$
\item \label{item:dimkernelT}
Find $\mbox{dim}(\mbox{ker}(T))$.
\end{enumerate}
\begin{explanation}

\ref{item:kernelT} We did this before, in the previous section, however it's always good to practice. To find the kernel of $T$, we need to find all vectors of $\RR^5$ that map to $\vec{0}$ in $\RR^4$.  This amounts to solving the equation $A\vec{x}=\vec{0}$.
 
\texttt{rref(A)} yields:
 
$$\mbox{rref}(A)= \begin{bmatrix} 1 & 0 & 0 & 1 & 2\\0 & 1 & 0 & 1 & 1\\0 & 0 & 1 & -2 & -2\\ 0 & 0 & 0 & 0 & 0 \end{bmatrix}$$
 
From our work with null spaces, we can form vectors whose linear combinations form the kernel of $T$:
$$\mbox{ker}(T)=\begin{bmatrix}\answer{-1}\\-1\\\answer{2}\\\answer{1}\\0\end{bmatrix}s+\begin{bmatrix}\answer{-2}\\-1\\2\\0\\\answer{1}\end{bmatrix}t$$
 
We conclude that
$$\mbox{ker}(T)=\mbox{span}\left(\begin{bmatrix}\answer{-1}\\-1\\\answer{2}\\\answer{1}\\0\end{bmatrix},\begin{bmatrix}\answer{-2}\\-1\\2\\0\\\answer{1}\end{bmatrix}\right)$$
 
\ref{item:dimkernelT}  Since $\mbox{ker}(T)$ is the span of two vectors of $\RR^5$, we can find the dimension of $\mbox{ker}(T)$. A quick check will determine that the two spanning vectors are linearly independent, so we can see that $\mbox{dim}(\mbox{ker}(T))=2$.

In fact, because of Gauss-Jordan elimination the spanning vectors are guaranteed to be linearly independent, so you can always assume that this method yields the entirety of the kernel (and hence the null space).
\end{explanation}
\end{example}
 
 
Recall that the \dfn{null space} of a matrix $A$ is defined to be set of all solutions to the homogeneous equation $A\vec{x}=\vec{0}$. This means that  if $T:\RR^n\rightarrow \RR^m$ is a linear transformation with standard matrix $A$ then
$$\mbox{ker}(T)=\mbox{null}(A)$$
 
\subsection*{Subspaces}

Let's conclude by nailing down some important properties of these spaces that we've been discussing. 

Recall that a \emph{vector space} $V$ is such that all linear combinations of vectors in $V$ stay in $V$, and that $\vec{0}\in V$.

Also recall that we sometimes want to talk about a \emph{subspace}. That is, we want to specifically reference some vector space, such as $\mbox{col}(A)$, while keeping in mind that it lives within an ambient vector space $V$. It's thus worth going over the following definition again:

\begin{definition}{Subspace}

  A vector space $W$ is a subspace of a vector space $V$ is $W$ is entirely contained within $V$, and is itself a vector space. 

  \begin{example}
  
    $\RR^2$ is a subspace of $\RR^3$, but not of $\RR^1$ since $\RR^2$ is not contained within $\RR^1$. 

  \end{example}

Importantly, all of the spaces we've discussed so far (images, kernels, null spaces) are all subspaces of the domain and co-domain of a transformation. 

BEGIN HERE

\end{definition}

We know that $\mbox{null}(A)$ of an $m\times n$ matrix is a subspace of $\RR^n$. (See Theorem \ref{th:nullsubspacern}.)  We conclude this section by showing that even when vector spaces other than $\RR^n$ are involved, the kernel of a linear transformation is a subspace of the domain of the transformation.
\begin{theorem}\label{th:kersubspace} Let $T:V\rightarrow W$ be a linear transformation, then $\mbox{ker}(T)$ is a subspace of $V$.
\end{theorem}
\begin{proof}
To show that $\mbox{ker}(T)$ is a subspace, we need to show that $\mbox{ker}(T)$ is closed under addition and scalar multiplication.
 
Suppose that $\vec{v}_1$ and $\vec{v}_2$ are in $\mbox{ker}(T)$.  Then,
$$T(\vec{v}_1+\vec{v}_2)=T(\vec{v}_1)+T(\vec{v}_2)=\vec{0}+\vec{0}=\vec{0}$$
This shows that $\vec{v}_1+\vec{v}_2$ is in $\mbox{ker}(T)$.
 
For any scalar $a$ we have:
$$T(a\vec{v}_1)=aT(\vec{v}_1)=a\vec{0}=\vec{0}$$
This shows that $a\vec{v}_1$ is in $\mbox{ker}(T)$.
 
\end{proof}
 
 
 
\begin{definition}\label{def:nullityT}
The \dfn{nullity} of a linear transformation $T:V\rightarrow W$, is the dimension of the kernel of $T$.
$$\mbox{nullity}(T)=\mbox{dim}(\mbox{ker}(T))$$
\end{definition}
 
This definition gives us the following relationship between nullity of a linear transformation $T:\RR^n\rightarrow\RR^m$ and the nullity of the standard matrix $A$ associated with it.
 
\begin{formula}\label{form:nullTnullA}
$$\mbox{nullity}(A) = \mbox{dim}(\mbox{null}(A))=\mbox{dim}(\mbox{ker}(T))=\mbox{nullity}(T)$$
\end{formula}
 
 
%Notice that since the nullity of a matrix $A$ was defined as $\mbox{nullity}(A) = \mbox{dim}(\mbox{null}(A))$ (Definition \ref{def:matrixnullity} of VSP-0040), and the kernel of a linear transformation $T:\RR^n\rightarrow \RR^m$ is the null space of its associated standard matrix, our definitions of nullity are in agreement.
 
\subsection*{Rank-Nullity Theorem for Linear Transformations}
 
In Examples \ref{ex:image2} and \ref{ex:kernel}, we found the image and the kernel of the linear transformation $T:\RR^5\rightarrow \RR^4$ with standard matrix
 
$$A=\begin{bmatrix}1 & 2 & 2 &-1 & 0\\-1 & 3 & 1 & 0 & -1\\3 & 0 & 0 & 3 & 6\\ 1 & -1 & 1 & -2 & -1\end{bmatrix}$$
 
We also found that
$$\mbox{rank}(T)=\mbox{dim}(\mbox{im}(T))=\mbox{dim}(\mbox{col}(A))=\mbox{rank}(A)=3$$
and
$$\mbox{nullity}(T)=\mbox{dim}(\mbox{ker}(T))=\mbox{dim}(\mbox{null}(A))=\mbox{nullity}(A)=2$$
 
Because of Rank-Nullity Theorem for matrices (Theorem \ref{th:matrixranknullity}), it is not surprising that
$$\mbox{rank}(T)+\mbox{nullity}(T)=3+2=5=\mbox{dim}(\RR^5)$$
 
 
The following theorem is a generalization of this result.
 
\begin{theorem}\label{th:ranknullityforT}
Let $T:V\rightarrow W$ be a linear transformation.  Suppose $\mbox{dim}(V)=n$, then
$$\mbox{rank}(T)+\mbox{nullity}(T)=n$$
\end{theorem}
 
\begin{proof}
By Theorem \ref{th:imagesubspace}, $\mbox{im}(T)$ is a subspace of $W$.  There exists a basis for $\mbox{im}(T)$ of the form $\{T(\vec{v}_1), \ldots,T(\vec{v}_r)\}$.  By Theorem \ref{th:kersubspace}, $\mbox{ker}(T)$ is a subspace of $V$.  Let $\{\vec{u}_1,\ldots,\vec{u}_s\}$ be a basis for $\mbox{ker}(T)$.
 
We will show that $\{\vec{u}_1,\ldots ,\vec{u}_s, \vec{v}_1,\ldots ,\vec{v}_r\}$ is a basis for $V$.
 
For any vector $\vec{v}$ in $V$, we have:
$$T(\vec{v})=c_1T(\vec{v}_1)+\ldots +c_rT(\vec{v}_r)$$
for some scalars $c_i$ $(1\leq i\leq r)$.
Thus,
$$T(\vec{v})-\big(c_1T(\vec{v}_1)+\ldots +c_rT(\vec{v}_r)\big)=\vec{0}$$
By linearity,
$$T((\vec{v}-(c_1\vec{v}_1+\ldots +c_r\vec{v}_r))=\vec{0}$$
Therefore $\vec{v}-(c_1\vec{v}_1+\ldots +c_r\vec{v}_r)$ is in $\mbox{ker}(T)$.
 
Hence there are scalars $a_i$ $(1\leq i\leq s)$ such that
$$\vec{v}-(c_1\vec{v}_1+\ldots +c_r\vec{v}_r)=a_1\vec{u}_1+\ldots +a_s\vec{u}_s$$
Thus,
$$\vec{v}=(c_1\vec{v}_1+\ldots +c_r\vec{v}_r)+(a_1\vec{u}_1+\ldots +a_s\vec{u}_s)$$
 
We conclude that
$$V=\mbox{span}(\vec{u}_1,\ldots ,\vec{u}_s, \vec{v}_1,\ldots ,\vec{v}_r)$$
 
Now we need to show that $\{\vec{u}_1,\ldots ,\vec{u}_s, \vec{v}_1,\ldots ,\vec{v}_r\}$ is linearly independent.
 
Suppose
\begin{align}\label{eq:kerplusimproof} c_1\vec{v}_1+\ldots +c_r\vec{v}_r+a_1\vec{u}_1+\ldots +a_s\vec{u}_s=\vec{0}\end{align}
Applying $T$ to both sides, we get
$$T(c_1\vec{v}_1+\ldots +c_r\vec{v}_r+a_1\vec{u}_1+\ldots +a_s\vec{u}_s)=T(\vec{0})$$
$$c_1T(\vec{v}_1)+\ldots +c_rT(\vec{v}_r)+a_1T(\vec{u}_1)+\ldots +a_sT(\vec{u}_s)=\vec{0}$$
 
But $T(\vec{u}_i)=\vec{0}$ for $1\leq i\leq s$, thus
$$c_1T(\vec{v}_1)+\ldots +c_rT(\vec{v}_r)=\vec{0}$$
Since $\{T(\vec{v}_1),\ldots ,T(\vec{v}_r)\}$ is linearly independent, it follows that each $c_i=0$. 
 
But then Equation (\ref{eq:kerplusimproof}) implies that $a_1\vec{u}_1+\ldots +a_s\vec{u}_s=\vec{0}$.  Because $\{\vec{u}_1, \ldots ,\vec{u}_s\}$ is linearly independent, it follows that each $a_i=0$. 
 
We conclude that $\{\vec{u}_1,\ldots ,\vec{u}_s,\vec{v}_1,\ldots ,\vec{v}_r\}$ is a basis for $V$.  Thus,
 
$$\mbox{dim}(\mbox{ker}(T))+\mbox{dim}(\mbox{im}(T))=s+r=n$$
\end{proof}
 
\section*{Practice Problems}
\begin{problem}
Describe the image and find the rank for each linear transformation $T:\RR^n\rightarrow \RR^m$ with standard matrix $A$ given below.
  \begin{problem}\label{prob:imagerankoflintrans1}
  $T:\RR^5\rightarrow \RR^2$, $A=\begin{bmatrix}3&2&4&7&1\\-1&-9&7&6&8\end{bmatrix}$.
   
  \begin{multipleChoice}
  \choice[correct]{$\mbox{im}(T)=\RR^2$}
  \choice{$\mbox{im}(T)$ is a line in $\RR^2$}
  \choice{$\mbox{im}(T)=\{\vec{0}\}$ }
  \choice{$\mbox{im}(T)=\RR^5$}
  \choice{$\mbox{im}(T)$ is a plane in $\RR^5$}
\end{multipleChoice}
   
  $\mbox{rank}(T)=\answer{2}$
  \end{problem}
   
  \begin{problem}\label{prob:imagerankoflintrans2}
  $T:\RR^2\rightarrow\RR^3$, $A=\begin{bmatrix}1&1\\1&1\\1&1\end{bmatrix}$
   
  \begin{multipleChoice}
  \choice{$\mbox{im}(T)=\RR^3$}
  \choice{$\mbox{im}(T)$ is a line in $\RR^2$}
  \choice[correct]{$\mbox{im}(T)$ is a line in $\RR^3$}
  \choice{$\mbox{im}(T)=\{\vec{0}\}$ }
  \choice{$\mbox{im}(T)$ is a plane in $\RR^3$}
\end{multipleChoice}
   
  $\mbox{rank}(T)=\answer{1}$
  \end{problem}
\end{problem}
 
 
\begin{problem}\label{prob:sametrans} Suppose linear transformations $T:\RR^2\rightarrow \RR^2$ and $S:\RR^2\rightarrow \RR^2$ are such that  $\mbox{im}(T)=\mbox{im}(S)=\mbox{span}\left(\begin{bmatrix}1\\-3\end{bmatrix}\right)$.  Does this mean that $T$ and $S$ are the same transformation?  Justify your claim.
\end{problem}
 
\begin{problem}
Describe the kernel and find the nullity for each linear transformation $T:\RR^n\rightarrow \RR^m$ with standard matrix $A$ given below.
 
\begin{problem}\label{prob:kerandnullityT1}
$T:\RR^3\rightarrow \RR^2$, $A=\begin{bmatrix}2&1&0\\-1&1&-3\end{bmatrix}$.
 
\begin{multipleChoice}
  \choice{$\mbox{ker}(T)=\RR^3$}
  \choice{$\mbox{ker}(T)=\{\vec{0}\}$ }
  \choice{$\mbox{ker}(T)=\RR^2$}
  \choice{$\mbox{ker}(T)$ is a plane in $\RR^3$}
   \choice[correct]{$\mbox{ker}(T)$ is a line in $\RR^3$}
\end{multipleChoice}
 
$\mbox{nullity}(T)=\answer{1}$
\end{problem}
 
\begin{problem}\label{prob:kerandnullityT2}
$T:\RR^2\rightarrow \RR^2$, $A=\begin{bmatrix}2&-1\\3&0\end{bmatrix}$.
 
\begin{multipleChoice}
  \choice{$\mbox{ker}(T)=\RR^2$}
  \choice[correct]{$\mbox{ker}(T)=\{\vec{0}\}$ }
   \choice{$\mbox{ker}(T)$ is a line in $\RR^2$}
\end{multipleChoice}
 
$\mbox{nullity}(T)=\answer{0}$
\end{problem}
 
\begin{problem}\label{prob:kerandnullityT3}
$T:\RR^3\rightarrow \RR^5$, $A=\begin{bmatrix}1&2&-1\\1&2&-1\\1&2&-1\\1&2&-1\\1&2&-1\end{bmatrix}$
 
\begin{multipleChoice}
 \choice[correct]{$\mbox{ker}(T)$ is a plane in $\RR^3$}
 \choice{$\mbox{ker}(T)$ is a line in $\RR^3$}
     \choice{$\mbox{ker}(T)$ is a line in $\RR^5$}
 \choice{$\mbox{ker}(T)=\RR^3$}
  \choice{$\mbox{ker}(T)=\{\vec{0}\}$ }
   
\end{multipleChoice}
 
$\mbox{nullity}(T)=\answer{2}$
\end{problem}
\end{problem}
 
\begin{problem}\label{prob:ranknullityT4}
Suppose a linear transformation $T:\RR^3\rightarrow \RR^3$ is such that $\mbox{im}(T)$ is a plane in $\RR^3$.  Then
$$\mbox{rank}(T)=\answer{2}$$ $$\mbox{nullity}(T)=\answer{1}$$
\end{problem}
 
\begin{problem}\label{prob:ranknullityT5}
Suppose a linear transformation $T:\RR^5\rightarrow \RR^5$ is such that $T(\vec{v})=\vec{0}$ for all $\vec{v}$ in $\RR^5$.  Then
$$\mbox{rank}(T)=\answer{0}$$ $$\mbox{nullity}(T)=\answer{5}$$
\end{problem}
 
\begin{problem}\label{prob:findimkergivenrref} Let $T:\RR^6\rightarrow \RR^4$ be a linear transformation with standard matrix
$$A=\begin{bmatrix}2&-1&1&-2&1&1\\1&2&3&6&-4&1\\0&2&2&4&-2&-1\\1&3&2&6&-3&2\end{bmatrix}$$
Find $\mbox{im}(T)$ and $\mbox{ker}(T)$ if the reduced row-echelon form of $A$ is
$$\text{rref}(A)=\begin{bmatrix}1&0&0&1&-1&0\\0&1&0&1&0&0\\0&0&1&1&-1&0\\0&0&0&0&0&1\end{bmatrix}$$
\end{problem}
\begin{problem}\label{prob:findimageandkernellintrans} Let $V=\mbox{span}\left(\begin{bmatrix}1\\1\end{bmatrix}\right)$, and let $T:V\rightarrow \RR^2$ be a linear transformation defined by
$T(\vec{v})=2\vec{v}$.  Find $\mbox{im}(T)$ and $\mbox{ker}(T)$.
\end{problem}
 
\begin{problem}\label{prob:ranknullitytrans}
Suppose a linear transformation $T$ is induced by a $4\times 6$ matrix $A$.  Let $S$ be a linear transformation induced by $A^T$.  Find $\mbox{nullity}(S)$, if $\mbox{nullity}(T)=3$.  Prove your claim.
\end{problem}
 
 

\end{document}