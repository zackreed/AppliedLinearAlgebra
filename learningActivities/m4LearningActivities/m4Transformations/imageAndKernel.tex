\documentclass{ximera}
\input{../../../preamble.tex}

\author{Zack Reed}
%borrowed from Anna Davis and Gilbert Strang
\title{Fundamental Subspaces}\license{CC BY-NC-SA 4.0}

\begin{document}
\begin{abstract}

\end{abstract}
\maketitle

\section*{Column and Null Space of a Matrix}

In the previous activity we observed that the image of the linear transformation $T$ is the same as the span of the columns of the associated matrix $A$. We also saw that we could exactly describe the images of linear transformations, as well as their kernels (aka the null space of the associated matrix).

We're going to press further into exploring the spaces related to linear transformations' domains and co-domains, and get some simply stated and at times useful results.

This will involve re-stating some results from the previous sections but in formal definitions, and making more explicit that these sets are actual vector spaces in their own right, called \emph{subspaces}.

First, let's formally talk about the image of a transformation as the span of the column vectors from the associated matrix. This is called the \emph{column space} of the matrix.

\begin{definition}{Column Space}

The column space of the matrix $A$ is the span of its column vectors.

Since the column space directly determines the image of the associated transformation $T$, we can state that:

$$\mbox{im}(T)=\mbox{col}(A)$$
\end{definition}

Moreover, since $\mbox{im}(T)$ and $\mbox{col}(A)$ are the same space, they have the same dimension. We have a special word for the dimension of the column space, called the \emph{rank} of the matrix. 

\begin{definition}{Rank}

  The rank of a matrix $A$ is the dimension of its column space. You might also describe this as the rank of the transformation $T$, but this is less common. 

  As such, we have

$$\mbox{dim}(\mbox{im}(T))=\mbox{dim}(\mbox{col}(A))=\mbox{rank}(A)$$
\end{definition}
 
\begin{example}\label{ex:image2}
Let $T:\RR^5\rightarrow \RR^4$ be a linear transformation with standard matrix $$A=\begin{bmatrix}1 & 2 & 2 &-1 & 0\\-1 & 3 & 1 & 0 & -1\\3 & 0 & 0 & 3 & 6\\ 1 & -1 & 1 & -2 & -1\end{bmatrix}$$
Find $\mbox{im}(T)$ and $\mbox{rank}(A)$.

\begin{explanation}
As before, the image of $T$ is the column space of $A$, so we have 

$$\mbox{im}(T)=\mbox{span}\left(\begin{bmatrix}1\\-1\\3\\1\end{bmatrix}, \begin{bmatrix}2\\3\\0\\-1\end{bmatrix}, \begin{bmatrix}2\\1\\0\\1\end{bmatrix}, \begin{bmatrix}-1\\0\\3\\-2\end{bmatrix}, \begin{bmatrix}0\\-1\\6\\-1\end{bmatrix}\right)=\mbox{col}(A)$$

To better describe this, and see which vectors might be linear combinations of others, let's find \texttt{rref(A)}.
 
$$\text{rref}(A)=\begin{bmatrix} 1 & 0 & 0 & 1 & 2\\0 & 1 & 0 & 1 & 1\\0 & 0 & 1 & -2 & -2\\ 0 & 0 & 0 & 0 & 0 \end{bmatrix}$$
 
We can see that, more descriptively, the image is just the span of the first three vectors.

$$\mbox{im}(T)=\mbox{span}\left(\begin{bmatrix}1\\-1\\3\\1\end{bmatrix}, \begin{bmatrix}2\\3\\0\\-1\end{bmatrix}, \begin{bmatrix}2\\1\\0\\1\end{bmatrix}\right)$$
 
This also lets us determine $\mbox{rank}(A)$, which is $\answer{3}$. You might also notice that there were $\answer{3}$ pivot columns in \texttt{rref(A)}. This is no coincidence, and in fact you can quickly determine the rank of any matrix by counting the number of pivot columns in its \rref.
\end{explanation}
\end{example}
 
In the spirit of ``having many names for the same thing'', we have the following simple equation that shows agreement between the image of transformations and the column space of matrices.
 
\begin{formula}\label{form:rankTrankA}
$$\mbox{rank}(A) = \mbox{dim}(\mbox{col}(A))=\mbox{dim}(\mbox{im}(T))=\mbox{rank}(T)$$
\end{formula}
 
%Note that since  $\mbox{rank}(A) = \mbox{dim}(\mbox{col}(A))=\mbox{dim}(\mbox{im}(T))$, our definitions of the rank of a linear transformation and the rank of its associated standard matrix are in agreement.
 
\subsection*{Nullity and the Kernel of a Linear Transformation}
 
Before, we discussed how the \emph{kernel} of a linear transformation is all vectors that map to $\vec{0}$, that is, $\vec{v}$ such that $T(\vec{v})=\vec{0}$. For the sake of again using different verbiage for the same thing, it is worth stating that the kernel of a transformation is the null space of its matrix.

$$\mbox{ker}(T)=\mbox{null}(A).$$
 
\begin{example}\label{ex:kernel} Let $T:\RR^5\rightarrow \RR^4$ be a linear transformation with standard matrix $$A=\begin{bmatrix}1 & 2 & 2 &-1 & 0\\-1 & 3 & 1 & 0 & -1\\3 & 0 & 0 & 3 & 6\\ 1 & -1 & 1 & -2 & -1\end{bmatrix}$$
\begin{enumerate}
\item \label{item:kernelT}
Find $\mbox{ker}(T)$
\item \label{item:dimkernelT}
Find $\mbox{dim}(\mbox{ker}(T))$.
\end{enumerate}
\begin{explanation}

\ref{item:kernelT} We did this before, in the previous section, however it's always good to practice.
 
\texttt{rref(A)} yields:
 
$$\mbox{rref}(A)= \begin{bmatrix} 1 & 0 & 0 & 1 & 2\\0 & 1 & 0 & 1 & 1\\0 & 0 & 1 & -2 & -2\\ 0 & 0 & 0 & 0 & 0 \end{bmatrix}$$
 
From our work with null spaces, we can form vectors whose linear combinations form the kernel of $T$:
$$\mbox{ker}(T)=\begin{bmatrix}\answer{-1}\\-1\\\answer{2}\\\answer{1}\\0\end{bmatrix}s+\begin{bmatrix}\answer{-2}\\-1\\2\\0\\\answer{1}\end{bmatrix}t$$
 
We conclude that
$$\mbox{ker}(T)=\mbox{span}\left(\begin{bmatrix}\answer{-1}\\-1\\\answer{2}\\\answer{1}\\0\end{bmatrix},\begin{bmatrix}\answer{-2}\\-1\\2\\0\\\answer{1}\end{bmatrix}\right)$$
 
\ref{item:dimkernelT}  Since $\mbox{ker}(T)$ is the span of \wordChoice{\choice[correct]{two}\choice{three}\choice{five}} vectors of $\RR^5$, we can find the dimension of $\mbox{ker}(T)$. A quick check will determine that the \wordChoice{\choice[correct]{two}\choice{three}\choice{five}} spanning vectors are linearly independent, so we can see that $\mbox{dim}(\mbox{ker}(T))=\answer{2}$.

In fact, because of Gauss-Jordan elimination the spanning vectors are guaranteed to be linearly independent, so you can always assume that this method yields the entirety of the kernel (and hence the null space).
\end{explanation}
\end{example}
 
 
Recall that the \dfn{null space} of a matrix $A$ is defined to be set of all solutions to the homogeneous equation $A\vec{x}=\vec{0}$. This means that  if $T:\RR^n\rightarrow \RR^m$ is a linear transformation with standard matrix $A$ then
$$\mbox{ker}(T)=\mbox{null}(A)$$

\subsection*{Rank, Nullity, and Invertibility Conditions}

One of the more important uses of rank and nullity for the purposes of the current chapter is for determining whether nor not a transformation is invertible. Let's return to our initial example of the squished mug.

\begin{center}
  \includegraphics[width=20cm, height=16cm]{og_mug.png}
\end{center}

\begin{center}
  \includegraphics[width=20cm, height=16cm]{squished_mug.png}
\end{center}

The transformation $A=\begin{bmatrix}
  1&0&1\\0&2&2\\1&2&3
\end{bmatrix}$ fundamentally alters the mug by squishing it down to a plane. Even though the plane resides in $3D$ space, there is no linear transformation (nor no function more generally) that can take the squished mug vectors and return them to the original mug vectors. From a transformation perspective, this is because vectors in the plane would need to map to multiple mug vectors in space, for which there is no real systematic way to assign such a mapping. Considering images and null spaces, this suggests that if the dimension of the image is less than the dimension of the domain (e.g., the transformation squishes space) then the underlying matrix cannot be invertible. 

Accordingly, for a matrix to be invertible, we at least need that the dimensions of the image and the domain are the same. In fact, not only is this necessary, but this is actually all we need! If the dimension of the image is the same as the domain, then the columns of the matrix form a basis for the image space, and are hence linearly independent! Linear independence means that the $\texttt{rref}$ of the matrix is the identity, and so we have invertibility! 

The above loose argument roughly outlines one direction of the reasoning behind the more comprehensive theorem, stated here:

\begin{theorem}
  Let $A$ be an $n\times n$ matrix. The following are equivalent to each other (meaning if one is true, all are true):
  \begin{enumerate}
    \item $A$ is invertible.
    \item $A$ can be row-reduced to the identity (i.e. $\texttt{rref(A)}$ is $I$).
    \item $A$ has $n$ pivot positions.
    \item The rank of $A$ is $n$.
    \item The nullity of $A$ is $0$.
    \item The columns of $A$ form a basis for $\RR^n$.
    \item $A$ is a finite product of elementary matrices.
  \end{enumerate}
\end{theorem}

You now have a laundry list of conditions that can tell you whether a matrix is invertible. Breaking any one of these means the matrix breaks all of them, and is hence not invertible.
 

\end{document}