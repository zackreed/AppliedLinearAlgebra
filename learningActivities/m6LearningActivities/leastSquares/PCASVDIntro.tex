\documentclass{ximera}
\input{../../../preamble.tex}

\author{Zack Reed}
%borrowed from selinger linear algebra
\title{Finding Directions of Maximum Spread: SVD}
\begin{document}
\begin{abstract}

\end{abstract}
\maketitle


\section*{Subspace Fitting and the SVD}

%\begin{outcome}
%  \begin{enumerate}
%  \item Compute the principal components of a matrix $A$.
%  \item Compute the centroid of a collection of data points.
%  \item Find the $k$-dimensional subspace that best approximates a
%    given collection of data points.
%  \item Find the $k$-dimensional affine subspace that best
%    approximates a given collection of data points.
%  \item Compute the total squared distance of the data points to the
%    best fit subspace (or best fit affine subspace).
%  \end{enumerate}
%\end{outcome}

In this section, we will more closely examine the geometric properties of the SVD that allow for analyses such as seen in \href{https://ximera.osu.edu/appliedlinearalgebra/c6ChapterSix/learningActivities/m6LearningActivities/leastSquares/leastSquaresApplicationVotingImages}{the Voting Records Application}.

First, here's a video overview of the ways that SVD can be used to approximate and analyze data:

\begin{center}
  \youtube{FMAffJiu5lM}
\end{center}

Whereas in the previous section we considered the columns of a matrix from the standard transformational perspective, we now consider the $n$ columns of a matrix to be data points in $\RR^m$, where the matrix $A$ is $m\times n$. 


\begin{center}
  \begin{tikzpicture}[baseline=-0.5ex]
    \draw[thin,->] (-4,0) -- (4,0);
    \draw[thin,->] (0,-4) -- (0,4);
    % \draw[thick,blue] (-4,-4/1.29) -- (4,4/1.29);
    \fill[color=red] (-2.25,-1.21) circle (0.06);
    \fill[color=red] (-1.44,-1.28) circle (0.06);
    \fill[color=red] (2.29,1.82) circle (0.06);
    \fill[color=red] (0.25,0.43) circle (0.06);
    \fill[color=red] (-1.57,-1.07) circle (0.06);
    \fill[color=red] (2.51,2.00) circle (0.06);
    \fill[color=red] (-2.53,-2.34) circle (0.06);
    \fill[color=red] (-3.04,-2.40) circle (0.06);
    \fill[color=red] (0.84,0.36) circle (0.06);
    \fill[color=red] (-3.16,-2.73) circle (0.06);
    \fill[color=red] (-2.02,-1.88) circle (0.06);
    \fill[color=red] (3.06,2.44) circle (0.06);
    \fill[color=red] (-1.40,-1.12) circle (0.06);
    \fill[color=red] (-2.09,-1.34) circle (0.06);
    \fill[color=red] (-1.23,-1.13) circle (0.06);
    \fill[color=red] (0.48,0.06) circle (0.06);
    \fill[color=red] (-1.49,-1.20) circle (0.06);
    \fill[color=red] (0.98,0.75) circle (0.06);
    \fill[color=red] (1.40,0.89) circle (0.06);
    \fill[color=red] (-0.12,0.09) circle (0.06);
    \fill[color=red] (0.88,0.40) circle (0.06);
    \fill[color=red] (-0.38,0.55) circle (0.06);
    \fill[color=red] (0.34,0.46) circle (0.06);
    \fill[color=red] (0.45,0.21) circle (0.06);
    \fill[color=red] (-0.59,-0.63) circle (0.06);
    \fill[color=red] (-0.53,-0.29) circle (0.06);
    \fill[color=red] (2.10,1.35) circle (0.06);
    \fill[color=red] (1.65,1.60) circle (0.06);
    \fill[color=red] (-3.88,-2.83) circle (0.06);
    \fill[color=red] (-0.07,-0.01) circle (0.06);
    \fill[color=red] (-0.37,-0.57) circle (0.06);
    \fill[color=red] (-0.99,-0.75) circle (0.06);
    \fill[color=red] (-2.34,-2.08) circle (0.06);
    \fill[color=red] (3.63,3.15) circle (0.06);
    \fill[color=red] (1.37,0.48) circle (0.06);
    \fill[color=red] (-0.96,-0.74) circle (0.06);
    \fill[color=red] (3.51,2.79) circle (0.06);
    \fill[color=red] (-3.33,-2.72) circle (0.06);
    \fill[color=red] (0.39,0.09) circle (0.06);
    \fill[color=red] (-1.38,-1.17) circle (0.06);
    \fill[color=red] (-0.36,-0.65) circle (0.06);
    \fill[color=red] (1.38,0.66) circle (0.06);
    \fill[color=red] (1.85,1.24) circle (0.06);
    \fill[color=red] (2.35,1.85) circle (0.06);
    \fill[color=red] (0.85,0.25) circle (0.06);
    \fill[color=red] (-0.23,0.19) circle (0.06);
    \fill[color=red] (-0.90,-0.74) circle (0.06);
    \fill[color=red] (2.50,1.31) circle (0.06);
    \fill[color=red] (-0.45,-0.71) circle (0.06);
    \fill[color=red] (0.92,0.56) circle (0.06);
    \fill[color=red] (0.97,1.21) circle (0.06);
    \fill[color=red] (-0.85,-0.74) circle (0.06);
    \fill[color=red] (-0.10,0.13) circle (0.06);
    \fill[color=red] (0.32,0.49) circle (0.06);
    \fill[color=red] (2.58,1.57) circle (0.06);
    \fill[color=red] (-0.59,-0.48) circle (0.06);
    \fill[color=red] (2.28,1.50) circle (0.06);
    \fill[color=red] (1.21,0.94) circle (0.06);
    \fill[color=red] (-0.35,-0.13) circle (0.06);
    \fill[color=red] (-1.53,-1.26) circle (0.06);
    \fill[color=red] (-2.77,-2.14) circle (0.06);
    \fill[color=red] (1.23,1.02) circle (0.06);
    \fill[color=red] (2.61,1.88) circle (0.06);
    \fill[color=red] (-0.04,-0.05) circle (0.06);
    \fill[color=red] (2.09,1.30) circle (0.06);
    \fill[color=red] (2.37,1.52) circle (0.06);
    \fill[color=red] (-2.01,-1.69) circle (0.06);
    \fill[color=red] (0.48,0.72) circle (0.06);
    \fill[color=red] (0.23,0.49) circle (0.06);
    \fill[color=red] (-1.16,-0.95) circle (0.06);
    \fill[color=red] (-0.14,-0.18) circle (0.06);
    \fill[color=red] (-1.57,-1.68) circle (0.06);
    \fill[color=red] (0.39,0.15) circle (0.06);
    \fill[color=red] (-0.24,-0.29) circle (0.06);
  \end{tikzpicture}
\end{center}

You can manipulate and work with this data by loading \texttt{+linalg/subspace\_fitting\_data.mat} in MATLAB.



Although these points are spread out in two dimensions, they seem to
be located pretty close to a 1-dimensional subspace. Probably the best
way to interpret this particular data set is to think of the points as
being ``essentially'' on a line, up to some small random errors. Note that this is the same principle we applied in the case of the voting application.

More generally, suppose we are given a collection of data points in
$n$-dimensional space, and we are looking for a $k$-dimensional
subspace that all data points are close to.  This is an important way
to make sense of high-dimensional data. For example, it would be very
difficult to visualize data in a $100$-dimensional space. However, it
we knew that the data points lie very close to a 2-dimensional
subspace, then we could project all of the points to the subspace to
obtain a 2-dimensional image of the data.

To state the problem more precisely, let us introduce the following
notation. If $W$ is a subspace of $\R^n$ and $\vect{v}\in\R^n$ is a
vector, let us write $d(\vect{v},W)$ for the shortest distance from
$\vect{v}$ to $W$ (i.e., the distance from $\vect{v}$ to $W$ along a
line that is perpendicular to $W$). Moreover, if $W$ is a subspace of
$\R^n$ and $\vect{v}_1,\ldots,\vect{v}_m\in\R^n$ are the position
vectors of $m$ points, we define the \textbf{total squared distance}%
\index{distance!total squared distance}%
\index{squared distance}%
\index{total squared distance}%
\index{subspace fitting!total squared distance} of the points to
the subspace to be the quantity
\begin{equation*}
  D = d(\vect{v}_1,W)^2 + \ldots + d(\vect{v}_m,W)^2.
\end{equation*}


Here, $d$ is the distance from the vector to the subspace, which is the same as the perpendicular vector $\vec{v}_\perp$ from the orthogonal decompositions in \href{https://ximera.osu.edu/appliedlinearalgebra/c5ChapterFive/learningActivities/m5LearningActivities/m5DotProduct/orthogonalityThree}{the previous chapter.}

This idea of distances to subspaces is crucial for data analysis and quantifying error. The following video goes over the connections between distance calculations, the SVD, and the best-fitting subspaces:

\begin{center}
  \youtube{3JvkXFatJJw}
\end{center}

You can visualize this phenomenon in $2$-dimensions with the following GeoGebra application:

\begin{center}
  \geogebra{tfds5q4w}{552}{576}
\end{center}



You can enter and alter points in Vector Lists 1 and 2 to change the data points you want to visualize with green dots. A line through the origin is visualzied in blue, and can be rotated by clicking and dragging the blue dot at the end of the direction vector $\vec{v}$. If you keep ``Emphasize Distances to Line" checked, you will see the distances $d(\vec{v}_i,L)$ from the points to the line visualized, and will also see a calculation ``Distance Square Sum='' which automatically calculates the sum of square distances \begin{equation*}
  D = d(\vect{v}_1,L)^2 + \ldots + d(\vect{v}_m,L)^2
\end{equation*} for you.

\begin{problem}
  Using the applet above, consider the following data matrix of points in $\RR^2$:

  $$\begin{bmatrix} 1 & 1.1 & 1 & -2 & -2.1 & .9\\ 
    .5 & .6 & -1 & 1 & 1.1 & -1.1\end{bmatrix}$$

  On which line given the by the following direction vectors does the data have the smallest total squared distance?

  \begin{multipleChoice}
    \choice{$\begin{bmatrix} -.982 \\ -.188 \end{bmatrix}$}
    \choice{$\begin{bmatrix} -.525 \\ -.851 \end{bmatrix}$}
    \choice[correct]{$\begin{bmatrix} -.898 \\ .441 \end{bmatrix}$}
    \choice{$\begin{bmatrix} 0 \\ 1 \end{bmatrix}$}

  \end{multipleChoice}

  Now, select ``Emphasize Projection Vectors'', which displays the lengths of the projection vectors of the data points onto the line. On which line given by the following direction vectors does the data have the largest total squared projection length sum?

  %use the same multiple choice
  \begin{multipleChoice}
    \choice{$\begin{bmatrix} -.982 \\ -.188 \end{bmatrix}$}
    \choice{$\begin{bmatrix} -.525 \\ -.851 \end{bmatrix}$}
    \choice[correct]{$\begin{bmatrix} -.898 \\ .441 \end{bmatrix}$}
    \choice{$\begin{bmatrix} 0 \\ 1 \end{bmatrix}$}
  \end{multipleChoice}

  This demonstrates a key idea that will be repeated throughout the rest of the course: Minimizing the distance to a subspace is the same as maximizing the projection length along the subspace.
\end{problem}

The goal of describing high-dimensional data using low-dimensional subspaces that ``closely approximate'' the data is akin to finding subspaces that minimize the sum of square distances from the data to the subspace, and can be generally stated as follows:


 
\begin{problem}\name{Subspace fitting problem}\label{prop:subspace-fitting}

  Given vectors $\vect{v}_1,\ldots,\vect{v}_m\in\R^n$ and given an
  integer $k\leq n$, find the $k$-dimensional subspace
  $W\subseteq\R^n$ that minimizes the total squared distance, i.e.,
  such that $D$ is as small as possible%
  \index{subspace fitting}%
  \index{fitting!subspace fitting}.
\end{problem}

\begin{example}{Subspace fitting in $\R^2$}{subspace-fitting-r2}
  Consider the following collection of points in $\R^2$:
  \begin{equation*}
    \set{
      \startmat{r}  2 \\ -3 \stopmat,
      \startmat{r} -1 \\  0 \stopmat,
      \startmat{r}  2 \\  3 \stopmat,
      \startmat{r} -6 \\ -7 \stopmat,
      \startmat{r}  6 \\ 11 \stopmat,
      \startmat{r}  0 \\ -1 \stopmat,
      \startmat{r}  1 \\  6 \stopmat,
      \startmat{r} -2 \\ -3 \stopmat,
      \startmat{r} -7 \\ -6 \stopmat
    }.
  \end{equation*}
  Find the 1-dimensional subspace that best approximates this
  collection of points. What is the total squared distance of the
  points to the subspace?

  As it turns out, the SVD provides the means of finding best-fitting subspaces. We will solve this problem after we more explicitly connect the idea of maximizing spread to an algorithm that finds the SVD. For now, we will just note that we use the SVD to answer these questions.


  The space $W$ is shown in the following illustration, along with the
  original points:
  \begin{center}
    \begin{tikzpicture}[scale=0.15]
      \draw[thin,->] (-12,0) -- (12,0);
      \draw[thin,->] (0,-12) -- (0,12);
      \draw[thin] (0,10) -- (-1,10) node[left] {$10$};
      \draw[thin] (10,0) -- (10,-1) node[below] {$10$};
      \draw[thick,blue] (-8,-12) -- (8,12);
      \fill[color=red] (2,-3) circle (0.4);
      \fill[color=red] (-1, 0) circle (0.4);
      \fill[color=red] (2, 3) circle (0.4);
      \fill[color=red] (-6, -7) circle (0.4);
      \fill[color=red] (6, 11) circle (0.4);
      \fill[color=red] (0, -1) circle (0.4);
      \fill[color=red] (1, 6) circle (0.4);
      \fill[color=red] (-2, -3) circle (0.4);
      \fill[color=red] (-7, -6) circle (0.4);
    \end{tikzpicture}
  \end{center}
  Of course, in real life the data is much more complex than in this example

\end{example}

Let's state another problem, in $\RR^3$, that we will return to and again use the SVD to solve. Because data can be very high dimensional and quite complex, it is important to observe that we have many dimensions of possibility on which to explore best-fit subspaces. 


\begin{example}\name{Subspace fitting in $\R^3$}\label{ex:subspace-fitting-r3}
  Consider the following collection of points in $\R^3$:
  \begin{equation*}
    \set{
      \startmat{r} -7 \\ 4 \\ 5 \stopmat,
      \startmat{r} 0 \\ 3 \\ 3 \stopmat,
      \startmat{r} 2 \\ -5 \\ -4 \stopmat,
      \startmat{r} 10 \\ -4 \\ 1 \stopmat,
      \startmat{r} -2 \\ 5 \\ 4 \stopmat,
      \startmat{r} -8 \\ -1 \\ -5 \stopmat,
      \startmat{r} 5 \\ 4 \\ 2 \stopmat,
      \startmat{r} -6 \\ 9 \\ 6 \stopmat,
      \startmat{r} 9 \\ -6 \\ 3 \stopmat,
      \startmat{r} -2 \\ -7 \\ -8 \stopmat
    }.
  \end{equation*}
  %\begin{enumialphparenastyle}
    \begin{enumerate}
    \item Find the 1-dimensional subspace that best approximates this
      collection of points.
    \item Find the 2-dimensional subspace that best approximates this
      collection of points.
    \item What is the 3-dimensional subspace that best approximates this
      collection of points?
    \end{enumerate}
  %\end{enumialphparenastyle}
  In each case, what is the total squared distance of the points to
  the subspace?
\end{example}

Now that we've asked questions begging answers, let's explore how the SVD will be used to provide answers.

\section*{Maximizing Spread, Minimizing Distance, Computing the SVD}

We now hone in on how the way that the SVD is constructed, which brings together a few key ideas including why the SVD gives us the best-fitting (in the sense of distance minimization) subspaces. 

The key is the interplay between \emph{minimizing square distances} and \emph{maximizing projection spread}. These are crucially linked together, which allows us to leverage an algorithm that finds direction of maximum projection spread to construct the SVD. 

\subsection*{Maximizing Spread and Minimizing Square Distances}

Let's return to the GeoGebra applet, but ask more targeted questions. First, consider the applet with just the first vector in list 1, $\begin{bmatrix}
  1\\.5
\end{bmatrix}$ (set Vector List 2 to just the zero vector).

\begin{center}
  \geogebra{tfds5q4w}{552}{576}
\end{center}

As you rotate the unit direction vector $\vec{v}$, if you select ``Emphasize Distances to Line", the applet will show the orthogonal projection of the data point to the line. If you select ``Emphasize Projection Vectors", the applet will de-emphasize the distance to the line and instead emphasize the length of the projection vector along the line. 

Which of the following statements is true of the relationship between the distances to and projections along the possible lines?



\begin{problem}

  \begin{selectAll}
  
    \choice{The $\vec{v}$ whose line minimizes the data distance is orthogonal to the vector $\vec{w}$ that maximizes the projection length of the data.}
    \choice{There is only one $\vec{v}$ that minimizes the distance of the data to the line.}
    \choice[correct]{The $\vec{v}$ whose line minimizes the data distance is parallel to the vector $\vec{w}$ that maximizes the projection length of the data.}

  \end{selectAll}

  \begin{feedback}
  
    With one data point, the distance-minimizing line crosses through the data point itself, and there are two vectors that acheive this (or rather, one vector and its negative). In this case $\pm \begin{bmatrix}
      .893\\.45
    \end{bmatrix}$. All other directions increase the distance from the data to the line. If you switch to ``Emphasize Projeciton Vectors'', you'll note that the length of the projection vector equals $1.25$ when the data point is on the line, and then only decreases when the data point is off of the line, being zero when the projection distance vector is orthogonal to the line. 

    As such, the line that minimizes the projection distance is also the line that maximizes the length of the projection vector.

    In fact, this is due to the Pythagorean Theorem. The norm of the data point vector is fixed, and so the data point vector, the distance to the projection, and the length of the projection make a right triangle.

    \begin{center}
      \begin{tikzpicture}
       \draw[line width=2pt,red,-stealth](3.6, 0.8)--(2,4);
       \node[red] at (3.5, 2.2)   (a) {$\vec{v}_{\text{distance}}$};
      \node[red] at (1.5, -0.95)   (b) {$\vec{v}_{\text{projection}}$};
       \node[] at (5, 1)   (c) {$l$};
       \node[blue] at (-0.8, 1.9)   (d) {$\vec{v}=\vec{v}_{\text{distance}}+\vec{v}_{\text{projection}}$};
        \draw [-,line width=1pt]  (-3,-2.5)--(5, 1.5);
     \draw[line width=2pt,blue,-stealth](-2, -2)--(2,4);
     \draw[line width=2pt,red,-stealth](-2, -2)--(3.6, 0.8);
     \end{tikzpicture}
       
     \end{center}

     Because the projection is orthogonal, we always have this right triangle, so 

     $$\left\|\vec{v}\right\|=\left\|\vec{v}_{\text{distance}}\right\|+\left\|\vec{v}_{\text{projection}}\right\|,$$

     meaning increasing the proejction or distance length will necessarily decrease the other.

  \end{feedback}



\end{problem}



In fact, this dynamic between the projection lengths and the projection distances is true for any number of data points, not just one data point, since each data point makes a right triangle via orthogonal projection along any line (unless the point lies on the line).

So, if \begin{equation*}
  D = d(\vect{v}_1,W)^2 + \ldots + d(\vect{v}_m,W)^2
\end{equation*} represents the sum of square projection distances, then we use the shorthand $p(\vect{v},W)^2$ for the square projection length of a vector onto a subspace and can say that the sum of square projection lengths \begin{equation*}
  P = p(\vect{v}_1,W)^2 + \ldots + p(\vect{v}_m,W)^2
\end{equation*} increases as $D$ decreases and vice versa. We'll call $P$ the \emph{spread} of the data along the subspace $W$.

More pointedly, we can turn this into a useful theorem for finding the SVD.

\begin{theorem}\name{Maximizing Spread and Minimizing Distance}
  If $A$ is an $m\times n$ matrix, then a subspace $W$ of $\RR^m$ that maximizes the spread of the columns of $A$ also minimizes the distances of the columns of $A$ to $W$ in $\RR^m$.
\end{theorem}

$P$ actually relates very closely to an important norm on {\bf matrices}, called the \emph{Frobenius Norm}.

\begin{definition}
  Let $A$ be an $m\times n$ matrix with columns denoted $a_i$. Then $\sqrt{P}$ is the Frobenius Norm of $A$, denoted $\left\|A\right\|_F$. 

  In other words, $\left\|A\right\|_F=\sqrt{\sum_{i=1}^n\left\|a_i\right\|}$.
\end{definition}

\subsection*{An algorithm for computing the SVD}

This brings us to one way to compute the SVD! We will not be doing this ourselves explicitly, but it is always important to know how useful tools such as the SVD are built so that we can understand why they work the way they do. 

There are four important features of singular values and singular vectors that contribute to their construction: \begin{enumerate}
  \item The singular vectors represent the directions of maximum projection spread, $P$,
  \item The singular values quantify that spread, and 
  \item The singular vectors form orthonormal bases of $\RR^m$ and $\RR^n$
  \item The SVD factors $A$ into $U*S*V^T$.
\end{enumerate}

Here's a video going over the following algorithm, explaining the connections between the algorithm and the features of the SVD:

\begin{center}
  \youtube{yv_3fuu2XAQ}
\end{center}

With these features in mind, the SVD production algorithm is as follows.

\begin{theorem}{SVD-Producing Algorithm}

  Given a $m\times n$ matrix $A$, one can find the SVD of $A$ by taking the following steps:

  \begin{enumerate}
    \item Find a direction vector $\vec{v}_1$ that maximizes $\left\|A\vec{v}\right\|$ over all unit vectors $\vec{v}$. $\vec{v}_1$ is the first right singular vector, and call $\sigma_1=\left\|A\vec{v}_1\right\|$ the first singular value. 
    \item Find the second right singular vector $\vec{v}_2$ by maximizing $\left\|A\vec{v}\right\|$ over all unit vectors $\vec{v}$ that are orthogonal to $\vec{v}_1$. The second singular value is $\sigma_2=\left\|A\vec{v}_2\right\|$. 
    \item Repeat until you have found a sufficient number of right singular vectors and singular values.
    \item Form the left singular vectors from the matrix products $\vec{u}_i=\frac{A\vec{v}_i}{\sigma_i}$.
  \end{enumerate}

  There are other ways to compute the SVD more efficiently and with connections to other concepts to be explored in future modules, but this first cosntruction method emphasizes that the singular vectors are built to maximize the spread of the vectors along the projected subspaces, that the singular values quantify the spread along the projected subpsaces, and that the singular vectors form orthonormal bases of $\RR^m$ and $\RR^n$. 

  For a more detailed analysis of this process, see Blum, Hopcroft, and Kannan's \emph{Foundations of Data Science} (2015). A free pdf version can be found \href{https://www.cs.cornell.edu/jeh/book.pdf}{here}.

\end{theorem}

\begin{remark}
  Unpacking the consequences of this algorithm, and the consequences of the SVD, is complex so let's review the important parts:
  \begin{enumerate}
    \item $A$ is an $m\times n$ matrix, meaning we have $n$ $m$-dimensional data vectors. 
    \item We want to project the data onto a $k$-dimensional subspace that maximizes the spread of the data along the subspace.
    \item We first find the direction $\vec{v}_1$ in $\RR^n$ such that $A\vec{v}_1$ maximizes the spread of the data along a line in $\RR^m$. $vec{v}_1$ is the first right singular vector of $A$, the line $A\vec{v}_1=\sigma_1\vec{u}_1$ is the first left singular vector of $A$, with direction $\vec{u}_1$ and the amount of spread $\sigma_1$.
    \item We then repeat, finding more direction vectors $\vec{v}_2,\vec{v}_3,\ldots$ that are orthogonal to the previous vectors and progressively maximize the spread of the data along the subspaces they span.
    \item The end result is an orthonormal basis $U$ of $\RR^m$ and an orthonormal basis $V$ of $\RR^n$ that span the subspaces that best fit the data, and the singular values $\sigma_1,\sigma_2,\ldots$ quantify the spread of the data along the subspaces.

  \end{enumerate}
\end{remark}

\begin{remark}\name{Quantifying Error}

  Importantly, the singular values tell you the spread along the best-fitting subspaces \emph{and} the error in the approximation. If you have $n$-dimensional data and use a $k$-dimensional approximation using the SVD, the sum of the first $k$ singular vectors tells you the amount of spread along that subspace. The remaining $n-k$ vectors, however, tell you the amount of spread of the data {\bf NOT} in the subspace, which is exactly the error of the approximation.

  So, if you have a matrix $A$ and you use a $k$-dimensional approximation using $\vec{u}_1,\ldots,\vec{u}_k$ and $\sigma_1,\ldots,\sigma_k$, the spread of the approximation data is given by $\sigma_1^2+\ldots+\sigma_k^2$, and the error of the approximation is given by $\sigma_{k+1}^2+\ldots+\sigma_n^2$.
\end{remark}

Now, let's use the SVD to solve our two problems.

\begin{problem}
  The matrix $A$ whose columns are the given points is
  \begin{equation*}
    A = \begin{bmatrix}
      2 & -1 & 2 & -6 & 6 & 0 & 1 & -2 & -7 \\
      -3 & 0 & 3 & -7 & 11 & -1 & 6 & -3 & -6
    \end{bmatrix}.
  \end{equation*}
  We compute the SVD of $A$ to find the best-fitting 1-dimensional subspace:

  \begin{verbatim}
    A = [2 -1 2 -6 6 0 1 -2 -7; -3 0 3 -7 11 -1 6 -3 -6];
    [U,S,V] = svd(A);
  \end{verbatim}

  Notice that $V$ is a $9\times 9$ matrix and $U$ is a $2\times 2$ matrix. Because we want to fit the data in $\RR^2$ to a 1-dimensional subspace, we will use the first left singular vector $\vec{u}_1$ as the direction of the best-fitting line. Because $\vec{v}_1$ gives the direction in $\RR^9$ over which the spread along the line is maximized, and we want to project onto the line spanned by $\vec{u}_1$, we need to project the data by the product $\vec{u}_1\vec{v}_1^T$. That projection would not account for the spread of the data, however, so we also need to multiply $\vec{v}_1$ by the singular value $\sigma_1$ to account for the spread. This means the best-fitting line is given by the product $\vec{u}_1\sigma_1\vec{v}_1^T$.

  We do this in MATLAB, pulling from $U$ and $V$ and $S$, by the following code:

  \begin{verbatim}
    line = U(:,1)*S(1,1)*V(:,1)';
  \end{verbatim}

  Plotting the data and the projection in MATLAB should reveal the following diagram:

  \begin{verbatim}
linalg.simple_plot_cloud(line,A)
hold on
linalg.simple_plot_line([0;0],U(:,1))
axis equal
hold off
  \end{verbatim}

  \begin{center}
    \begin{tikzpicture}[scale=0.15]
      \draw[thin,->] (-12,0) -- (12,0);
      \draw[thin,->] (0,-12) -- (0,12);
      \draw[thin] (0,10) -- (-1,10) node[left] {$10$};
      \draw[thin] (10,0) -- (10,-1) node[below] {$10$};
      \draw[thick,blue] (-8,-12) -- (8,12);
      \fill[color=red] (2,-3) circle (0.4);
      \fill[color=red] (-1, 0) circle (0.4);
      \fill[color=red] (2, 3) circle (0.4);
      \fill[color=red] (-6, -7) circle (0.4);
      \fill[color=red] (6, 11) circle (0.4);
      \fill[color=red] (0, -1) circle (0.4);
      \fill[color=red] (1, 6) circle (0.4);
      \fill[color=red] (-2, -3) circle (0.4);
      \fill[color=red] (-7, -6) circle (0.4);
    \end{tikzpicture}
  \end{center}

  From this, the projections of the data onto the best fitting line are: 

  $$\vec{p}_1=\begin{bmatrix}
    \answer[tolerance=.01]{-.7692}\\-1.1538
  \end{bmatrix},\vec{p}_2=\begin{bmatrix}
    -.3077\\\answer[tolerance=.01]{-.4615}
  \end{bmatrix},\vec{p}_3=\begin{bmatrix}
    \answer[tolerance=.01]{2}\\3
  \end{bmatrix},\vec{p}_4=\begin{bmatrix}
    -5.0769\\\answer[tolerance=.01]{-7.6154}
  \end{bmatrix},\vec{p}_5=\begin{bmatrix}
    \answer[tolerance=.01]{6.9231}\\10.3846
  \end{bmatrix},$$
  
  $$\vec{p}_6=\begin{bmatrix}
    -.4615\\\answer[tolerance=.01]{-.6923}
  \end{bmatrix},\vec{p}_7=\begin{bmatrix}
    3.0769\\\answer[tolerance=.01]{4.6154}
  \end{bmatrix},\vec{p}_8=\begin{bmatrix}
    \answer[tolerance=.01]{-2}\\-3
  \end{bmatrix},\vec{p}_9=\begin{bmatrix}
    -4.9231\\\answer[tolerance=.01]{-7.3846}
  \end{bmatrix}.$$

  You can find the total squared distance of the points to the subspace in a couple of ways. First, take the difference between \texttt{A} and \texttt{line}, square each entry, and sum the entries. If you do this, you get a total squared distance of $\answer{27}$ (Use the following code.):

  \begin{verbatim}
    diff=A-line
    total_length=0
    for i=1:9
        len=norm(diff(:,i))^2;
        total_length=total_length+len;
    end
    total_length
  \end{verbatim}

  Alternatively, $S$ encodes the singular values, the sum of the spread in the two directions of $\RR^2$. $\sigma_1$ is the spread along the best-fitting line, so $\sigma_2$ is the spread orthogonal to the line, so $\sigma_2^2$ is the total squared distance of the points to the subspace. In this case, $\sigma_2^2=\answer{27}$.

  So we see that the singular values tell us both information about the spread along subspaces as well as the total squared distance of the data to the subspaces!
\end{problem}

Now, similarly for our problem using data in $\RR^3$, we can see how this plays out with multiple dimensions of best-fit subspaces:

\begin{problem}
  The matrix $B$ whose columns are the given data in $\RR^3$ is
  \begin{equation*}
    B = \begin{bmatrix}
      -7 & 0 & 2 & 10 & -2 & -8 & 5 & -6 & 9 & -2 \\
      4 & 3 & -5 & -4 & 5 & -1 & 4 & 9 & -6 & -7 \\
      5 & 3 & -4 & 1 & 4 & -5 & 2 & 6 & 3 & -8
    \end{bmatrix}.
  \end{equation*}
  We compute the SVD of $B$ to find the best-fitting 1-dimensional subspace:

  \begin{verbatim}
    B = [-7 0 2 10 -2 -8 5 -6 9 -2; 
    4 3 -5 -4 5 -1 4 9 -6 -7; 
    5 3 -4 1 4 -5 2 6 3 -8];
    [U,S,V] = svd(B);
  \end{verbatim}

  Notice that $V$ is a $10\times 10$ matrix and $U$ is a $3\times 3$ matrix. Because we want to fit the data in $\RR^3$ to a 1-dimensional subspace, we will use the first left singular vector $\vec{u}_1$ as the direction of the best-fitting line. Because $\vec{v}_1$ gives the direction in $\RR^{10}$ over which $B\vec{v}$ maximally spreads the data along the line, and we want to project onto the line spanned by $\vec{u}_1$, we need to project the data by the product $\vec{u}_1\vec{v}_1^T$. 
  
  That projection would not account for the amount of spread of the data, however, so we also need to multiply $\vec{v}_1$ by the singular value $\sigma_1$ to account for the spread. This means the best-fitting line is given by the product $\vec{u}_1\sigma_1\vec{v}_1^T$.

  We do this in MATLAB, pulling from $U$ and $V$ and $S$, by the following code:

  \begin{verbatim}
    line = U(:,1)*S(1,1)*V(:,1)';
  \end{verbatim}

  Plotting the data and the projection in MATLAB should reveal the following diagram:

  \begin{verbatim}
linalg.simple_plot_cloud(line,B)
hold on
linalg.simple_plot_line([0;0;0],U(:,1))
axis equal
hold off
  \end{verbatim}

  The projections onto the best-fitting line are:

$$\vec{p}_1=\begin{bmatrix}
    \answer{-6}\\6\\\answer{3}
  \end{bmatrix},\vec{p}_2=\begin{bmatrix}
    -2\\\answer{2}\\1
  \end{bmatrix},\vec{p}_3=\begin{bmatrix}
    \answer{4}\\-4\\-2
  \end{bmatrix},\vec{p}_4=\begin{bmatrix}
    6\\\answer{-6}\\-3
  \end{bmatrix},\vec{p}_5=\begin{bmatrix}
    \answer{-4}\\4\\2
  \end{bmatrix},\vec{p}_6=\begin{bmatrix}
    -2\\\answer{2}\\\answer{1}
  \end{bmatrix},\vec{p}_7=\begin{bmatrix}
    0\\0\\\answer{0}
  \end{bmatrix},$$
  
  $$\vec{p}_8=\begin{bmatrix}
    \answer{-8}\\8\\4
  \end{bmatrix},\vec{p}_9=\begin{bmatrix}
    6\\\answer{-6}\\-3
  \end{bmatrix},\vec{p}_{10}=\begin{bmatrix}
    \answer{4}\\-4\\-2
  \end{bmatrix}.$$

  Keeping in mind the relationship between the total spread along the line and the singular values, the total squared distance from the data to the line is $\answer{333}$. (Hint: Use the squares of the singular values not used in the projection.)

  Now, for the 2-dimensional subspace, we can use the first two left singular vectors to find the best-fitting plane. Here, we project onto the first two left singular vectors, $\vec{u}_1$ and $\vec{u}_2$, scaling by the first two singular values, and along the directions dicated by the first two right singular vectors, $\vec{v}_1$ and $\vec{v}_2$.

  \begin{verbatim}
    plane = U(:,1:2)*S(1:2,1:2)*V(:,1:2)';
  \end{verbatim}

Plotting the data and the projection in MATLAB should reveal the following plane:

  \begin{verbatim}
linalg.simple_plot_cloud(plane,B)
hold on
linalg.simple_plot_plane([0;0;0],U(:,1:2))
axis equal
hold off
  \end{verbatim}

  From this, the projections of the data onto the best fitting plane are: 

  $$\vec{p}_1=\begin{bmatrix}
    \answer{-6}\\\answer{6}\\3
  \end{bmatrix},\vec{p}_2=\begin{bmatrix}
    0\\\answer{3}\\\answer{3}
  \end{bmatrix},\vec{p}_3=\begin{bmatrix}
    \answer{2}\\-5\\-4
  \end{bmatrix},\vec{p}_4=\begin{bmatrix}
    \answer{10}\\-4\\1
  \end{bmatrix},\vec{p}_5=\begin{bmatrix}
    -2\\\answer{5}\\\answer{4}
  \end{bmatrix},\vec{p}_6=\begin{bmatrix}
    \answer{-8}\\-1\\-5
  \end{bmatrix},\vec{p}_7=\begin{bmatrix}
    4\\\answer{2}\\4
  \end{bmatrix},$$
  
  $$\vec{p}_8=\begin{bmatrix}
    \answer{-6}\\9\\\answer{6}
  \end{bmatrix},\vec{p}_9=\begin{bmatrix}
    10\\\answer{-4}\\1
  \end{bmatrix},\vec{p}_{10}=\begin{bmatrix}
    \answer{-2}\\-7\\\answer{-8}
  \end{bmatrix}.$$

  Again, keeping in mind that the singular values tell you both the amount of spread and the total distance to the subspace, we get that the total square distance from the plane to the data is $\answer{27}$.

  Finally, for the 3-dimensional subspace, we actaully  have the entire space. Since the data is in $\RR^3$, the best-fitting 3-dimensional subspace is $\RR^3$ itself. If the data were in $\RR^4$, we might be able to have a 3-dimensional subspace that best fits the data, but that we could not visualize this.
  
\end{problem}

\end{document}