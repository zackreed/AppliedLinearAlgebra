\documentclass{ximera}

\input{../../../preamble.tex}

\author{Zack Reed}
%borrowed from selinger linear algebra
\title{SVD Application: Least Squares Approximations}
\begin{document}
\begin{abstract}

    One application of IPSs is least squares.

\end{abstract}
\maketitle


\section{Least Squares and the Moore-Penrose Pseudo-Inverse}
\label{sec:least-squares}

%\begin{outcome}
%  \begin{enumerate}
%  \item Find least squares approximations for a system of equations.
%  \item Find best fit lines and parabolas for a set of data points.
%  \end{enumerate}
%\end{outcome}

One immediate application of the SVD is a new ability to still ``solve'' inconsistent systems of equations $A\vect{v}=\vect{b}$. Though no true exact solution is possible, we would still like
to find a ``best'' solution. For example, consider the following system
of equations.
\begin{equation*}
  \begin{array}{rcrcl}
    0.1 x &+& 0.2 y &=& 0.3, \\
    0.2 x &+& 0.5 y &=& 0.7, \\
    0.7 x &+& 0.2 y &=& 0.9. \\
  \end{array}
\end{equation*}
This system has the solution $(x,y)=(1,1)$, so it is clearly
consistent. Now imagine that we introduce some small inaccuracies into
the equations. The inaccuracies might perhaps be due to round-off
errors, or due to measurement errors if the coefficients are obtained
from experimental data. We might end up with the following system of
equations:
\begin{equation*}
  \begin{array}{rcrcl}
    0.1000001 x &+& 0.2 y &=& 0.3, \\
    0.2 x &+& 0.5 y &=& 0.7, \\
    0.7 x &+& 0.2 y &=& 0.9. \\
  \end{array}
\end{equation*}
Except for a tiny error in one of the coefficients, this is the same
system of equations as before. We would expect that such a small error
does not affect the result much. However, this last system of
equations is inconsistent; it has no solutions at all. You can see
this by observing that $(x,y)=(1,1)$ is still the unique solution to
the last two equations; substituting this into the first equation, we
get $0.3000001 = 0.3$, which almost, but not exactly, true. What we
would like to find in a situation like this is an ``approximate
solution'', i.e., numbers $x$ and $y$ such that each of the three
equations ``almost'' holds. We can formulate the problem more
precisely as follows:

\begin{problem}{Least squares approximation problem}{least squares-approximation}
  Given a (possibly inconsistent) system of equations
  $A\vect{v}=\vect{b}$, find $\vect{v}$ such that
  \begin{equation*}
    \norm{A\vect{v} - \vect{b}}
  \end{equation*}
  is as small as possible. We call such a vector $\vect{v}$ a
  \textbf{least squares approximation}%
  \index{least squares approximation}%
  \index{approximation!least squares} for the system of equations.
\end{problem}

To see why this is called a ``least squares'' approximation, consider
a system of equations
\begin{equation*}
  \begin{array}{c@{~}c@{~}c@{~}c@{~}c@{~}c@{~}c}
    a_{11}x_1 &+& \cdots &+& a_{1n}x_n &=& b_1, \\
    a_{21}x_1 &+& \cdots &+& a_{2n}x_n &=& b_2, \\
    \cdots  \\
    a_{m1}x_1 &+& \cdots &+& a_{mn}x_n &=& b_m. \\
  \end{array}
\end{equation*}
We can write this in matrix form $A\vect{v}=\vect{b}$, where
\begin{equation*}
  A = \startmat{ccc}
    a_{11} & \cdots & a_{1n} \\
    \vdots & \ddots & \vdots \\
    a_{m1} & \cdots & a_{mn} \\
  \stopmat,
  \quad
  \vect{b} = \startmat{c} b_1 \\ \vdots \\ b_m \stopmat,
  \quad\mbox{and}\quad
  \vect{v} = \startmat{c} x_1 \\ \vdots \\ x_n \stopmat.
\end{equation*}
Then
\begin{equation*}
  A\vect{v} - \vect{b} ~=~
  \startmat{c@{~}c@{~}c@{~}c@{~}c@{~}c@{~}c}
    a_{11}x_1 &+& \cdots &+& a_{1n}x_n &-& b_1 \\
    \cdots \\
    a_{m1}x_1 &+& \cdots &+& a_{mn}x_n &-& b_m \\
  \stopmat,
\end{equation*}
and therefore
\begin{equation*}
  \norm{A\vect{v} - \vect{b}}^2 ~=~
  (a_{11}x_1 + \cdots + a_{1n}x_n - b_1)^2 + \ldots
  + (a_{m1}x_1 + \cdots + a_{mn}x_n - b_m)^2.
\end{equation*}
Therefore, minimizing $\norm{A\vect{v}-\vect{b}}$ is the same as
minimizing the sum of the squares of the errors of all the equations,
where the error of each equation is defined to be the difference
between its left-hand side and right-hand side.

Luckily, we just spent quite a bit of time developing methods for minimizing the sum of squared errors between data in $\RR^m$ and possible best-fitting subspaces in $\RR^m$!

Moreover, when the system is consistent, a least squares approximation also turns out to be the true solution! Note that $\norm{A\vect{v}-\vect{b}}=0$ if and only if
$A\vect{v}=\vect{b}$. Therefore, if the system of equations
$A\vect{v}=\vect{b}$ is consistent, then its least squares
approximations are exactly the solutions of the system of equations in
the usual sense.

You might recall from earlier chapters that solving systems of equations (for which one solution exists) is equivalent to finding an inverse of the coefficients matrix of the system. 


Consider the following system, for which we found a solution \href{https://ximera.osu.edu/appliedlinearalgebra/c3ChapterThree/learningActivities/m3LearningActivities/modThreeSystems/matricesAreSystemsOfEquations3}{in Chapter 3}. 



$$\begin{matrix}
  2x& -&y&=&-10\\
  3x & +&2y&= &-8
\end{matrix}$$



This system can be represented in matrix format as $A\vec{x}=\vec{b}$, with the coefficient matrix $A=\begin{bmatrix}
  2 & -1\\ 3 & 2
\end{bmatrix}$ and the constant vector $\vec{b}=\begin{bmatrix}
  -10 \\-8
\end{bmatrix}$. 

Since $A$ is invertible, meaning $A^{-1}*A=I$ (where $A^{-1}$ is the inverse matrix), we can solve for a solution vector $\vec{x}$ by left-multiplicaiton by $A^{-1}$, 

$$\vec{x}=A^{-1}*A*\vec{x}=A^{-1}\vec{b}.$$

This yeilds the solution 



\begin{verbatim}
  
  A=[2 -1;
  3 2]
  A_inv=inv(A)
  b=[-10;-8]
  x=A_inv*b

\end{verbatim}


to yield $\vec{x}=\begin{bmatrix}
  -4\\2
\end{bmatrix}$. You can similarly find an inverse by $\texttt{x=A\\b}$, without explicitly solving for $A^{-1}$.

Now, let's use the SVD as a new way to construct $A^{-1}$ that will also give us a means of finding a least-squares solution when an inverse does not exist. 

Computing the SVD of $A$ gives $U=\begin{bmatrix}
  -.3827 & -.9239\\-.9239& -.3827
\end{bmatrix}$, $V=\begin{bmatrix}
  -.9239 & -.3827\\
  -.3827 & .9239
\end{bmatrix}$, and singular values $\sigma_1=3.8284$ and $\sigma_2=1.8284$.

Because $A=U*S*V^T$, we can re-state the matrix equation as 

$$U*S*V^T\vec{x}=\vec{b}.$$

You might recall that $U$ and $V$ are orthogonal matrices. This means that $U*U^T=U^T*U=I$ and $V*V^T=V^T*V=I$. 

Since the product of two diagonal matrices is diagonal, and the product of the diagonal entries, theoretically we can also invert $S$ by multiplying by a diagonal matrix of $S^\dagger=\begin{bmatrix}
  1/\sigma_1 & 0\\0 & 1/\sigma_2
\end{bmatrix}$. We need to be careful, however, this only works because $\sigma_1$ and $\sigma_2$ are nonzero. 

So, we can solve for $\vec{x}$ by the following right-multiplications:

$$S*V^T\vec{x}=U^T\vec{b},$$

$$V^T\vec{x}=S^\dagger*U^T\vec{b},$$

and

$$\vec{x}=V*S^\dagger*U^T\vec{b}.$$

Moreover, the product $V*S^\dagger*U^T$ equals the inverse matrix $A^{-1}$. 

Even more importantly, however, a slight adjustment to this process also gives us what's called the \emph{Moore-Penrose pseudo-inverse}, and the ability to find least squares approximations of solutions to systems of equations. 



\begin{example}{Least squares approximation}{least-squares-approximation}
  Find the least squares approximation for the system of equations
  \begin{equation*}
    \begin{array}{r@{~}c@{~}r@{~}c@{~}r@{~}c@{~}r}
      2x &+& 2y &+& 2z &=&  1, \\
      x  &-&  y &-&  z &=& -2, \\
      -x &-&  y &+& 2z &=&  4, \\
      2x &+& 2y &-&  z &=& -8. \\
    \end{array}
  \end{equation*}
\end{example}

\begin{solution}
  We first write the system in matrix form $A\vect{v}=\vect{b}$, where
  \begin{equation*}
    A = \startmat{rrr}
      2 & 2 & 2 \\
      1 & -1 & -1 \\
      -1 & -1 & 2 \\
      2 & 2 & -1 \\
    \stopmat
    \quad\mbox{and}\quad
    \vect{b} = \startmat{r} 1 \\ -2 \\ 4 \\ -8 \stopmat.
  \end{equation*}


First, note that $A$ is a rectangular matrix, and hence is automatically not invertible. Moreover, there are more equations than variables in the system, and so there certainly is not guaranteed to be one exact solution.

Let's use the SVD to overcome these hurdles. 

Finding the SVD of $A$ yields the matrices

\[
U =
\begin{bmatrix}
-0.6667 & -0.6159 & -0.2551 & -0.3333 \\
0.0000 & 0.3827 & -0.9239 & 0.0000 \\
0.3333 & -0.6159 & -0.2551 & 0.6667 \\
-0.6667 & 0.3080 & 0.1276 & 0.6667
\end{bmatrix}
\]

\[
S =
\begin{bmatrix}
4.2426 & 0      & 0 \\
0      & 3.2004 & 0 \\
0      & 0      & 1.3257 \\
0      & 0      & 0
\end{bmatrix}
\]

and

\[
V =
\begin{bmatrix}
-0.7071 & 0.1196 & -0.6969 \\
-0.7071 & -0.1196 & 0.6969 \\
-0.0000 & -0.9856 & -0.1691
\end{bmatrix}
\].

$U$ and $V$ are always orthogonal, which is nice, but $S^\dagger$ will need more care. We can still ``invert'' $S$ by taking the reciprocal singular values, but $S$ is not square and so we can't just make a new diagonal matrix inverting the entries. We can, however take $S^T$ and invert the entries to make $S^\dagger$. This will also help keep the dimensions of the matrices straight as we attempt to solve for $\vec{x}$. 

If you do the following: 

\begin{verbatim}
  S_dagger = S' 
  for i = 1:min(size(S))
    S_dagger(i, i) = 1 / S(i, i);
  end
  S_dagger
\end{verbatim}

you get $S^\dagger=\begin{bmatrix}
  .2357 & 0      & 0      & 0 \\
  0      & .3125 & 0      & 0 \\
  0      & 0      & .7543 & 0
  \end{bmatrix}$. 

  Then, we can do the same steps as before, however let's keep track of the dimensions as we go. 

  First, $\vec{b}$ is a $4\times 1$ vector, and $U$ is a $4\times4$ matrix. So multiplication by $U^T$ makes sense.

$$S*V^T\vec{x}=U^T\vec{b}$$

Now, $S^\dagger$ is a $3\times 4$ matrix, $S^\dagger*S=I$, where $I$ is the $3\times 3$ identity matrix. This also turns the right side of the equation into a $3\times 1$ vector instead of a $4\times 1$ vector. 

$$V^T\vec{x}=S^\dagger*U^T\vec{b},$$

Finally, $V$ is a $3\times 3$ vector, so we end with a least squares solution vector

$$\vec{x}=V*S^\dagger*U^T\vec{b}.$$

This makes sense with the system of equations, however, since there were only three variables in the systems. 

Doing this in MATLAB, we get $\vec{x}=\begin{bmatrix}
  -1\\-1\\2
\end{bmatrix}$. 

Let's check how far off our system solution is. We want $A\vec{x}=\vec{b}$. 

If we do $A*\begin{bmatrix}
  -1\\-1\\2
\end{bmatrix}$, we get $\overline{\vec{b}}=\begin{bmatrix}
  0\\-2\\6\\-6
\end{bmatrix}$. In each coordinate, we were not too far off from $\vec{b}$. 

We can also check the approximation error by checking the norm of $A\vec{x}-\vec{b}$. This is a least squares check on the error, since the norm $\norm{A\vect{x}-\vect{b}}$ involves the sum of square errors. 

In our case, $\norm{A\vect{x}-\vect{b}}=3$. While we typically want much smaller errors than $3$, the use of SVD guarantees that we have found the solution with the smallest square error out of all possible $3D$ solution vectors. 

\end{solution}

\begin{remark}

  The construction of $S^\dagger$ thus far relies on $A$ having nonzero singular values. In the case of zero singular values, you only invert the nonzero diagonal elements, leaving the rest to be zero. 

  MATLAB code such as the following:

  \begin{verbatim}
    % Invert the nonzero diagonal entries
    for i = 1:min(size(S))
        if S(i, i) ~= 0
            S_dagger(i, i) = 1 / S(i, i);
        else
            S_dagger(i, i) = 0; % Keep zero for zero diagonal entries
        end
    end
  \end{verbatim}

  can achieve this. Also, MATLAB has an built in \texttt{pinv(A)} command to autimatically calculate the Moore-Penrose pseudo-inverse of a matrix $A$.

\end{remark}



\end{document}