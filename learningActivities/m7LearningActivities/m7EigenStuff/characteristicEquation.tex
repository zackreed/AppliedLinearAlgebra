\documentclass{ximera}
\input{../../../preamble.tex}

\author{Zack Reed}
%borrowed from selinger linear algebra
\title{The Characteristic Equation}
\begin{document}
\begin{abstract}

\end{abstract}
\maketitle


\section*{The Characteristic Equation}

    
Let $A$ be an $n \times n$ matrix.  In \href{https://ximera.osu.edu/appliedlinearalgebra/c7ChapterSeven/learningActivities/m7LearningActivities/m7EigenStuff/describingEigenstuff}{the previous section} we learned that the eigenvectors and eigenvalues of $A$ are vectors $\vec{x}$ and scalars $\lambda$ that satisfy the equation 
$A \vec{v} = \lambda \vec{v}$

We listed a few reasons why we are interested in finding eigenvalues and eigenvectors, but we did not give any process for finding them.  In this section we will focus on a process which can be used for small matrices.  %For larger matrices, the best methods we have are iterative methods, and we will explore some of these in \href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/EIG-0070/main}{The Power Method and the Dominant Eigenvalue}.
    
For an $n \times n$ matrix, we will see that the eigenvalues are the roots of a polynomial called the \dfn{characteristic polynomial}.  So finding eigenvalues is equivalent to solving a polynomial equation of degree $n$.  Finding the corresponding eigenvectors turns out to be a matter of computing the null space of a matrix, as the following exploration demonstrates.
    
\begin{exploration}\label{exp:slowdown}
If a vector $\vec{x}$ is an eigenvector satisfying Equation (\ref{def:eigen}), then clearly it also satisfies  $A\vec{x}-\lambda \vec{x} =$ \wordChoice{\choice{0} \choice[correct]{$\vec{0}$}}.
    
It seems natural at this point to try to factor.  We would love to ``factor out'' $\vec{x}$.  Here is the procedure:
\begin{align*}
A\vec{x}-\lambda \vec{x} &= \vec{0} \\
A\vec{x}-\lambda I\vec{x} &= \vec{0} \\
(A-\lambda I)\vec{x} &= \vec{0}
\end{align*}
The middle step was necessary before factoring because \wordChoice{\choice[correct]{we cannot subtract a $1 \times 1$ scalar $\lambda$ from an $n \times n$ matrix $A$} \choice{$\lambda$ is a Greek letter}}.
    
This shows that any eigenvector $\vec{x}$ of $A$ is in the \wordChoice{\choice{row space}\choice{column space}\choice[correct]{null space}}  of the related matrix, $A-\lambda I$.
    
Since eigenvectors are non-zero vectors, this means that $A$ will have eigenvectors if and only if the null space of $A-\lambda I$ is nontrivial.  The only way that $\mbox{null}(A-\lambda I)$ can be nontrivial is if $\mbox{rank}(A-\lambda I)$ \wordChoice{\choice{$=$}\choice[correct]{$<$}\choice{$>$}} $n$.
    
If the rank of an $n \times n$ matrix is less than $n$, then the matrix is singular.  Since $(A-\lambda I)$ must be singular for any eigenvalue $\lambda$, we see that $\lambda$ is an eigenvalue of $A$ if and only if
\begin{equation}\label{eqn:chareqn}
\mbox{det}(A-\lambda I) = \answer{0}
\end{equation}
\end{exploration}
In theory, Exploration \ref{exp:slowdown} offers us a way to find eigenvalues.  To find the eigenvalues of $A$, one can solve Equation (\ref{eqn:chareqn}) for $\lambda$.
    
\subsection*{Eigenvalues}
    
\begin{definition}\label{def:chareqcharpoly}
The equation
$$\mbox{det}(A-\lambda I) = 0$$ is called the \dfn{characteristic equation} of $A$.
\end{definition}
    
\begin{example}\label{ex:2x2eig}
Let $A=\begin{bmatrix} 2& 1\\ 1&2
\end{bmatrix}$.  Compute the eigenvalues of this matrix using the characteristic equation.
\begin{explanation}
\begin{align*}\det(A-\lambda I)=\begin{vmatrix}2-\lambda&1\\1&2-\lambda\end{vmatrix}&=(2-\lambda)^2-1\\
&=\lambda^2-4\lambda+3\\
&=(\lambda-1)(\lambda-3)
\end{align*}
The characteristic equation $(\lambda-1)(\lambda-3)=0$ has solutions $\lambda_1=1$ and $\lambda_2=3$.  These are the eigenvalues of $A$.
\end{explanation}
\end{example}
    
\begin{example}\label{ex:2x2eig2}
Let $B=\begin{bmatrix} 2& 1\\ 4&2
\end{bmatrix}$.  Compute the eigenvalues of $B$ using the characteristic equation.  (List your answers in an increasing order.)
    
$\lambda_1=\answer{0}$ and $\lambda_2=\answer{4}$
\end{example}


    
\begin{example}\label{ex:3x3eig}
Let $C=\begin{bmatrix} 2 & 1 & 1\\ 1 & 2 & 1\\ 1 & 1 & 2\end{bmatrix}$.  Compute the eigenvalues of $C$ using the characteristic equation.
\begin{explanation}
\begin{align*}\det(C-\lambda I)&=\begin{vmatrix}2-\lambda & 1 & 1\\ 1 & 2-\lambda & 1\\ 1 & 1 & 2-\lambda\end{vmatrix}\\
&=(2-\lambda)\begin{vmatrix}2-\lambda&1\\1&2-\lambda\end{vmatrix}-1\begin{vmatrix}1&1\\1&2-\lambda\end{vmatrix}+1\begin{vmatrix}1&2-\lambda\\1&1\end{vmatrix}\\
&=(2-\lambda)^3-(2-\lambda)-((2-\lambda)-1)+1-(2-\lambda)\\
&=8-12\lambda+6\lambda^2-\lambda^3-2+\lambda-2+\lambda+1+1-2+\lambda\\
&=4-9\lambda+6\lambda^2-\lambda^3\\
&=-(\lambda-4)(\lambda-1)^2
\end{align*}
Matrix $C$ has eigenvalues $\lambda_1=1$ and $\lambda_2=4$.
\end{explanation}
\end{example}
    
In Example \ref{ex:3x3eig}, the factor  $(\lambda-1)$ appears twice.  This repeated factor gives rise to the eigenvalue $\lambda_1=1$.  We say that the eigenvalue $\lambda_1=1$ has \dfn{algebraic multiplicity} $2$.
    
The three examples above are a bit contrived.  It is not always possible to completely factor the characteristic polynomial using only real numbers.  However, a fundamental fact from algebra is that every degree $n$ polynomial has $n$ roots (counting multiplicity) provided that we allow complex numbers.  This is why sometimes eigenvalues and their corresponding eigenvectors involve complex numbers.  The next example illustrates this point.
    
\begin{example}\label{ex:3x3_complex_eig}
Let $D=\begin{bmatrix} 0&0&0\\ 0 &1&1\\ 0 & -1&1\end{bmatrix}$.  Compute the eigenvalues of this matrix.
    
\begin{explanation}
\begin{align*}\det(D-\lambda I)&=\begin{vmatrix} -\lambda&0&0\\ 0 &1-\lambda&1\\ 0 & -1&1-\lambda\end{vmatrix}\\
&=-\lambda\begin{vmatrix}1-\lambda&1\\-1&1-\lambda\end{vmatrix}\\
&=-\lambda((1-\lambda)^2+1)\\
&=-\lambda(\lambda^2-2\lambda+2)
\end{align*}
So one of the eigenvalues of $D$ is $\lambda=0$.  To get the other eigenvalues we must solve $\lambda^2-2\lambda+2=0$.  Using the quadratic formula, we compute that $\lambda_1=1+i$ and $\lambda_2=1-i$ are also eigenvalues of $D$.
\end{explanation}
\end{example}


    
\begin{exploration}\label{init:3x3tri}
Let $T=\begin{bmatrix} 1 & 2 & 3\\ 0 & 5 & 6\\ 0 & 0 & 9\end{bmatrix}$.  Compute the eigenvalues of this matrix.  (List your answers in an increasing order.)
    
$$\lambda_1=\answer{1},\quad\lambda_2=\answer{5},\quad\text{and}\quad\lambda_3=\answer{9}$$
    
What do you observe about the eigenvalues?
\begin{hint}
The eigenvalues are the diagonal entries of the matrix.
\end{hint}
    
What property of the matrix makes this ``coincidence" possible?
    
\begin{hint}
$T$ is a triangular matrix.
\end{hint}
\end{exploration}
    
The matrix in Exploration Problem \ref{init:3x3tri} is a triangular matrix, and the property you observed holds in general.
    
\begin{theorem}\label{th:eigtri}
Let $T$ be a triangular matrix.  Then the eigenvalues of $T$ are the entries on the main diagonal.
\end{theorem}
    
    
\begin{corollary}\label{th:eigdiag}
Let $D$ be a diagonal matrix.  Then the eigenvalues of $D$ are the entries on the main diagonal.
\end{corollary}
    
One final note about eigenvalues.  We began this section with the sentence, "In theory, then, to find the eigenvalues of $A$, one can solve Equation (\ref{eqn:chareqn}) for $\lambda$."  In general, one does not attempt to compute eigenvalues by solving the characteristic equation of a matrix, as there is no simple way to solve this polynomial equation for $n>4$.  Instead, one can often approximate the eigenvalues using \dfn{iterative methods}.

    
\subsection*{Eigenvectors}
Once we have computed an eigenvalue $\lambda$ of an $n \times n$ matrix $A$, the next step is to compute the associated eigenvectors.  In other words, we seek vectors $\vec{x}$ such that $A\vec{x}=\lambda \vec{x}$, or equivalently,
\begin{equation}\label{eqn:nullspace}
    (A-\lambda I) \vec{x}=\vec{0}  
\end{equation}
For any given eigenvalue $\lambda$ there are infinitely many eigenvectors associated with it.  In fact, the eigenvectors associated with $\lambda$ form a subspace of $\RR^n$.
    
\begin{theorem}\label{th:eigenspace}
    Let $A$ be an $n\times n$ matrix and let $\lambda$ be an eigenvalue of $A$.  Then the set of all eigenvectors associated with $\lambda$ is a subspace of $\RR^n$.
\end{theorem}
    
This motivates the following definition.
    
\begin{definition}\label{def:eigspace}
The set of all eigenvectors associated with a given eigenvalue of a matrix is known as the \dfn{eigenspace} associated with that eigenvalue.
\end{definition}
    
So given an eigenvalue $\lambda$, there is an associated eigenspace $\mathcal{S}$, and our goal is to find a basis of $\mathcal{S}$, for then any eigenvector $\vec{x}$ will be a linear combination of the vectors in that basis.  Moreover, we are trying to find a basis for the set of vectors that satisfy Equation \ref{eqn:nullspace}, which means we seek a basis for $\mbox{null}(A-\lambda I)$. 
    
Let's return to the examples we did in the first part of this section.
    
\begin{example}\label{ex:eigvect2x2eig} (Finding eigenvectors for Example \ref{ex:2x2eig} )
    
Recall that $A=\begin{bmatrix} 2& 1\\ 1&2
\end{bmatrix}$ has eigenvalues $\lambda_1=1$ and $\lambda_2=3$.  Compute a basis for the eigenspace associated with each of these eigenvalues.
\begin{explanation}
Eigenvectors associated with the eigenvalue $\lambda_1=1$ are in the null space of $A-I$.  So we seek a basis for $\mbox{null}(A-I)$.  We compute:
\begin{align*}\mbox{rref}(A-I)=\mbox{rref}\left(\begin{bmatrix}1&1\\1&1\end{bmatrix}\right)&=\begin{bmatrix}1&1\\0&0\end{bmatrix},
\end{align*}
From this we see that the eigenspace $\mathcal{S}_1$ associated with $\lambda_1=1$ consists of vectors of the form $\begin{bmatrix}-1\\1\end{bmatrix}t$.
%for any eigenvector $\begin{bmatrix}x_1\\x_2\end{bmatrix}$, we have $x_1+x_2=0$, so that $x_1=-x_2$ 
This means that $\left\{\begin{bmatrix}-1\\1\end{bmatrix}\right\}$ is one possible basis for $\mathcal{S}_1$.
    
In a similar way, we compute a basis for $\mathcal{S}_3$, the subspace of all eigenvectors associated with the eigenvalue $\lambda_2=3$.  Now we compute:
\begin{align*}\mbox{rref}(A-3I)=\mbox{rref}\left(\begin{bmatrix}-1&1\\1&-1\end{bmatrix}\right)&=\begin{bmatrix}1&-1\\0&0\end{bmatrix},
\end{align*}
Vectors in the null space have the form $\begin{bmatrix}1\\1\end{bmatrix}t$ This means that $\left\{\begin{bmatrix}1\\1\end{bmatrix}\right\}$ is one possible basis for the eigenspace $\mathcal{S}_3$.
\end{explanation}
\end{example}
    
\begin{example}\label{ex:eigvectors2x2eig2} (Finding eigenvectors for Example \ref{ex:2x2eig2})
We know from Example \ref{ex:2x2eig2} that $B=\begin{bmatrix} 2& 1\\ 4&2
\end{bmatrix}$ has eigenvalues $\lambda_1=0$ and $\lambda_2=4$.  Compute a basis for the eigenspace associated with each of these eigenvalues.
\begin{explanation}
Let's begin by finding a basis for the eigenspace $\mathcal{S}_0$, which is the subspace of $\RR^n$ consisting of eigenvectors corresponding to the eigenvalue $\lambda_1=0$.  We need to compute a basis for $\mbox{null}(B-0I) = \mbox{null}(B)$.  We compute:
\begin{align*}\mbox{rref}(B)=\mbox{rref}\left(\begin{bmatrix}2&1\\4&2\end{bmatrix}\right)&=\begin{bmatrix}1&\frac{1}{2}\\0&0\end{bmatrix},
\end{align*}
From this we see that an eigenvector in $\mathcal{S}_0$ has the form $\begin{bmatrix}-1/2\\1\end{bmatrix}t$. %$\begin{bmatrix}x_1\\x_2\end{bmatrix}$ in $\mathcal{S}_0$ satisfies $x_1+\frac{1}{2} x_2=0$, so that $2x_1=-x_2$.
This means that $\left\{\begin{bmatrix}-1/2\\1\end{bmatrix}\right\}$ is one possible basis for the eigenspace $\mathcal{S}_0$.  By letting $t=-2$, we obtain an arguably nicer-looking basis: $\left\{\begin{bmatrix}1\\-2\end{bmatrix}\right\}$.
    
See if you can compute a basis for $\mathcal{S}_4$. 
    
Click on the arrow if you need help.
    
\begin{expandable}
To compute a basis for $\mathcal{S}_4$, the subspace of all eigenvectors associated to the eigenvalue $\lambda_2=4$, we compute:
$$\mbox{rref}(B-4I)=\mbox{rref}\left(\begin{bmatrix}-2&1\\4&-2\end{bmatrix}\right)=\begin{bmatrix}1&-\frac{1}{2}\\0&0\end{bmatrix}$$
\end{expandable}
    
From this we find that $\left\{\begin{bmatrix}1\\\answer{2}\end{bmatrix}\right\}$ is one possible basis for the eigenspace $\mathcal{S}_4$.
\end{explanation}
\end{example}
    
\begin{example}\label{ex:eigvectors3x3eig} (Finding eigenvectors for Example \ref{ex:3x3eig})
We know from Example \ref{ex:3x3eig} that $C=\begin{bmatrix} 2 & 1 & 1\\ 1 & 2 & 1\\ 1 & 1 & 2\end{bmatrix}$ has eigenvalues $\lambda_1=1$ and $\lambda_2=4$.  Compute a basis for the eigenspace associated to each of these eigenvalues.
\begin{explanation}
We first find a basis for the eigenspace $\mathcal{S}_1$.  We need to compute a basis for $\mbox{null}(C-I)$.  We compute:
\begin{align*}\mbox{rref}(C-I)=\mbox{rref}\left(\begin{bmatrix} 1 & 1 & 1\\ 1 & 1 & 1\\ 1 & 1 & 1\end{bmatrix}\right)&=\begin{bmatrix} 1 & 1 & 1\\ 0 & 0 & 0\\ 0 & 0 & 0\end{bmatrix},
\end{align*}
%From this we see for any eigenvector $\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}$ in $\mathcal{S}_1$ satisfies $x_1+x_2+x_3=0$. 
Notice that there are two free variables.  %Let $x_2=s$ and $x_3=t$.
The eigenvectors in $\mathcal{S}_1$ have the form
$$\begin{bmatrix}-s-t\\s\\t\end{bmatrix} = s\begin{bmatrix}-1\\1\\0\end{bmatrix} + t\begin{bmatrix}-1\\0\\1\end{bmatrix}$$
    
So one possible basis for the eigenspace $\mathcal{S}_1$ is given by $\left\{\begin{bmatrix}-1\\1\\0\end{bmatrix}, \begin{bmatrix}-1\\0\\1\end{bmatrix}\right\}$.
    
Next we find a basis for the eigenspace $\mathcal{S}_4$.  We need to compute a basis for $\mbox{null}(C-4I)$.  We compute:
\begin{align*}\mbox{rref}(C-4I)=\mbox{rref}\left(\begin{bmatrix} -2 & 1 & 1\\ 1 & -2 & 1\\ 1 & 1 & -2\end{bmatrix}\right)&=\begin{bmatrix} 1 & 0 & -1\\ 0 & 1 & -1\\ 0 & 0 & 0\end{bmatrix}
\end{align*}
This time there is one free variable.  %Setting $x_3=t$, we also get $x_1=t$ and $x_2=t$.  From this we see
The eigenvectors in $\mathcal{S}_4$ have the form $\begin{bmatrix}t\\t\\t\end{bmatrix}$, so a possible basis for the eigenspace $\mathcal{S}_4$ is given by $\left\{\begin{bmatrix}1\\1\\1\end{bmatrix}\right\}$.
\end{explanation}
\end{example}
    
\begin{example}\label{ex:3x3_complex_ev} (Finding eigenvectors for Example \ref{ex:3x3_complex_eig})
We know from Example \ref{ex:3x3_complex_eig} that $D=\begin{bmatrix} 0&0&0\\ 0 &1&1\\ 0 & -1&1\end{bmatrix}$ has eigenvalues $\lambda=0$, $\lambda_1=1+i$, and $\lambda_2=1-i$.  Compute a basis for the eigenspace associated with each eigenvalue.
\begin{explanation}
We first find a basis for the eigenspace $\mathcal{S}_0$.  We need to compute a basis for $\mbox{null}(D-0I)=\mbox{null}(D)$.  We compute:
\begin{align*}\mbox{rref}(D)=\mbox{rref}\left(\begin{bmatrix} 0&0&0\\ 0 &1&1\\ 0 & -1&1\end{bmatrix}\right)&=\begin{bmatrix} 0 & 1 & 1\\ 0 & 0 & 1\\ 0 & 0 & 0\end{bmatrix},
\end{align*}
From this we see that for any eigenvector $\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}$ in $\mathcal{S}_0$ we have $x_2=0$ and $x_3=0$, but $x_1$ is a free variable.
So one possible basis for the eigenspace $\mathcal{S}_0$ is given by $$\left\{\begin{bmatrix}1\\0\\0\end{bmatrix}\right\}$$
Next we find a basis for the eigenspace $\mathcal{S}_{1+i}$.  We need to compute a basis for $\mbox{null}(D-(1+i)I)$.  We compute:
\begin{align*}\mbox{rref}(D-(1+i)I)&=\mbox{rref}\left(\begin{bmatrix} -(1+i)&0&0\\ 0 &1-(1+i)&1\\ 0 & -1&1-(1+i)\end{bmatrix}\right) \\
&=\begin{bmatrix} 1 & 0 &0\\ 0 & 1 & i\\ 0 & 0 & 0\end{bmatrix}
\end{align*}
There is one free variable.  Setting $x_3=t$, we get $x_1=0$ and $x_2=ti$.  From this we see that eigenvectors in $\mathcal{S}_{1+i}$ have the form $\begin{bmatrix}0\\i\\1\end{bmatrix}t$, so a possible basis for the eigenspace $\mathcal{S}_{1+i}$ is given by $\left\{\begin{bmatrix}0\\i\\1\end{bmatrix}\right\}$.
\end{explanation}
\end{example}
    
We conclude this section by establishing the significance of a matrix having an eigenvalue of zero.
    
\begin{theorem}\label{th:zero_ew}
A square matrix has an eigenvalue of zero if and only if it is singular.
\end{theorem}
    
\begin{proof}
A square matrix $A$ is singular if and only if $\det{A}=0$.  But $\det{A}=0$ if and only if $\det{A-0I}=0$, which is true if and only if zero is an eigenvalue of $A$.
\end{proof}

    

\end{document}