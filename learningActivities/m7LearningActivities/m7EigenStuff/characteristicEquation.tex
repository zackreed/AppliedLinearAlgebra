\documentclass{ximera}
\input{../../../preamble.tex}

\author{Zack Reed}
%borrowed from selinger linear algebra
\title{The Characteristic Equation}
\begin{document}
\begin{abstract}

\end{abstract}
\maketitle


\section*{The Characteristic Equation}

    
Let $A$ be an $n \times n$ matrix.  In \href{https://ximera.osu.edu/appliedlinearalgebra/c7ChapterSeven/learningActivities/m7LearningActivities/m7EigenStuff/describingEigenstuff}{the previous section} we learned that the eigenvectors and eigenvalues of $A$ are vectors $\vec{x}$ and scalars $\lambda$ that satisfy the equation 
$A \vec{v} = \lambda \vec{v}$

We listed a few reasons why we are interested in finding eigenvalues and eigenvectors, but we did not give any process for finding them.  In this section we will focus on a process which can be used for small matrices.  %For larger matrices, the best methods we have are iterative methods, and we will explore some of these in \href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/EIG-0070/main}{The Power Method and the Dominant Eigenvalue}.
    
For an $n \times n$ matrix, we will see that the eigenvalues are the roots of a polynomial called the \dfn{characteristic polynomial}.  So finding eigenvalues is equivalent to solving a polynomial equation of degree $n$.  Finding the corresponding eigenvectors turns out to be a matter of computing the null space of a matrix, as the following exploration demonstrates.
    
\begin{exploration}\label{exp:slowdown}
If a vector $\vec{x}$ is an eigenvector satisfying the equation $A\vec{x}=\lambda \vec{x}$, then clearly it also satisfies  $A\vec{x}-\lambda \vec{x} =$ \wordChoice{\choice{0} \choice[correct]{$\vec{0}$}}.
    
It seems natural at this point to try to factor.  We would love to ``factor out'' $\vec{x}$.  Here is the procedure:
\begin{align*}
A\vec{x}-\lambda \vec{x} &= \vec{0} \\
A\vec{x}-\lambda I\vec{x} &= \vec{0} \\
(A-\lambda I)\vec{x} &= \vec{0}
\end{align*}
The middle step was necessary before factoring because \wordChoice{\choice[correct]{we cannot subtract a $1 \times 1$ scalar $\lambda$ from an $n \times n$ matrix $A$} \choice{$\lambda$ is a Greek letter}}.
    
This shows that any eigenvector $\vec{x}$ of $A$ is in the \wordChoice{\choice{row space}\choice{column space}\choice[correct]{null space}}  of the related matrix, $A-\lambda I$.
    
Since eigenvectors are non-zero vectors, this means that $A$ will have eigenvectors if and only if the null space of $A-\lambda I$ is nontrivial.  The only way that $\mbox{null}(A-\lambda I)$ can be nontrivial is if $\mbox{rank}(A-\lambda I)$ \wordChoice{\choice{$=$}\choice[correct]{$<$}\choice{$>$}} $n$.
    
If the rank of an $n \times n$ matrix is less than $n$, then the matrix is singular.  Since $(A-\lambda I)$ must be singular for any eigenvalue $\lambda$, we see that $\lambda$ is an eigenvalue of $A$ if and only if
\begin{equation}\label{eqn:chareqn}
\mbox{det}(A-\lambda I) = \answer{0}
\end{equation}
\end{exploration}
In theory, Exploration \ref{exp:slowdown} offers us a way to find eigenvalues.  To find the eigenvalues of $A$, one can solve Equation (\ref{eqn:chareqn}) for $\lambda$.
    
\subsection*{Eigenvalues}
    
\begin{definition}\label{def:chareqcharpoly}
The equation
$$\mbox{det}(A-\lambda I) = 0$$ is called the \dfn{characteristic equation} of $A$.
\end{definition}
    
\begin{example}\label{ex:2x2eig}
Let $A=\begin{bmatrix} 2& 1\\ 1&2
\end{bmatrix}$.  Compute the eigenvalues of this matrix using the characteristic equation. (List your answers in an increasing order.)
\begin{explanation}
    \begin{align*}\det(A-\lambda I)=\begin{vmatrix} \answer{2}-\lambda & \answer{1} \\ \answer{1} &\answer{2}-\lambda \end{vmatrix} &= (\lambda{2}-\lambda)^2 - \answer{1} \\
    &= \lambda^2 - \answer{4} \lambda + \answer{3} \\
    &= (\lambda - \answer{1})(\lambda - \answer{3})
    \end{align*}
    The characteristic equation \((\lambda - \answer{1})(\lambda - \answer{3}) = \answer{0}\) has solutions \(\lambda_1 = \answer{1}\) and \(\lambda_2 = \answer{3}\). These are the eigenvalues of \(A\).
    \end{explanation}
\end{example}
    
\begin{example}\label{ex:2x2eig2}
Let $B=\begin{bmatrix} 2& 1\\ 4&2
\end{bmatrix}$.  Compute the eigenvalues of $B$ using the characteristic equation.  (List your answers in an increasing order.)
    
$\lambda_1=\answer{0}$ and $\lambda_2=\answer{4}$
\end{example}


    
\begin{example}\label{ex:3x3eig}
Let $C=\begin{bmatrix} 2 & 1 & 1\\ 1 & 2 & 1\\ 1 & 1 & 2\end{bmatrix}$.  Compute the eigenvalues of $C$ using the characteristic equation.
\begin{explanation}
    \begin{align*}\det(C-\lambda I)&=\begin{vmatrix} \answer{2}-\lambda & \answer{1} & \answer{1} \\ \answer{1} & \answer{2}-\lambda & \answer{1} \\ \answer{1} & \answer{1} & \answer{2}-\lambda \end{vmatrix}\\
    &=\answer{4} - \answer{9} \lambda + \answer{6} \lambda^2 - \lambda^3\\
    &=-(\lambda - \answer{4})(\lambda - \answer{1})^2
    \end{align*}
    Matrix \(C\) has eigenvalues \(\lambda_1 = \answer{1}\) and \(\lambda_2 = \answer{4}\).
    \end{explanation}
\end{example}
    
In Example \ref{ex:3x3eig}, the factor  $(\lambda-1)$ appears twice.  This repeated factor gives rise to the eigenvalue $\lambda_1=1$.  We say that the eigenvalue $\lambda_1=1$ has \dfn{algebraic multiplicity} $2$.

There are actually two kinds of multiplicities associated with eigenvalues that we want to consider, particulalry in the next chapter. 

\begin{definition}\name{Eigenvalue multiplicities and Eigenspaces}\label{def:geommulteig}

The \emph{algebraic multiplicity} of an eigenvalue $\lambda$ is the number of times that it occurs as a root of the characteristic polynomial.

The eigenspace $\mathcal{S}_\lambda$ is the span of all eigenvectors associated with the eigenvalue $\lambda$.

The \emph{geometric multiplicity} of an eigenvalue $\lambda$ is the dimension of the corresponding eigenspace $\mathcal{S}_\lambda$. 
\end{definition}

Consider now the following lemma.

\begin{lemma}\label{lemma:dimeigenspace}
Let $A$ be an $n\times n$ matrix, and let $\mathcal{S}_{\lambda_1}$ be the eigenspace corresponding to the eigenvalue $\lambda_1$ which has algebraic multiplicity $m$.  Then
$$\mbox{dim}(\mathcal{S}_{\lambda_1})\leq m$$
\end{lemma}

In other words, the geometric multiplicity of an eigenvalue is less than or equal to the algebraic multiplicity of that same eigenvalue.

    
The three examples above are a bit contrived.  It is not always possible to completely factor the characteristic polynomial using only real numbers.  However, a fundamental fact from algebra is that every degree $n$ polynomial has $n$ roots (counting multiplicity) provided that we allow complex numbers.
    
\begin{example}\label{ex:3x3_complex_eig}
Let $D=\begin{bmatrix} 0&0&0\\ 0 &1&1\\ 0 & -1&1\end{bmatrix}$.  Compute the eigenvalues of this matrix.
    
\begin{explanation}
    \begin{align*}
    \det(D-\lambda I) &= \begin{vmatrix} 
    \answer{-1}\lambda & 0 & 0\\ 
    0 & \answer{1}+\answer{-1}\lambda & \answer{1}\\ 
    0 & \answer{-1} & \answer{1}+\answer{-1}\lambda 
    \end{vmatrix} \\
    &= \answer{-1}\lambda(\lambda^2 + \answer{-2}\lambda + \answer{2})
    \end{align*}
    So one of the eigenvalues of $D$ is $\lambda=\answer{0}$.  To get the other eigenvalues we must solve $\lambda^2+\answer{-2}\lambda+\answer{2}=0$.  We compute that $\lambda_1=\answer{1}+i$ and $\lambda_2=\answer{1}-i$ are also eigenvalues of $D$.
    \end{explanation}
\end{example}


    
\begin{exploration}\label{init:3x3tri}
Let $T=\begin{bmatrix} 1 & 2 & 3\\ 0 & 5 & 6\\ 0 & 0 & 9\end{bmatrix}$.  Compute the eigenvalues of this matrix.  (List your answers in an increasing order.)
    
$$\lambda_1=\answer{1},\quad\lambda_2=\answer{5},\quad\text{and}\quad\lambda_3=\answer{9}$$
    
Note that the eigenvalues of $T$ are the entries on the main diagonal. This is a special property of triangular matrices, and it follows directly from the very simple to calculate determinant of a triangular matrix.
\end{exploration}
    
The matrix in Exploration Problem \ref{init:3x3tri} is a triangular matrix, and the property you observed holds in general.
    
\begin{theorem}\label{th:eigtri}
Let $T$ be a triangular matrix.  Then the eigenvalues of $T$ are the entries on the main diagonal.
\end{theorem}
    
    
\begin{corollary}\label{th:eigdiag}
Let $D$ be a diagonal matrix.  Then the eigenvalues of $D$ are the entries on the main diagonal.
\end{corollary}
    
\begin{remark}
    One final note about eigenvalues. The relationship to the characteristic polynomial is primarily for conceptual grounding. In practice, much like with the SVD from the last chapter, we compute eigenvalues in a much more efficient way than is presented here. Understanding the connection to the characteristic equation is still an important idea, however.
\end{remark}


    
\subsection*{Eigenvectors}
Once we have computed an eigenvalue $\lambda$ of an $n \times n$ matrix $A$, the next step is to compute the associated eigenvectors.  In other words, we seek vectors $\vec{x}$ such that $A\vec{x}=\lambda \vec{x}$, or equivalently,
\begin{equation}\label{eqn:nullspace}
    (A-\lambda I) \vec{x}=\vec{0}  
\end{equation}
For any given eigenvalue $\lambda$ there are infinitely many eigenvectors associated with it. This underlies the notion of an eigenspace that was defined earlier. We again state the definition below.

    
\begin{definition}\label{def:eigspace}
The set of all eigenvectors associated with a given eigenvalue of a matrix is known as the \dfn{eigenspace} associated with that eigenvalue, and is a subspace of $\RR^n$.
\end{definition}
    
So given an eigenvalue $\lambda$, there is an associated eigenspace $\mathcal{S}$, and our goal is to find a basis of $\mathcal{S}$, for then any eigenvector $\vec{x}$ will be a linear combination of the vectors in that basis.  Moreover, we are trying to find a basis for the set of vectors that satisfy Equation \ref{eqn:nullspace}, which means we seek a basis for $\mbox{null}(A-\lambda I)$. 
    
Let's return to our previous examples to find eigenvectors for the matrices $A$, $B$, $C$, and $D$.
    
\begin{example}\label{ex:eigvect2x2eig} (Finding eigenvectors for Example \ref{ex:2x2eig} )
    
Recall that $A=\begin{bmatrix} 2& 1\\ 1&2
\end{bmatrix}$ has eigenvalues $\lambda_1=1$ and $\lambda_2=3$.  Compute a basis for the eigenspace associated with each of these eigenvalues.
\begin{explanation}
Eigenvectors associated with the eigenvalue $\lambda_1=1$ are in the null space of $A-I$.  So we seek a basis for $\mbox{null}(A-I)$.  We compute:
\begin{align*}\mbox{rref}(A-I)=\mbox{rref}\left(\begin{bmatrix}1&1\\1&1\end{bmatrix}\right)&=\begin{bmatrix}\answer{1}&\answer{1}\\\answer{0}&\answer{0}\end{bmatrix},
\end{align*}
From this we see that the eigenspace $\mathcal{S}_1$ associated with $\lambda_1=1$ consists of vectors of the form $\begin{bmatrix}\answer{-1}\\\answer{1}\end{bmatrix}t$.

This means that $\left\{\begin{bmatrix}\answer{-1}\\\answer{1}\end{bmatrix}\right\}$
is one possible basis for $\mathcal{S}_1$.
    
In a similar way, we compute a basis for $\mathcal{S}_3$, the subspace of all eigenvectors associated with the eigenvalue $\lambda_2=3$.  Now we compute:
\begin{align*}\mbox{rref}(A-3I)=\mbox{rref}\left(\begin{bmatrix}-1&1\\1&-1\end{bmatrix}\right)&=\begin{bmatrix}\answer{1}&\answer{-1}\\\answer{0}&\answer{0}\end{bmatrix},
\end{align*}
Vectors in the null space have the form $\begin{bmatrix}\answer{1}\\\answer{1}\end{bmatrix}t$.  So one possible basis for the eigenspace $\mathcal{S}_3$ is given by $\left\{\begin{bmatrix}\answer{1}\\\answer{1}\end{bmatrix}\right\}$.
\end{explanation}
\end{example}
    
\begin{example}\label{ex:eigvectors2x2eig2} (Finding eigenvectors for Example \ref{ex:2x2eig2})
We know from Example \ref{ex:2x2eig2} that $B=\begin{bmatrix} 2& 1\\ 4&2
\end{bmatrix}$ has eigenvalues $\lambda_1=0$ and $\lambda_2=4$.  Compute a basis for the eigenspace associated with each of these eigenvalues.
\begin{explanation}
Let's begin by finding a basis for the eigenspace $\mathcal{S}_0$, which is the subspace of $\RR^n$ consisting of eigenvectors corresponding to the eigenvalue $\lambda_1=0$.  We need to compute a basis for $\mbox{null}(B-0I) = \mbox{null}(B)$.  We compute:
\begin{align*}\mbox{rref}(B)=\mbox{rref}\left(\begin{bmatrix}2&1\\4&2\end{bmatrix}\right)&=\begin{bmatrix}\answer{1}&\answer{1/2}\\0&0\end{bmatrix},
\end{align*}
From this we see that an eigenvector in $\mathcal{S}_0$ has the form $\begin{bmatrix}\answer{-1/2}\\\answer{1}\end{bmatrix}t$.
This means that $\left\{\begin{bmatrix}\answer{-1/2}\\\answer{1}\end{bmatrix}\right\}$
     is one possible basis for the eigenspace $\mathcal{S}_0$.  By letting $t=-2$, we obtain an arbitrary but arguably nicer-looking basis: $\left\{\begin{bmatrix}\answer{1}\\\answer{-2}\end{bmatrix}\right\}$.

    
To compute a basis for $\mathcal{S}_4$, the subspace of all eigenvectors associated to the eigenvalue $\lambda_2=4$, we compute:
$$\mbox{rref}(B-4I)=\mbox{rref}\left(\begin{bmatrix}-2&1\\4&-2\end{bmatrix}\right)=\begin{bmatrix}\answer{1}&\answer{-1/2}\\0&0\end{bmatrix}$$

    
From this we find that $\left\{\begin{bmatrix}1\\\answer{2}\end{bmatrix}\right\}$ is one possible basis for the eigenspace $\mathcal{S}_4$.
\end{explanation}
\end{example}
    
\begin{example}\label{ex:eigvectors3x3eig} (Finding eigenvectors for Example \ref{ex:3x3eig})
    We know from Example \ref{ex:3x3eig} that $C=\begin{bmatrix} 2 & 1 & 1\\ 1 & 2 & 1\\ 1 & 1 & 2\end{bmatrix}$ has eigenvalues $\lambda_1=1$ and $\lambda_2=4$. Compute a basis for the eigenspace associated to each of these eigenvalues.
    \begin{explanation}
    We first find a basis for the eigenspace $\mathcal{S}_1$.  We need to compute a basis for $\mbox{null}(C-\answer{1}I)$.  We compute:
    \begin{align*}
    \mbox{rref}(C-\answer{1}I) &= \mbox{rref}\left(\begin{bmatrix} \answer{1} & \answer{1} & \answer{1}\\ \answer{1} & \answer{1} & \answer{1}\\ \answer{1} & \answer{1} & \answer{1}\end{bmatrix}\right) \\
    &= \begin{bmatrix} \answer{1} & \answer{1} & \answer{1}\\ 0 & 0 & 0\\ 0 & 0 & 0\end{bmatrix},
    \end{align*}
    Notice that there are two free variables. The eigenvectors in $\mathcal{S}_1$ have the form
    $$\begin{bmatrix}-s-t\\s\\t\end{bmatrix} = s\begin{bmatrix}-\answer{1}\\\answer{1}\\\answer{0}\end{bmatrix} + t\begin{bmatrix}-\answer{1}\\\answer{0}\\\answer{1}\end{bmatrix}$$
        
    So one possible basis for the eigenspace $\mathcal{S}_1$ is given by $\left\{\begin{bmatrix}-\answer{1}\\\answer{1}\\\answer{0}\end{bmatrix}, \begin{bmatrix}-\answer{1}\\\answer{0}\\\answer{1}\end{bmatrix}\right\}$.
        
    Next we find a basis for the eigenspace $\mathcal{S}_4$.  We need to compute a basis for $\mbox{null}(C-\answer{4}I)$.  We compute:
    \begin{align*}
    \mbox{rref}(C-\answer{4}I) &= \mbox{rref}\left(\begin{bmatrix} -\answer{2} & \answer{1} & \answer{1}\\ \answer{1} & -\answer{2} & \answer{1}\\ \answer{1} & \answer{1} & -\answer{2}\end{bmatrix}\right) \\
    &= \begin{bmatrix} \answer{1} & \answer{0} & -\answer{1}\\ 0 & \answer{1} & -\answer{1}\\ 0 & 0 & 0\end{bmatrix}
    \end{align*}
    This time there is one free variable. The eigenvectors in $\mathcal{S}_4$ have the form $\begin{bmatrix}\answer{1}\\\answer{1}\\\answer{1}\end{bmatrix}t$, so a possible basis for the eigenspace $\mathcal{S}_4$ is given by $\left\{\begin{bmatrix}\answer{1}\\\answer{1}\\\answer{1}\end{bmatrix}\right\}$.
    \end{explanation}
    \end{example}
    
    \begin{example}\label{ex:3x3_complex_ev} (Finding eigenvectors for Example \ref{ex:3x3_complex_eig})
        We know from Example \ref{ex:3x3_complex_eig} that $D=\begin{bmatrix} 0&0&0\\ 0 &1&1\\ 0 & -1&1\end{bmatrix}$ has eigenvalues $\lambda=0$, $\lambda_1=1+i$, and $\lambda_2=1-i$.  Compute a basis for the eigenspace associated with each eigenvalue.
        \begin{explanation}
        We first find a basis for the eigenspace $\mathcal{S}_0$.  We need to compute a basis for $\mbox{null}(D-\answer{0}I)=\mbox{null}(D)$.  We compute:
        \begin{align*}
        \mbox{rref}(D) &= \mbox{rref}\left(\begin{bmatrix} 0 & 0 & 0\\ 0 & 1 & 1\\ 0 & -1 & 1\end{bmatrix}\right) \\
        &= \begin{bmatrix} \answer{0} & \answer{1} & \answer{1}\\ \answer{0} & \answer{0} & \answer{1}\\ \answer{0} & \answer{0} & \answer{0}\end{bmatrix},
        \end{align*}
        From this we see that for any eigenvector $\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}$ in $\mathcal{S}_0$ we have $x_2=\answer{0}$ and $x_3=\answer{0}$, but $x_1$ is a free variable.
        So one possible basis for the eigenspace $\mathcal{S}_0$ is given by $$\left\{\begin{bmatrix}\answer{1}\\\answer{0}\\\answer{0}\end{bmatrix}\right\}$$

        Next we find a basis for the eigenspace $\mathcal{S}_{1+i}$.  We need to compute a basis for $\mbox{null}(D-(1+i)I)$.  We compute:
        \begin{align*}
        \mbox{rref}(D-(1+i)I) &= \mbox{rref}\left(\begin{bmatrix} -(1+i)&0&0\\ 0 &1-(1+i)&1\\ 0 & -1&1-(1+i)\end{bmatrix}\right) \\
        &= \begin{bmatrix} \answer{1} & \answer{0} &\answer{0}\\ \answer{0} & \answer{1} & i\\ \answer{0} & \answer{0} & \answer{0}\end{bmatrix}
        \end{align*}
        There is one free variable.  Setting $x_3=t$, we get $x_1=\answer{0}$ and $x_2=ti$.  From this we see that eigenvectors in $\mathcal{S}_{1+i}$ have the form $\begin{bmatrix}\answer{0}\\i\\\answer{1}\end{bmatrix}t$, so a possible basis for the eigenspace $\mathcal{S}_{1+i}$ is given by $\left\{\begin{bmatrix}\answer{0}\\i\\\answer{1}\end{bmatrix}\right\}$.
        \end{explanation}
        \end{example}
    
We conclude this section by establishing the significance of a matrix having an eigenvalue of zero.
    
\begin{theorem}\label{th:zero_ew}
A square matrix has an eigenvalue of zero if and only if it is singular.
\end{theorem}
    
\begin{proof}
A square matrix $A$ is singular if and only if $\det{A}=0$.  But $\det{A}=0$ if and only if $\det{A-0I}=0$, which is true if and only if zero is an eigenvalue of $A$.
\end{proof}

    

\end{document}