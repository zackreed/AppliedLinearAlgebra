\documentclass{ximera}
\input{../../../preamble.tex}

\author{Zack Reed}
%borrowed from selinger linear algebra
\title{Diagonalization: Symmetry}
\begin{document}
\begin{abstract}

\end{abstract}
\maketitle

\section*{Diagonalization and Real Symmetric Matrices}

We have seen that some matrices are diagonalizable, and others are not. In this section, we will see that
symmetric matrices have very nice properties related to diagonalization. 

\begin{remark}
  Recall that a matrix $A$ is
  \textbf{symmetric} if $A=A^T$. We begin with some observations
  about the eigenvalues and eigenvectors of a real symmetric matrix.
\end{remark}

\begin{proposition}\name{Eigenvalues and eigenvectors of real symmetric matrices}\label{prop:eigenvalues-symmetric}

  Let $A$ be a symmetric matrix with real entries. Then

    \begin{enumerate}
    \item All eigenvalues of $A$ are real (i.e., not complex).
    \item Eigenvectors for distinct eigenvalues of $A$ are orthogonal.
    \end{enumerate}

\end{proposition}

\begin{remark}
  We emphasize the ``real'' property of the matrix entries and the eigenvalues because we have observed that some matrices have ``complex'' eigenvalues. 
  
  The complex number system and complex vector spaces are very important for mathematics and science, but will not be heavily explored in this class. 

  As a brief summary of the idea, for further independent exploration, the complex numbers arise from a need to find roots of polynomials. If we just restrict ourselves to the real number system (the number system you've worked with for the vast majority of mathematics), even simple polynomial root equations such as $x^2+1=0$ have no solution. If we, however, define a new number $i$ to be $i=\sqrt{-1}$, then by default $i$ is the solution to the equation $x^2+1=0$.

  As it turns out, $i$ is the foundation for a whole new number system. Complex numbers are of the form $a+bi$, where $a$ and $b$ are real numbers. 

  These sometimes arise when finding eigenvalues and eigenvectors because of the connection between eigenvalues and the characteristic polynomial. 

  For instance, the characteristic polynomial of $A=\begin{bmatrix}
    0 & -1 \\ 1 & 0
  \end{bmatrix}$ is $\lambda^2+1$. Thus its eigenvalues are $\lambda=\pm i$. These complex eigenvalues can sometimes be harder to work with, so a nice feature of real symmetric matrices is that they are guaranteed to have real eigenvalues.
\end{remark}

\begin{definition}\name{Orthogonal diagonalizability}\label{def:orthogonal-diagonalizable}

  Recall that an $n\times n$-matrix $A$ is \textbf{diagonalizable} if there exists an invertible matrix $P$
  and a diagonal matrix $D$ such that $D = P^{-1}AP$. 

  We say that $A$ is \textbf{orthogonally diagonalizable} if $P$ can, moreover, be
chosen to be orthogonal. Orthogonal diagonalizability is a convenient
property, because when $P$ is orthogonal, then $P^{-1}=P^T$, and we
can interchangeably write $D = P^{-1}AP$ or $D = P^TAP$.  The
following is the main theorem about the diagonalization of real
symmetric matrices.
\end{definition}




\begin{theorem}\name{Diagonalization of real symmetric matrices}\label{th:diagonalization-symmetric}
  Every real symmetric matrix $A$ is orthogonally diagonalizable.
\end{theorem}

\begin{example}\name{Diagonalization of real symmetric matrices}\label{ex:diagonalization-symmetric}

  Orthogonally diagonalize the matrix
  \begin{equation*}
    A = \startmat{rr}
      3 & 2 \\
      2 & 6 \\
    \stopmat,
  \end{equation*}
  i.e., find an orthogonal matrix $P$ and a diagonal matrix $D$ such
  that $D = P^{-1}AP$.

  \textbf{Solution:}

  We proceed in much the same way as with prior diagonalization work,
  except that at a crucial moment, we ensure that $P$ is orthogonal.

  The following solution will first work with the characteristic polynomial, however you can feel free to expidite this process by using the MATLAB code below:

  \begin{verbatim}
    A = [3 2; 2 6];
    A=sym(A)
    [P,D] = eig(A)
    P
    D
  \end{verbatim}

  (Note: You will get a fraction in the second column of $P$, scale the column so that the fraction becomes $1$ to get the same answer as below.)

  We start by calculating the characteristic polynomial of $A$:
  \begin{equation*}
    \det(A-\eigenvar I)
    ~=~ \begin{array}{|ccc|}
      3-\eigenvar & 2 \\
      2 & 6-\eigenvar
    \end{array}
    ~=~ (3-\eigenvar)(6-\eigenvar) - 4
    ~=~ \eigenvar^2 - 9\eigenvar + 14.
  \end{equation*}
  We find the roots using the quadratic formula. The roots of the
  characteristic polynomial, and therefore the eigenvalues of $A$, are
  $\eigenvar=7$ and $\eigenvar=2$. For the eigenvalue $\eigenvar=2$, we find the eigenvector
  \begin{equation*}
    \vect{w} = \startmat{r} \answer{-2} \\ \answer{1} \stopmat,
  \end{equation*}
  and for the eigenvalue $\eigenvar=7$,
  we find the eigenvector
  \begin{equation*}
    \vect{v} = \startmat{r} \answer{1} \\ \answer{2} \stopmat.
  \end{equation*} 

  A quick matrix multiplication reveals that these vectors \wordChoice{\choice[correct]{are}\choice{are not}} orthogonal, and moreover reveals that the norm of both vectors is $\sqrt{5}$.
  
  So
  the vectors $\set{\vect{v},\vect{w}}$ form an orthogonal set. 
  
  We
  turn this into an orthonormal set by normalizing each eigenvector,
  i.e., the normalized eigenvectors are (NOTE the order of $\vec{v}$ and $\vec{w}$!):

  \begin{equation*}
    \vect{u}_1
    ~=~ \frac{1}{\norm{\vect{v}}}\vect{v}
    ~=~ \frac{1}{\answer{\sqrt{5}}} \startmat{r} 1 \\ 2 \stopmat
    \quad\mbox{and}\quad
    \vect{u}_2
    ~=~ \frac{1}{\norm{\vect{w}}}\vect{w}
    ~=~ \frac{1}{\answer{\sqrt{5}}} \startmat{r} -2 \\ 1 \stopmat.
  \end{equation*}
  We let $P$ be the matrix that has columns $\vect{u}_1$ and
  $\vect{u}_2$.
  
  Note that since $\vect{u}_1$ and $\vect{u}_2$ are orthonormal, $P$
  is automatically orthogonal. Moreover, we
  have
  \begin{equation*}
    P^{-1}
    ~=~ P^T
  \end{equation*}

  So, we confirm that $A$ is diagonalizable by the calculation
  \begin{equation*}
    P^{-1}AP
    ~=~ P^TAP
    ~=~ \startmat{rr} \answer{7} & 0 \\ 0 & \answer{2} \stopmat.
  \end{equation*}
\end{example}

\begin{example}\name{Diagonalization of real symmetric matrices}\label{ex:diagonalization-symmetric2}

  Find an orthogonal matrix $P$ and a diagonal matrix $D$ such that $D =
  P^{-1}AP$, where
  \begin{equation*}
    A = \startmat{rrr}
      3  & 1 & -2 \\
      1  & 3 &  2 \\
      -2 & 2 &  0 \\
    \stopmat.
  \end{equation*}

  \textbf{Solution:}

  Again, we start by calculating the characteristic polynomial and its roots, however you can feel free to expidite this process by using MATLAB. Note that MATLAB will list $P$ and $D$ in a different order than we do below.

  \begin{equation*}
    \det(A-\eigenvar I)
    ~=~ \begin{array}{|rrr|}
      3-\eigenvar  & 1 & -2 \\
      1  & 3-\eigenvar &  2 \\
      -2 & 2 &  -\eigenvar  \\
    \end{array}
    ~=~ - \eigenvar^3 + 6\eigenvar^2 -32.
  \end{equation*}
  The roots are $\eigenvar=4$ and $\eigenvar=-2$. To find the
  eigenvectors for $\eigenvar=4$, we solve the system of equations
  \begin{equation*}
    (A-4I)\vect{v}
    =
    \startmat{rrr}
      -1  & 1 & -2 \\
      1  & -1 &  2 \\
      -2 & 2 &  -4 \\
    \stopmat\vect{v}
    = \vect{0}.
  \end{equation*}

  The solution space is 2-dimensional, with basis
  \begin{equation*}
    \vect{v}_1 = \startmat{r} \answer{1} \\ \answer{1} \\ 0 \stopmat
    \quad\mbox{and}\quad
    \vect{v}_2 = \startmat{r} 0 \\ \answer{2} \\ \answer{1} \stopmat.
  \end{equation*}
  To find the eigenvectors for $\eigenvar=-2$, we solve the system of
  equations
  \begin{equation*}
    (A+2I)\vect{v}
    =
    \startmat{rrr}
      5  & 1 & -2 \\
      1  & 5 &  2 \\
      -2 & 2 &  2 \\
    \stopmat\vect{v}
    = \vect{0}.
  \end{equation*}
  The solution space is 1-dimensional, with basis
  \begin{equation*}
    \vect{v}_3 = \startmat{r} 1 \\ \answer{-1} \\ \answer{2} \stopmat.
  \end{equation*}

  As we would expect,
  $\vect{v}_3$ is orthogonal to $\vect{v}_1$ and
  $\vect{v}_2$. 
  
  However, there is a slight complication: $\vect{v}_1$
  and $\vect{v}_2$ are not orthogonal to each other. 
  
  However, we can fix
  this by normalizing $\vec{v}_1$ into $\vec{v}_1^{\prime}$, then projecting $\vec{v}_2$ onto $\vec{v}_1^{\prime}$ and creating a new vector $\vec{v}_2^{\prime}=\vec{v}_2-\vec{v}_1^\prime$, and normalizing $\vec{v}_2^\prime$. This orthogonalization process will be discussed more in Chapter 9, but for now, we will just give the results.

  Doing this yields the orthogonal basis
  $\set{\vect{u}_1,\vect{u}_2}$ for the eigenspace for $\eigenvar=4$,
  where
  \begin{equation*}
    \vect{u}_1 = \startmat{r} 1/\sqrt{2} \\ 1/\sqrt{2} \\ 0 \stopmat
    \quad\mbox{and}\quad
    \vect{u}_2 = \startmat{r} -1/\sqrt{3} \\ 1/\sqrt{3} \\ 1/\sqrt{3} \stopmat.
\end{equation*}
Normalizing $\vec{v}_3$, we have an orthonormal basis
$\set{\vect{v}_1^\prime,\vect{u}_2^\prime,\vect{v}_3^\prime}$ of eigenvectors of the
matrix $A$. This makes the matrix $P$:
\begin{equation*}
    \def\arraystretch{1.4}
    P = \startmat{ccc}
      \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{3}} & \frac{1}{\sqrt{6}}  \\
      \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{3}}  & -\frac{1}{\sqrt{6}} \\
      0                  & \frac{1}{\sqrt{3}}  & \frac{2}{\sqrt{6}}  \\
    \stopmat.
\end{equation*}
It seems that inverting $P$ will not be all that easy, but in fact,
since $P$ is orthogonal, the inverse is just the transpose:
\begin{equation*}
    \def\arraystretch{1.4}
    P^{-1}
    ~=~ P^T
    ~=~ \startmat{ccc}
      \frac{1}{\sqrt{2}}  & \frac{1}{\sqrt{2}}  & 0 \\
      -\frac{1}{\sqrt{3}} & \frac{1}{\sqrt{3}}  & \frac{1}{\sqrt{3}} \\
      \frac{1}{\sqrt{6}}  & -\frac{1}{\sqrt{6}} & \frac{2}{\sqrt{6}} \\
    \stopmat.
\end{equation*}
We have
\begin{equation*}
    P^{-1}AP
    ~=~ D
    ~=~ \startmat{ccc} 4 & 0 & 0 \\ 0 & 4 & 0 \\ 0 & 0 & -2 \stopmat.
\end{equation*}

\end{example}



\end{document}