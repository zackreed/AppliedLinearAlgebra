\documentclass{ximera}
\input{../../../preamble.tex}

\author{Zack Reed}
%borrowed from selinger linear algebra
\title{Diagonalization}
\begin{document}
\begin{abstract}

\end{abstract}
\maketitle

\section*{Diagonalization and Real Symmetric Matrices}

We have seen that some matrices are diagonalizable, and others are not. In this section, we will see that
symmetric matrices have very nice properties related to diagonalization. Recall that a matrix $A$ is
\textbf{symmetric} if $A=A^T$. We begin with some observations
about the eigenvalues and eigenvectors of a real symmetric matrix.

\begin{proposition}\name{Eigenvalues and eigenvectors of real symmetric matrices}\label{prop:eigenvalues-symmetric}
  Let $A$ be a symmetric matrix with real entries. Then

    \begin{enumerate}
    \item All eigenvalues of $A$ are real (i.e., not complex).
    \item Eigenvectors for distinct eigenvalues of $A$ are orthogonal.
    \end{enumerate}

\end{proposition}

\begin{remark}
  We emphasize the ``real'' property of the matrix entries and the eigenvalues because of an as-of-yet unexplored fact that some matrices have ``complex'' eigenvalues. The complex number system and complex vector spaces are very important for mathematics and science, but will not be heavily explored in this class. 

  As a brief summary of the idea, for further independent exploration, the complex numbers arise from a need to find roots of polynomials. If we just restrict ourselves to the real number system (the number system you've worked with for the vast majority of mathematics), even simple polynomial root equations such as $x^2+1=0$ have no solution. If we, however, define a new number $i$ to be $i=\sqrt{-1}$, then by default $i$ is the solution to the equation $x^2+1=0$.

  As it turns out, $i$ is the foundation for a whole new number system. Complex numbers are of the form $a+bi$, where $a$ and $b$ are real numbers. 

  These sometimes arise when finding eigenvalues and eigenvectors because of the connection between eigenvalues and the characteristic polynomial. 

  For instance, the characteristic polynomial of $A=\begin{bmatrix}
    0 & -1 \\ 1 & 0
  \end{bmatrix}$ is $\lambda^2+1$. Thus its eigenvalues are $\lambda=\pm i$. These complex eigenvalues can sometimes be harder to work with, so a nice feature of real symmetric matrices is that they are guaranteed to have real eigenvalues.
\end{remark}

Recall that an $n\times n$-matrix $A$ is \textbf{diagonalizable} if there exists an invertible matrix $P$
and a diagonal matrix $D$ such that $D = P^{-1}AP$. 

We say that $A$ is \textbf{orthogonally diagonalizable} if $P$ can, moreover, be
chosen to be orthogonal. Orthogonal diagonalizability is a convenient
property, because when $P$ is orthogonal, then $P^{-1}=P^T$, and we
can interchangeably write $D = P^{-1}AP$ or $D = P^TAP$.  The
following is the main theorem about the diagonalization of real
symmetric matrices.

\begin{theorem}\name{Diagonalization of real symmetric matrices}\label{th:diagonalization-symmetric}
  Every real symmetric matrix $A$ is orthogonally diagonalizable.
\end{theorem}

\begin{example}\name{Diagonalization of real symmetric matrices}\label{ex:diagonalization-symmetric}

  Orthogonally diagonalize the matrix
  \begin{equation*}
    A = \startmat{rr}
      3 & 2 \\
      2 & 6 \\
    \stopmat,
  \end{equation*}
  i.e., find an orthogonal matrix $P$ and a diagonal matrix $D$ such
  that $D = P^{-1}AP$.
\end{example}

\begin{solution}
  We proceed in much the same way as with prior diagonalization work,
  except that at a crucial moment, we ensure that $P$ is orthogonal.
  We start by calculating the characteristic polynomial of $A$:
  \begin{equation*}
    \det(A-\eigenvar I)
    ~=~ \begin{array}{|ccc|}
      3-\eigenvar & 2 \\
      2 & 6-\eigenvar
    \end{array}
    ~=~ (3-\eigenvar)(6-\eigenvar) - 4
    ~=~ \eigenvar^2 - 9\eigenvar + 14.
  \end{equation*}
  We find the roots using the quadratic formula. The roots of the
  characteristic polynomial, and therefore the eigenvalues of $A$, are
  $\eigenvar=7$ and $\eigenvar=2$. For the eigenvalue $\eigenvar=7$,
  we find the eigenvector
  \begin{equation*}
    \vect{v} = \startmat{r} 1 \\ 2 \stopmat,
  \end{equation*}
  and for the eigenvalue $\eigenvar=2$, we find the eigenvector
  \begin{equation*}
    \vect{w} = \startmat{r} -2 \\ 1 \stopmat.
  \end{equation*}
  We note that these two eigenvectors are orthogonal to each other,
  as we would expect. So
  the vectors $\set{\vect{v},\vect{w}}$ form an orthogonal set. We
  turn this into an orthonormal set by normalizing each eigenvector,
  i.e., the normalized eigenvectors are
  \begin{equation*}
    \vect{u}_1
    ~=~ \frac{1}{\norm{\vect{v}}}\vect{v}
    ~=~ \frac{1}{\sqrt{5}} \startmat{r} 1 \\ 2 \stopmat
    \quad\mbox{and}\quad
    \vect{u}_2
    ~=~ \frac{1}{\norm{\vect{w}}}\vect{w}
    ~=~ \frac{1}{\sqrt{5}} \startmat{r} -2 \\ 1 \stopmat.
  \end{equation*}
  We let $P$ be the matrix that has columns $\vect{u}_1$ and
  $\vect{u}_2$, i.e.,
  \begin{equation*}
    P = \frac{1}{\sqrt{5}} \startmat{rr} 1 & -2 \\ 2 & 1 \stopmat.
  \end{equation*}
  Note that since $\vect{u}_1$ and $\vect{u}_2$ are orthonormal, $P$
  is automatically orthogonal. Moreover, we
  have
  \begin{equation*}
    P^{-1}
    ~=~ P^T
    ~=~ \frac{1}{\sqrt{5}} \startmat{rr} 1 & 2 \\ -2 & 1 \stopmat.
  \end{equation*}
  Finally,
  \begin{equation*}
    P^{-1}AP
    ~=~ P^TAP
    ~=~ \frac{1}{5}
    \startmat{rr} 1 & 2 \\ -2 & 1 \stopmat
    \startmat{rr} 3 & 2 \\ 2 & 6 \stopmat
    \startmat{rr} 1 & -2 \\ 2 & 1 \stopmat
    ~=~ \startmat{rr} 7 & 0 \\ 0 & 2 \stopmat.
  \end{equation*}
\end{solution}

\begin{example}\name{Diagonalization of real symmetric matrices}\label{ex:diagonalization-symmetric2}
  Find an orthogonal matrix $P$ and a diagonal matrix $D$ such that $D =
  P^{-1}AP$, where
  \begin{equation*}
    A = \startmat{rrr}
      3  & 1 & -2 \\
      1  & 3 &  2 \\
      -2 & 2 &  0 \\
    \stopmat.
  \end{equation*}
\end{example}

\begin{solution}
  Again, we start by calculating the characteristic polynomial and its roots:
  \begin{equation*}
    \det(A-\eigenvar I)
    ~=~ \begin{array}{|rrr|}
      3-\eigenvar  & 1 & -2 \\
      1  & 3-\eigenvar &  2 \\
      -2 & 2 &  -\eigenvar  \\
    \end{array}
    ~=~ - \eigenvar^3 + 6\eigenvar^2 -32.
  \end{equation*}
  The roots are $\eigenvar=4$ and $\eigenvar=-2$. To find the
  eigenvectors for $\eigenvar=4$, we solve the system of equations
  \begin{equation*}
    (A-4I)\vect{v}
    =
    \startmat{rrr}
      -1  & 1 & -2 \\
      1  & -1 &  2 \\
      -2 & 2 &  -4 \\
    \stopmat\vect{v}
    = \vect{0}.
  \end{equation*}
  The solution space is 2-dimensional, with basis
  \begin{equation*}
    \vect{v}_1 = \startmat{r} 1 \\ 1 \\ 0 \stopmat
    \quad\mbox{and}\quad
    \vect{v}_2 = \startmat{r} 0 \\ 2 \\ 1 \stopmat.
  \end{equation*}
  To find the eigenvectors for $\eigenvar=-2$, we solve the system of
  equations
  \begin{equation*}
    (A+2I)\vect{v}
    =
    \startmat{rrr}
      5  & 1 & -2 \\
      1  & 5 &  2 \\
      -2 & 2 &  2 \\
    \stopmat\vect{v}
    = \vect{0}.
  \end{equation*}
  The solution space is 1-dimensional, with basis
  \begin{equation*}
    \vect{v}_3 = \startmat{r} 1 \\ -1 \\ 2 \stopmat.
  \end{equation*}

  As we would expect,
  $\vect{v}_3$ is orthogonal to $\vect{v}_1$ and
  $\vect{v}_2$. However, there is a slight complication: $\vect{v}_1$
  and $\vect{v}_2$ are not orthogonal to each other. This is because
  they are eigenvectors for the {\em same} eigenvalue ($\eigenvar=4$),
  not for {\em distinct} eigenvalues. We found basis
  $\set{\vect{v}_1,\vect{v}_2}$ for the eigenspace for $\eigenvar=4$,
  but it doesn't happen to be an orthogonal basis. 
  
  However, we can fix
  this by normalizing $\vec{v}_1$ into $\vec{v}_1^{\prime}$, then projecting $\vec{v}_2$ onto $\vec{v}_1^{\prime}$ and creating a new vector $\vec{v}_2^{\prime}=\vec{v}_2-\vec{v}_1^\prime$, and normalizing $\vec{v}_2^\prime$. This orthogonalization process will be discussed more in Chapter 9. 

  Doing this yields the orthogonal basis
  $\set{\vect{u}_1,\vect{u}_2}$ for the eigenspace for $\eigenvar=4$,
  where
  \begin{equation*}
    \vect{u}_1 = \startmat{r} 1/\sqrt{2} \\ 1/\sqrt{2} \\ 0 \stopmat
    \quad\mbox{and}\quad
    \vect{u}_2 = \startmat{r} -1/\sqrt{3} \\ 1/\sqrt{3} \\ 1/\sqrt{3} \stopmat.
\end{equation*}
Normalizing $\vec{v}_3$, we have an orthonormal basis
$\set{\vect{v}_1^\prime,\vect{u}_2^\prime,\vect{v}_3^\prime}$ of eigenvectors of the
matrix $A$. This makes the matrix $P$:
\begin{equation*}
    \def\arraystretch{1.4}
    P = \startmat{ccc}
      \frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{3}} & \frac{1}{\sqrt{6}}  \\
      \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{3}}  & -\frac{1}{\sqrt{6}} \\
      0                  & \frac{1}{\sqrt{3}}  & \frac{2}{\sqrt{6}}  \\
    \stopmat.
\end{equation*}
It seems that inverting $P$ will not be all that easy, but in fact,
since $P$ is orthogonal, the inverse is just the transpose:
\begin{equation*}
    \def\arraystretch{1.4}
    P^{-1}
    ~=~ P^T
    ~=~ \startmat{ccc}
      \frac{1}{\sqrt{2}}  & \frac{1}{\sqrt{2}}  & 0 \\
      -\frac{1}{\sqrt{3}} & \frac{1}{\sqrt{3}}  & \frac{1}{\sqrt{3}} \\
      \frac{1}{\sqrt{6}}  & -\frac{1}{\sqrt{6}} & \frac{2}{\sqrt{6}} \\
    \stopmat.
\end{equation*}
We have
\begin{equation*}
    P^{-1}AP
    ~=~ D
    ~=~ \startmat{ccc} 4 & 0 & 0 \\ 0 & 4 & 0 \\ 0 & 0 & -2 \stopmat.
\end{equation*}

\end{solution}



\end{document}