\documentclass{ximera}
\input{../../../preamble.tex}

\author{Zack Reed}
%borrowed from selinger linear algebra
\title{Gram-Schmidt}
\begin{document}
\begin{abstract}


\end{abstract}
\maketitle


\section{The Gram-Schmidt orthogonalization procedure}
\label{sec:gram-schmidt}

%\begin{outcome}
  \begin{enumerate}
  \item Use the Gram-Schmidt procedure to find an orthogonal basis
    of a subspace of an inner product space.
  \item Find an orthonormal basis of a subspace.
  \end{enumerate}
%\end{outcome}

Although we have already seen some potential uses for orthogonal
bases, we have not yet seen very many examples of such bases. In this
section, we will look at the Gram-Schmidt orthogonalization procedure,
a method for turning any basis into an orthogonal one.

The basic idea is very simple: if two vectors $\vect{v}_1,\vect{v}_2$
are not orthogonal, then we can make them orthogonal by replacing
$\vect{v}_2$ by a vector of the form $\vect{u}_2 = \vect{v}_2 -
t\vect{v}_1$, for a suitable parameter $t$.
\begin{center}
  \begin{tikzpicture}[scale=1.25]
    \draw[thick,red] (1.5,0) -- (1.5,2) -- (0,2);
    \draw[thick,blue,->] (0,0) -- node[left]{$\vect{v}_1$} (0,2.7);
    \draw[thick,blue,->] (0,0) -- node[left, pos=0.6]{$\vect{v}_2$} (1.5,2);
    \draw[thick,green!50!black,->] (0,0) -- node[below right, pos=0.3]{$\vect{u}_2 = \vect{v}_2 - t\vect{v}_1$} (1.5,0);
  \end{tikzpicture}
\end{center}
But what is the correct value of $t$? It turns out that this value is
uniquely determined by the requirement that $\vect{v}_1$ and
$\vect{u}_2$ must be orthogonal. We calculate
\begin{equation*}
  \iprod{\vect{v}_1,\vect{u}_2}
  ~=~ \iprod{\vect{v}_1,\vect{v}_2-t\vect{v}_1}
  ~=~ \iprod{\vect{v}_1,\vect{v}_2}-t\iprod{\vect{v}_1,\vect{v}_1}.
\end{equation*}
Setting this equal to $0$ yields the unique solution
\begin{equation*}
  t = \frac{\iprod{\vect{v}_1,\vect{v}_2}}{\iprod{\vect{v}_1,\vect{v}_1}}
\end{equation*}
Note that this is exactly the same thing as the Fourier coefficient of
$\vect{v}_2$ in the direction of $\vect{v}_1$. The following
proposition summarizes what we have found so far. For consistency with
our later notation, we also rename the first basis vector $\vect{v}_1$
to $\vect{u}_1$.

\begin{proposition}\name{Gram-Schmidt orthogonalization procedure for 2 vectors}\label{prop:gram-schmidt-2}
  Let $\set{\vect{v}_1,\vect{v}_2}$ be a basis for some subspace $W$
  of an inner product space $V$. Define vectors
  $\vect{u}_1,\vect{u}_2$ as follows:
  \begin{eqnarray*}
    \vect{u}_1 &=& \vect{v}_1, \\
    \vect{u}_2 &=& \vect{v}_2 ~-~ \frac{\iprod{\vect{u}_1,\vect{v}_2}}{\iprod{\vect{u}_1,\vect{u}_1}}\vect{u}_1.
  \end{eqnarray*}
  Then $\set{\vect{u}_1,\vect{u}_2}$ is an orthogonal basis of $W$.
\end{proposition}

\begin{example}\name{Gram-Schmidt orthogonalization procedure for 2 vectors}\label{ex:gram-schmidt-2}
  In $\R^3$ with the usual dot product, find an orthogonal basis for
  \begin{equation*}
    \sspan\set{
      \startmat{c} 1 \\ 1 \\ 0 \stopmat,~
      \startmat{c} 0 \\ 1 \\ 1 \stopmat
    }.
  \end{equation*}
\end{example}

\begin{solution}
  Let $\vect{v}_1 = \startmat{c} 1 \\ 1 \\ 0 \stopmat$
  and $\vect{v}_2 = \startmat{c} 0 \\ 1 \\ 1 \stopmat$.
  We calculate
  \begin{eqnarray*}
    \vect{u}_1
    &=& \vect{v}_1
        ~=~ \startmat{c} 1 \\ 1 \\ 0 \stopmat, \\
    \vect{u}_2
    &=& \vect{v}_2 ~-~ \frac{\iprod{\vect{u}_1,\vect{v}_2}}{\iprod{\vect{u}_1,\vect{u}_1}}\vect{u}_1
        ~=~ \startmat{c} 0 \\ 1 \\ 1 \stopmat
    ~-~ \frac{1}{2}\startmat{c} 1 \\ 1 \\ 0 \stopmat
    ~=~ \startmat{c} -1/2 \\ 1/2 \\ 1 \stopmat.
  \end{eqnarray*}
  Therefore the desired orthogonal basis is
  $\set{\startmat{c} 1 \\ 1 \\ 0 \stopmat,
    \startmat{c} -1/2 \\ 1/2 \\ 1 \stopmat}$.
\end{solution}

The procedure for finding an orthogonal basis of a $k$-dimensional
space is very similar. We adjust each basis vector $\vect{v}_i$ by
subtracting a suitable linear combination of previous orthogonal basis
vectors.

\begin{proposition}\name{Gram-Schmidt orthogonalization procedure for $k$ vectors}\label{prop:gram-schmidt-k}
  Let $\set{\vect{v}_1,\ldots,\vect{v}_k}$ be a basis for some subspace $W$
  of an inner product space $V$.%
  \index{Gram-Schmidt procedure}%
  \index{orthogonalization}%
  \index{orthogonal basis!Gram-Schmidt procedure}
  Define vectors
  $\vect{u}_1,\ldots,\vect{u}_k$ as follows:
  \begin{eqnarray*}
    \vect{u}_1
    &=& \vect{v}_1,
    \\
    \vect{u}_2
    &=& \vect{v}_2
        ~-~ \frac{\iprod{\vect{u}_1,\vect{v}_2}}{\iprod{\vect{u}_1,\vect{u}_1}}\vect{u}_1,
    \\
    \vect{u}_3
    &=& \vect{v}_3
        ~-~ \frac{\iprod{\vect{u}_1,\vect{v}_3}}{\iprod{\vect{u}_1,\vect{u}_1}}\vect{u}_1
        ~-~ \frac{\iprod{\vect{u}_2,\vect{v}_3}}{\iprod{\vect{u}_2,\vect{u}_2}}\vect{u}_2,
    \\
    &\vdots&
    \\
    \vect{u}_k
    &=& \vect{v}_k
        ~-~ \frac{\iprod{\vect{u}_1,\vect{v}_k}}{\iprod{\vect{u}_1,\vect{u}_1}}\vect{u}_1
        ~-~ \frac{\iprod{\vect{u}_2,\vect{v}_k}}{\iprod{\vect{u}_2,\vect{u}_2}}\vect{u}_2
        ~-~ \ldots
        ~-~ \frac{\iprod{\vect{u}_{k-1},\vect{v}_k}}{\iprod{\vect{u}_{k-1},\vect{u}_{k-1}}}\vect{u}_{k-1}.
  \end{eqnarray*}
  Then $\set{\vect{u}_1,\ldots,\vect{u}_k}$ is an orthogonal basis of $W$.
\end{proposition}

\begin{proof}
  First, it is clear that $\set{\vect{v}_1,\ldots,\vect{v}_k}$ and
  $\set{\vect{u}_1,\ldots,\vect{u}_k}$ span the same subspace, as each
  $\vect{v}_i$ is a linear combination of
  $\vect{u}_1,\ldots,\vect{u}_i$ and conversely, each $\vect{u}_i$ is
  a linear combination of $\vect{v}_1,\ldots,\vect{v}_i$. So the only
  thing we must check is that $\set{\vect{u}_1,\ldots,\vect{u}_k}$ is
  an orthogonal set. In other words, we must show that
  $\iprod{\vect{u}_j,\vect{u}_i}=0$ for all $j<i$. We prove this by
  induction on $i$, i.e., we assume it is already true for all pairs
  of indices smaller than $i$. To show
  $\iprod{\vect{u}_j,\vect{u}_i}=0$, we calculate:
  \begin{eqnarray*}
    \iprod{\vect{u}_j,\vect{u}_i}
    &=& \textstyle
        \iprod{\vect{u}_j, \vect{v}_i
        ~-~ \frac{\iprod{\vect{u}_1,\vect{v}_i}}{\iprod{\vect{u}_1,\vect{u}_1}}\vect{u}_1
        ~-~ \ldots
        ~-~ \frac{\iprod{\vect{u}_j,\vect{v}_i}}{\iprod{\vect{u}_j,\vect{u}_j}}\vect{u}_j
        ~-~ \ldots
        ~-~ \frac{\iprod{\vect{u}_{i-1},\vect{v}_i}}{\iprod{\vect{u}_{i-1},\vect{u}_{i-1}}}\vect{u}_{i-1}}
    \\
    &=& \textstyle
        \iprod{\vect{u}_j, \vect{v}_i}
        ~-~ \frac{\iprod{\vect{u}_1,\vect{v}_i}}{\iprod{\vect{u}_1,\vect{u}_1}}\iprod{\vect{u}_j,\vect{u}_1}
        ~-~ \ldots
        ~-~ \frac{\iprod{\vect{u}_j,\vect{v}_i}}{\iprod{\vect{u}_j,\vect{u}_j}}\iprod{\vect{u}_j,\vect{u}_j}
        ~-~ \ldots
        ~-~ \frac{\iprod{\vect{u}_{i-1},\vect{v}_i}}{\iprod{\vect{u}_{i-1},\vect{u}_{i-1}}}\iprod{\vect{u}_j,\vect{u}_{i-1}}
    \\
    &=& \textstyle
        \iprod{\vect{u}_j, \vect{v}_i}
        ~-~ 0
        ~-~ \ldots
        ~-~ \frac{\iprod{\vect{u}_j,\vect{v}_i}}{\iprod{\vect{u}_j,\vect{u}_j}}\iprod{\vect{u}_j,\vect{u}_j}
        ~-~ \ldots
        ~-~ 0
    \\
    &=& \iprod{\vect{u}_j, \vect{v}_i}
        ~-~ \iprod{\vect{u}_j,\vect{v}_i}
    \\
    &=& 0.
  \end{eqnarray*}
  It follows that the set $\set{\vect{u}_1,\ldots,\vect{u}_k}$ is
  orthogonal, as desired.
\end{proof}

\begin{example}\name{Gram-Schmidt orthogonalization procedure}\label{ex:gram-schmidt-r4}
  In $\R^4$, find an orthogonal basis for
  \begin{equation*}
    \sspan\set{
      \startmat{c} 1 \\ 1 \\ 1 \\ 1 \stopmat,~
      \startmat{c} 1 \\ 1 \\ 1 \\ 0 \stopmat,~
      \startmat{c} 1 \\ 1 \\ 0 \\ 0 \stopmat
    }.
  \end{equation*}
\end{example}

\begin{solution}
  Let
  \begin{equation*}
    \vect{v}_1 = \startmat{c} 1 \\ 1 \\ 1 \\ 1 \stopmat,\quad
    \vect{v}_2 = \startmat{c} 1 \\ 1 \\ 1 \\ 0 \stopmat,\quad\mbox{and}\quad
    \vect{v}_3 = \startmat{c} 1 \\ 1 \\ 0 \\ 0 \stopmat.
  \end{equation*}
  We calculate
  \begin{eqnarray*}
    \vect{u}_1
    &=& \vect{v}_1
        ~=~ \startmat{c} 1 \\ 1 \\ 1 \\ 1 \stopmat,
    \\
    \vect{u}_2
    &=& \vect{v}_2 ~-~ \frac{\iprod{\vect{u}_1,\vect{v}_2}}{\iprod{\vect{u}_1,\vect{u}_1}}\vect{u}_1
    ~=~ \startmat{c} 1 \\ 1 \\ 1 \\ 0 \stopmat
    ~-~ \frac{3}{4}\startmat{c} 1 \\ 1 \\ 1 \\ 1 \stopmat
    ~=~ \startmat{c} 1/4 \\ 1/4 \\ 1/4 \\ -3/4 \stopmat,
    \\
    \vect{u}_3
    &=& \vect{v}_3
        ~-~ \frac{\iprod{\vect{u}_1,\vect{v}_3}}{\iprod{\vect{u}_1,\vect{u}_1}}\vect{u}_1
        ~-~ \frac{\iprod{\vect{u}_2,\vect{v}_3}}{\iprod{\vect{u}_2,\vect{u}_2}}\vect{u}_2
    \\
    &=& \startmat{c} 1 \\ 1 \\ 0 \\ 0 \stopmat
    ~-~ \frac{2}{4}\startmat{c} 1 \\ 1 \\ 1 \\ 1 \stopmat
    ~-~ \frac{1/2}{3/4}\startmat{c} 1/4 \\ 1/4 \\ 1/4 \\ -3/4 \stopmat
    ~=~ \startmat{c} 1/3 \\ 1/3 \\ -2/3 \\ 0 \stopmat.
  \end{eqnarray*}
  Therefore the orthogonal basis is
  $\set{\vect{u}_1,\vect{u}_2,\vect{u}_3} = \set{
    \startmat{c} 1 \\ 1 \\ 1 \\ 1 \stopmat,~
    \startmat{c} 1/4 \\ 1/4 \\ 1/4 \\ -3/4 \stopmat,~
    \startmat{c} 1/3 \\ 1/3 \\ -2/3 \\ 0 \stopmat
  }$.
\end{solution}

The Gram-Schmidt procedure is sensitive to reordering the vectors. For
example, if we order the original basis vectors in
Example~\ref{exa:gram-schmidt-r4} in the opposite order, we end up
with a different orthogonal basis at the end. Sometimes this can
simplify the calculations, as the following example shows.

\begin{example}\name{Gram-Schmidt orthogonalization procedure: reordering the vectors}\label{ex:gram-schmidt-r4-b}
  In $\R^4$, find an orthogonal basis for
  \begin{equation*}
    \sspan\set{
      \startmat{c} 1 \\ 1 \\ 0 \\ 0 \stopmat,~
      \startmat{c} 1 \\ 1 \\ 1 \\ 0 \stopmat,~
      \startmat{c} 1 \\ 1 \\ 1 \\ 1 \stopmat
    }.
  \end{equation*}
\end{example}

\begin{solution}
  Note that these are the same basis vectors as in
  Example~\ref{exa:gram-schmidt-r4}, but listed in a different order.
  Let
  \begin{equation*}
    \vect{v}_1 = \startmat{c} 1 \\ 1 \\ 0 \\ 0 \stopmat,\quad
    \vect{v}_2 = \startmat{c} 1 \\ 1 \\ 1 \\ 0 \stopmat,\quad\mbox{and}\quad
    \vect{v}_3 = \startmat{c} 1 \\ 1 \\ 1 \\ 1 \stopmat.
  \end{equation*}
  We calculate
  \begin{eqnarray*}
    \vect{u}_1
    &=& \vect{v}_1
        ~=~ \startmat{c} 1 \\ 1 \\ 0 \\ 0 \stopmat,
    \\
    \vect{u}_2
    &=& \vect{v}_2 ~-~ \frac{\iprod{\vect{u}_1,\vect{v}_2}}{\iprod{\vect{u}_1,\vect{u}_1}}\vect{u}_1
    ~=~ \startmat{c} 1 \\ 1 \\ 1 \\ 0 \stopmat
    ~-~ \frac{2}{2}\startmat{c} 1 \\ 1 \\ 0 \\ 0 \stopmat
    ~=~ \startmat{c} 0 \\ 0 \\ 1 \\ 0 \stopmat,
    \\
    \vect{u}_3
    &=& \vect{v}_3
        ~-~ \frac{\iprod{\vect{u}_1,\vect{v}_3}}{\iprod{\vect{u}_1,\vect{u}_1}}\vect{u}_1
        ~-~ \frac{\iprod{\vect{u}_2,\vect{v}_3}}{\iprod{\vect{u}_2,\vect{u}_2}}\vect{u}_2
    \\
    &=& \startmat{c} 1 \\ 1 \\ 1 \\ 1 \stopmat
    ~-~ \frac{2}{2}\startmat{c} 1 \\ 1 \\ 0 \\ 0 \stopmat
    ~-~ \frac{1}{1}\startmat{c} 0 \\ 0 \\ 1 \\ 0 \stopmat
    ~=~ \startmat{c} 0 \\ 0 \\ 0 \\ 1 \stopmat.
  \end{eqnarray*}
  This time, we end up with the orthogonal basis
  $\set{\vect{u}_1,\vect{u}_2,\vect{u}_3} = \set{
    \startmat{c} 1 \\ 1 \\ 0 \\ 0 \stopmat,~
    \startmat{c} 0 \\ 0 \\ 1 \\ 0 \stopmat,~
    \startmat{c} 0 \\ 0 \\ 0 \\ 1 \stopmat
  }$.
\end{solution}

In the next example, we will consider $\R^n$, but with a non-standard
inner product.

\begin{example}\name{Gram-Schmidt orthogonalization procedure, non-standard inner product}\label{ex:gram-schmidt-non-standard}
  Let
  \begin{equation*}
    A = \startmat{ccc}
      1 & 2 & -2 \\
      2 & 6 & -1 \\
      -2 & -1 & 9 \\
    \stopmat,
  \end{equation*}
  and consider the vector space $\R^3$ with the inner product given by
  $\iprod{\vect{v},\vect{w}} = \vect{v}^T A\vect{w}$.
  Let
  \begin{equation*}
    \vect{v}_1 = \startmat{c} 1 \\ 0 \\ 0 \stopmat,
    \quad
    \vect{v}_2 = \startmat{c} 0 \\ 1 \\ 0 \stopmat,
    \quad\mbox{and}\quad
    \vect{v}_3 = \startmat{c} 0 \\ 0 \\ 1 \stopmat.
  \end{equation*}
  Apply the Gram-Schmidt procedure to
  $\vect{v}_1,\vect{v}_2,\vect{v}_3$ to find an orthogonal basis
  $\set{\vect{u}_1,\vect{u}_2,\vect{u}_3}$ for $\R^3$ with respect to
  the above inner product.
\end{example}

\begin{solution}
  As usual, we start with
  \begin{eqnarray*}
    \vect{u}_1 &=& \vect{v}_1 = \startmat{c} 1 \\ 0 \\ 0 \stopmat.
  \end{eqnarray*}
  Next, we calculate
  \begin{eqnarray*}
    \iprod{\vect{u}_1,\vect{v}_2} &=& \vect{u}_1^TA\vect{v}_2 = 2, \\
    \iprod{\vect{u}_1,\vect{u}_1} &=& \vect{u}_1^TA\vect{u}_1 = 1.
  \end{eqnarray*}
  Therefore,
  \begin{eqnarray*}
    \vect{u}_2
    &=& \vect{v}_2
        - \frac{\iprod{\vect{u}_1,\vect{v}_2}}{\iprod{\vect{u}_1,\vect{u}_1}} \vect{u}_1
        = \startmat{c} 0 \\ 1 \\ 0 \stopmat
    - \frac{2}{1} \startmat{c} 1 \\ 0 \\ 0 \stopmat
    = \startmat{c} -2 \\ 1 \\ 0 \stopmat.
  \end{eqnarray*}
  Finally, we calculate
  \begin{eqnarray*}
    \iprod{\vect{u}_1,\vect{v}_3} &=& \vect{u}_1^TA\vect{v}_3 = -2, \\
    \iprod{\vect{u}_2,\vect{v}_3} &=& \vect{u}_2^TA\vect{v}_3 = 3, \\
    \iprod{\vect{u}_2,\vect{u}_2} &=& \vect{u}_2^TA\vect{u}_2 = 2.
  \end{eqnarray*}
  Therefore,
  \begin{eqnarray*}
    \vect{u}_3
    &=& \vect{v}_3
        - \frac{\iprod{\vect{u}_1,\vect{v}_3}}{\iprod{\vect{u}_1,\vect{u}_1}} \vect{u}_1
        - \frac{\iprod{\vect{u}_2,\vect{v}_3}}{\iprod{\vect{u}_2,\vect{u}_2}} \vect{u}_2
        = \startmat{c} 0 \\ 0 \\ 1 \stopmat
    - \frac{-2}{1} \startmat{c} 1 \\ 0 \\ 0 \stopmat
    - \frac{3}{2} \startmat{c} -2 \\ 1 \\ 0 \stopmat
    = \startmat{c} 5 \\ -3/2 \\ 1 \stopmat.
  \end{eqnarray*}
  So the desired orthogonal basis is
  \begin{equation*}
    \set{\vect{u}_1,\vect{u}_2,\vect{u}_3}
    = \set{
      \startmat{c} 1 \\ 0 \\ 0 \stopmat,
      \startmat{c} -2 \\ 1 \\ 0 \stopmat,
      \startmat{c} 5 \\ -3/2 \\ 1 \stopmat
    }.
  \end{equation*}
  Note that it is not orthogonal with respect to the dot product, but
  with respect to the inner product defined above.
\end{solution}

The Gram-Schmidt procedure yields an {\em orthogonal} basis. If we
want to compute an {\em orthonormal} basis, we also have to normalize
each basis vector. Since normalization usually involves dividing by a
square root, it is best to do this at the end, i.e., after the entire
Gram-Schmidt procedure is complete, rather than normalizing each
$\vect{u}_i$ immediately after it is found. Note that the Gram-Schmidt
procedure itself does not involve computing any square roots.

\begin{example}\name{Finding an orthonormal basis}\label{ex:finding-orthonormal-basis-r4}
  In $\R^4$, find an orthogonal basis for
  \begin{equation*}
    \sspan\set{
      \startmat{c} 1 \\ 1 \\ 1 \\ 1 \stopmat,~
      \startmat{c} 1 \\ 1 \\ 1 \\ 0 \stopmat,~
      \startmat{c} 1 \\ 1 \\ 0 \\ 0 \stopmat
    }.
  \end{equation*}
\end{example}

\begin{solution}
  In Example~\ref{exa:gram-schmidt-r4}, we already found an orthogonal basis
  \begin{equation*}
    \set{\vect{u}_1,\vect{u}_2,\vect{u}_3} ~=~
    \set{
      \startmat{c} 1 \\ 1 \\ 1 \\ 1 \stopmat,~
      \startmat{c} 1/4 \\ 1/4 \\ 1/4 \\ -3/4 \stopmat,~
      \startmat{c} 1/3 \\ 1/3 \\ -2/3 \\ 0 \stopmat
    }
  \end{equation*}
  for this space. So all that is left to do is to normalize each
  vector. The orthonormal basis is
  \begin{equation*}
    \set{
      \frac{\vect{u}_1}{\norm{\vect{u}_1}},
      \frac{\vect{u}_2}{\norm{\vect{u}_2}},
      \frac{\vect{u}_3}{\norm{\vect{u}_3}}
    }
    ~=~
    \set{
      \frac{1}{2}\startmat{c} 1 \\ 1 \\ 1 \\ 1 \stopmat,~
      \frac{2}{\sqrt{3}}\startmat{c} 1/4 \\ 1/4 \\ 1/4 \\ -3/4 \stopmat,~
      \frac{\sqrt{3}}{\sqrt{2}}\startmat{c} 1/3 \\ 1/3 \\ -2/3 \\ 0 \stopmat
    }.
  \end{equation*}
  Alternatively, we could have also normalized the orthogonal basis we
  found in Example~\ref{exa:gram-schmidt-r4-b}. In that case, we
  obtain the orthonormal basis
  \begin{equation*}
    \set{
      \frac{1}{\sqrt{2}}
      \startmat{c} 1 \\ 1 \\ 0 \\ 0 \stopmat,~
      \startmat{c} 0 \\ 0 \\ 1 \\ 0 \stopmat,~
      \startmat{c} 0 \\ 0 \\ 0 \\ 1 \stopmat
    }.
  \end{equation*}
\end{solution}

We finish this section by remarking that the formula
\begin{equation*}
  \frac{\iprod{\vect{u},\vect{v}}}{\iprod{\vect{u},\vect{u}}}\vect{u}
\end{equation*}
is exactly what we called the \textbf{projection of $\vect{v}$ onto
  $\vect{u}$}%
\index{vector!projection of}%
\index{projection!in inner product space}%
\index{projection!vector to vector} in Section~\ref{ssec:projections},
except that we have generalized this concept from $\R^n$ to an
arbitrary inner product space. We can define
\begin{equation*}
  \proj_{\vect{u}}(\vect{v})
  = \frac{\iprod{\vect{u},\vect{v}}}{\iprod{\vect{u},\vect{u}}}\vect{u}.
\end{equation*}
With this definition, the Gram-Schmidt procedure can also be expressed
more succinctly as follows.
\begin{equation*}
  \begin{array}{rcl}
    \vect{u}_1
    &=& \vect{v}_1,
    \\
    \vect{u}_2
    &=& \vect{v}_2
        ~-~ \proj_{\vect{u}_1}(\vect{v}_2),
    \\
    \vect{u}_3
    &=& \vect{v}_3
        ~-~ \proj_{\vect{u}_1}(\vect{v}_3)
        ~-~ \proj_{\vect{u}_2}(\vect{v}_3),
    \\
    &\vdots&
    \\
    \vect{u}_k
    &=& \vect{v}_k
        ~-~ \proj_{\vect{u}_1}(\vect{v}_k)
        ~-~ \proj_{\vect{u}_2}(\vect{v}_k)
        ~-~ \ldots
        ~-~ \proj_{\vect{u}_{k-1}}(\vect{v}_k).
  \end{array}
\end{equation*}

Finally, let's use this to compute the SVD of a matrix that is slightly more complicated than the examples from previous chapters.

\begin{example}\label{ex:SVD3x1}
Find an SVD for
$A=\left[\begin{array}{r} -1 \\ 2\\ 2 \end{array}\right]$.
 
\begin{explanation}
Since $A$ is $3\times 1$, $A^T A$ is a $1\times 1$ matrix
whose eigenvalues are easier to find than the eigenvalues of
the $3\times 3$ matrix $AA^T$.
 
\[ A^TA=\left[\begin{array}{ccc} -1 & 2 & 2 \end{array}\right]
\left[\begin{array}{r} -1 \\ 2 \\ 2 \end{array}\right]
=\left[\begin{array}{r} 9 \end{array}\right]\]
Thus $A^TA$ has eigenvalue $\lambda_1=9$, and
the eigenvalues of $AA^T$ are $\lambda_1=9$, $\lambda_2=0$, and
$\lambda_3=0$.
Furthermore, $A$ has only one singular value, $S_1=3$.
 
\textbf{To find the matrix $V$}:
To do so we find an eigenvector for $A^TA$ and
normalize it.
In this case, finding a unit eigenvector is trivial:
$\vec{v}_1=\left[\begin{array}{r} 1 \end{array}\right]$, and
\[ V=\left[\begin{array}{r} 1 \end{array}\right]\]
Also,
$S =\left[\begin{array}{r} 3 \\ 0\\ 0 \end{array}\right]$,
and we use $A$, $V^T$, and $S$ to find $U$.
 
\noindent Now $AV=US$, with
$V=\left[\begin{array}{r}\vec{v}_1 \end{array}\right]$,
and $U=\left[\begin{array}{rrr} \vec{u}_1 & \vec{u}_2 & \vec{u}_3 \end{array}\right]$,
where $\vec{u}_1$, $\vec{u}_2$, and $\vec{u}_3$ are the columns of $U$.
Thus
\begin{eqnarray*}
A\left[\begin{array}{r} \vec{v}_1 \end{array}\right]
&=& \left[\begin{array}{rrr} \vec{u}_1 & \vec{u}_2 & \vec{u}_3 \end{array}\right]S\\
\left[\begin{array}{r} A\vec{v}_1 \end{array}\right]
&=& \left[\begin{array}{r} S_1 \vec{u}_1+0\vec{u}_2+0\vec{u}_3 \end{array}\right]\\
&=& \left[\begin{array}{r} S_1 \vec{u}_1 \end{array}\right]
\end{eqnarray*}
This gives us $A\vec{v}_1=S_1 \vec{u}_1= 3\vec{u}_1$, so
 
\[ \vec{u}_1 = \frac{1}{3}A\vec{v}_1
= \frac{1}{3}
\left[\begin{array}{r} -1 \\ 2 \\ 2 \end{array}\right]
\left[\begin{array}{r} 1 \end{array}\right]
= \frac{1}{3}
\left[\begin{array}{r} -1 \\ 2 \\ 2 \end{array}\right]\]
The vectors $\vec{u}_2$ and $\vec{u}_3$ are eigenvectors of $AA^T$ corresponding
to the eigenvalue $\lambda_2=\lambda_3=0$.
Instead of solving the system $(0I-AA^T)\vec{x}= 0$ and then using the
Gram-Schmidt process on the resulting set of
two basic eigenvectors, the following approach may be used.
 
 
Find vectors $\vec{u}_2$ and $\vec{u}_3$ by first extending $\{ \vec{u}_1\}$ to a basis of
$\RR^3$, then using the Gram-Schmidt algorithm to orthogonalize the basis,
and finally normalizing the vectors.
 
Starting with $\{ 3\vec{u}_1 \}$ instead of $\{ \vec{u}_1 \}$ makes the
arithmetic a bit easier.
It is easy to verify that
 
\[ \left\{ \left[\begin{array}{r} -1 \\ 2 \\ 2 \end{array}\right],
\left[\begin{array}{r} 1 \\ 0 \\ 0 \end{array}\right],
\left[\begin{array}{r} 0 \\ 1 \\ 0 \end{array}\right]\right\}\]
is a basis of $\RR^3$.  Set
 
\[ \vec{x}_1 = \left[\begin{array}{r} -1 \\ 2 \\ 2 \end{array}\right],
\vec{x}_2 = \left[\begin{array}{r} 1 \\ 0 \\ 0 \end{array}\right],
\vec{x}_3 =\left[\begin{array}{r} 0 \\ 1 \\ 0 \end{array}\right]\]
and apply the Gram-Schmidt algorithm to
$\{ \vec{x}_1, \vec{x}_2, \vec{x}_3\}$. This gives us
 
\[ \vec{f}_1 = \left[\begin{array}{r} -1 \\ 2 \\ 2 \end{array}\right], \vec{f}_2 = \left[\begin{array}{r} 4 \\ 1 \\ 1 \end{array}\right]
\mbox{ and }
\vec{f}_3 = \left[\begin{array}{r} 0 \\ 1 \\ -1 \end{array}\right]\]
Therefore,
\[ \vec{u}_2 = \frac{1}{\sqrt{18}}
 \left[\begin{array}{r} 4 \\ 1 \\ 1 \end{array}\right],
\vec{u}_3 = \frac{1}{\sqrt 2}
\left[\begin{array}{r} 0 \\ 1 \\ -1 \end{array}\right]\]
and
\[ U = \left[  \begin{array}{rrr} -\frac{1}{3} & \frac{4}{\sqrt{18}} & 0 \\
\frac{2}{3} & \frac{1}{\sqrt{18}} & \frac{1}{\sqrt 2} \\
\frac{2}{3} & \frac{1}{\sqrt{18}} & -\frac{1}{\sqrt 2} \end{array}\right]\]
Finally,
\[ A =
\left[\begin{array}{r} -1 \\ 2 \\ 2 \end{array}\right]
=
\left[ \begin{array}{rrr} -\frac{1}{3} & \frac{4}{\sqrt{18}} & 0 \\
\frac{2}{3} & \frac{1}{\sqrt{18}} & \frac{1}{\sqrt 2} \\
\frac{2}{3} & \frac{1}{\sqrt{18}} & -\frac{1}{\sqrt 2} \end{array}\right]
\left[\begin{array}{r} 3 \\ 0 \\ 0 \end{array}\right]
\left[\begin{array}{r} 1 \end{array}\right]\]
 
\end{explanation}
\end{example}


\end{document}