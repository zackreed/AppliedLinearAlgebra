\documentclass{ximera}
\input{../../../preamble.tex}

\author{Zack Reed}
%borrowed from anna davis
\title{Useful Factorizations} \license{CC BY-NC-SA 4.0}
\begin{document}
\begin{abstract}

\end{abstract}
\maketitle
 

\section*{$LU$ factorization}

 
\begin{exploration}\label{init:LUfactorization}
Consider the equation:
$$\begin{bmatrix}1&0&0\\3&1&0\\-2&2&1\end{bmatrix}\begin{bmatrix}2&-1&2\\0&4&-1\\0&0&3\end{bmatrix}\vec{x}=\begin{bmatrix}1\\8\\5\end{bmatrix}$$
This equation is unusual in that it involves two matrices on the left-hand side.  If we multiply the matrices together, we get
$$\begin{bmatrix}2&-1&2\\6&1&5\\-4&10&-3\end{bmatrix}\vec{x}=\begin{bmatrix}1\\8\\5\end{bmatrix}$$
Gaussian elimination yields:
$$\left[\begin{array}{ccc|c}
2&-1&2&1\\6&1&5&8\\-4&10&-3&5
\end{array}\right]
\quad\rightsquigarrow\quad
\left[\begin{array}{ccc|c}
 1&0&0&2\\0&1&0&1\\0&0&1&-1
\end{array}\right]
$$
We conclude that
$$\vec{x}=\begin{bmatrix}2\\1\\-1\end{bmatrix}.$$
 
Now that we know the answer, we will turn to the original question and consider the advantages of the format of the original problem.  Observe that the two matrices have a distinct pattern.  The matrix on the left has zeros above the main diagonal, while the matrix on the right has zeros below the main diagonal.  Matrices that follow such a pattern are called \dfn{lower triangular} and \dfn{upper triangular} matrices, respectively.
 
Let $\vec{y}=\begin{bmatrix}2&-1&2\\0&4&-1\\0&0&3\end{bmatrix}\vec{x}$.  Then we can write the original equation as
$$\begin{bmatrix}1&0&0\\3&1&0\\-2&2&1\end{bmatrix}\left(\begin{bmatrix}2&-1&2\\0&4&-1\\0&0&3\end{bmatrix}\vec{x}\right)=\begin{bmatrix}1&0&0\\3&1&0\\-2&2&1\end{bmatrix}\vec{y}=\begin{bmatrix}1\\8\\5\end{bmatrix}$$
The equation
$$\begin{bmatrix}1&0&0\\3&1&0\\-2&2&1\end{bmatrix}\vec{y}=\begin{bmatrix}1&0&0\\3&1&0\\-2&2&1\end{bmatrix}\begin{bmatrix}y_1\\y_2\\y_3\end{bmatrix}=\begin{bmatrix}1\\8\\5\end{bmatrix}$$
corresponds to the system
$$\begin{matrix}
     y_1&&&=&1\\
       3y_1 & +y_2&&= &8 \\
      -2y_1&+2y_2&+y_3&=&5
    \end{matrix}$$
     
This system can be easily solved by substitution, giving us
$$y_1=1$$
$$3(1)+y_2=8 \rightsquigarrow y_2=5$$
$$-2(1)+2(5)+y_3=5 \rightsquigarrow y_3=-3$$
So,
$$\vec{y}=\begin{bmatrix}y_1\\y_2\\y_3\end{bmatrix}=\begin{bmatrix}1\\5\\-3\end{bmatrix}$$
     
Recall that we let $\vec{y}=\begin{bmatrix}2&-1&2\\0&4&-1\\0&0&3\end{bmatrix}\vec{x}$. 
The equation
$$\begin{bmatrix}2&-1&2\\0&4&-1\\0&0&3\end{bmatrix}\vec{x}=\begin{bmatrix}1\\5\\-3\end{bmatrix}$$
corresponds to the system
$$\begin{matrix}
     2x_1& -&x_2&+&2x_3&=&1\\
       & &4x_2&-&x_3&= &5 \\
      & &&&3x_3&=&-3
    \end{matrix}$$
    which can be easily solved by back-substitution:
    $$x_3=\answer{-1}$$
    $$x_2=\answer{1}$$
    $$x_1=\answer{2}$$
     
    Thus, $\vec{x}=\begin{bmatrix}2\\1\\-1\end{bmatrix}$.  Observe that this is the same answer that we obtained in the beginning of the problem using Gaussian elimination. 
\end{exploration}

 
Given a matrix equation such as
$$\begin{bmatrix}2&-1&2\\6&1&5\\-4&10&-3\end{bmatrix}\vec{x}=\begin{bmatrix}1\\8\\5\end{bmatrix}$$
of Exploration \ref{init:LUfactorization}, often in practice we express the matrix on the left as a product of an upper triangular matrix $U$ and a lower triangular matrix $L$ and use substitution to solve the equation instead of using Gaussian elimination to find the solution.  In Exploration \ref{init:LUfactorization},
 
$$\begin{bmatrix}2&-1&2\\6&1&5\\-4&10&-3\end{bmatrix}=LU=\begin{bmatrix}1&0&0\\3&1&0\\-2&2&1\end{bmatrix}\begin{bmatrix}2&-1&2\\0&4&-1\\0&0&3\end{bmatrix}$$
 
The process of taking a matrix $A$ and expressing it as a product $A=LU$ of a lower triangular matrix $L$ and an upper triangular matrix $U$ is called \dfn{$LU$ factorization}.
%An $LU$ factorization of a matrix $A$ is when we express the given matrix as the
%product
%$$A=LU$$
%where  $L$ is a lower triangular square matrix and $U$ is an upper triangular matrix. 
We also adopt the (common) convention that $L$ is a matrix with a diagonal consisting entirely of $1$'s, which is often called a \dfn{lower unit triangular} matrix.
 
Not every matrix has an $LU$ factorization, but we will see that we can correct for that.
 
\begin{example}\label{ex:usingLU}
Consider the system
$$\begin{array}{ccccccccc}
        & &12x_2&-&18x_3&= &6 \\
     x_1 &+ &2x_2&-&3x_3&= &1\\
      x_1&- &2x_2&+&x_3&=&1
    \end{array}.$$
We can express this system as the matrix equation $A\vec{x}=\vec{b}$, and we get $$\begin{bmatrix}0&12&-18\\1&2&-3\\1&-2&1\end{bmatrix}\vec{x}=\begin{bmatrix}6\\1\\1\end{bmatrix}.$$



 
Unfortunately, for reasons we will explain below, it is not possible to find an $LU$ factorization of this coefficient matrix.  However, if we simply swap the first two equations:
$$\begin{array}{ccccccccc}
     x_1 &+ &2x_2&-&3x_3&= &1\\
      & &12x_2&-&18x_3&= &6 \\
      x_1&- &2x_2&+&x_3&=&1
    \end{array},$$
we are able to find an $LU$ factorization of the coefficient matrix.
\[
\begin{bmatrix}
1 & 2 & -3 \\
0 & 12 & -18 \\
1 & -2 & 1\end{bmatrix}
=
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
1  & -1/3  & 1
\end{bmatrix}
\begin{bmatrix}
1 & 2 & -3 \\
0 & 12  & -18 \\
0 & 0 & -2
\end{bmatrix}
\]
(How we actually come up with an  $LU$ factorization of $A$ will be discussed after this example.)
 
To understand how an $LU$ factorization can be used, it is helpful to think of this system of equations as a matrix equation.
$$A\vec{x}=\vec{b}$$
$$(LU)\vec{x}=\vec{b}$$
$$L(U\vec{x})=\vec{b}$$
So if we let $\vec{y}=U\vec{x}$ and also write $\vec{y}=\begin{bmatrix} y_1 \\ y_2 \\ y_3 \end{bmatrix}$, then we are able to solve for $ y_1, y_2, \text{ and } y_3$ using \emph{forward-substitution}.
$$L\vec{y}=\vec{b}$$
\[
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
1  & -1/3  & 1
\end{bmatrix}
\begin{bmatrix} y_1 \\ y_2 \\ y_3 \end{bmatrix}
=
\begin{bmatrix} 1 \\ 6 \\ 1 \end{bmatrix}
\]
See if you can do it!
$$y_1=\answer{1},\quad y_2=\answer{6},\quad y_3=\answer{2}$$
Once we have the values of $\vec{y}$, since $\vec{y}=U\vec{x}$, all that remains is to solve the following matrix equation using back-substitution.
$$U\vec{x}=\vec{y}$$
 

$$\begin{bmatrix}
1 & 2 & -3 \\
0 & 12  & -18 \\
0 & 0 & -2
\end{bmatrix}
\begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix}
=
\begin{bmatrix} 1 \\ 6 \\ 2 \end{bmatrix}$$
 

 
By now you are quite used to solving this kind of problem...
$$x_1=\answer{0},\quad x_2=\answer{-1},\quad x_3=\answer{-1}$$

\end{example}



 
 
\subsection*{Finding an $LU$ factorization by Inspection}
 
\begin{example}\label{ex:LU1}
Find an $LU$ factorization of $A=
\begin{bmatrix}
1 & 2 & 0 & 2 \\
1 & 3 & 2 & 1 \\
2 & 3 & 4 & 0
\end{bmatrix}
.$
\end{example}
 
One way to find the $LU$ factorization%
 is to simply look for it directly.
We need
\[
\begin{bmatrix}
1 & 2 & 0 & 2 \\
1 & 3 & 2 & 1 \\
2 & 3 & 4 & 0
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 & 0 \\
x & 1 & 0 \\
y & z & 1
\end{bmatrix}
\begin{bmatrix}
a & d & h & j \\
0 & b & e & i \\
0 & 0 & c & f
\end{bmatrix}
\]
By multiplying the latter matrices we get
\[
\begin{bmatrix}
a & d & h & j \\
xa & xd+b & xh+e & xj+i \\
ya & yd+zb & yh+ze+c & yj+iz+f
\end{bmatrix}
\]
and from this, it is a simple exercise to determine each of the unknowns. For example, from the first
column, we get $a=1$ and then $x=1,y=2.$
 
% Now go to the second column. You need $
% d=2,xd+b=3$ so $b=1,yd+zb=3$ so $z=-1.$ From the third column, $h=0,e=2,c=6.$
% Now from the fourth column, $j=2,i=-1,f=-5.$
 
See if you can continue to determine the entire $LU$
factorization of $A$.
\[
\begin{bmatrix}
1 & 2 & 0 & 2 \\
1 & 3 & 2 & 1 \\
2 & 3 & 4 & 0
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 & 0 \\
1 & 1 & 0 \\
2 & \answer{-1} & 1
\end{bmatrix}
\begin{bmatrix}
1 & \answer{2} & \answer{0} & \answer{2} \\
0 & \answer{1} & \answer{2} & \answer{-1} \\
0 & 0 & \answer{6} & \answer{-5}
\end{bmatrix}
\]



\section*{QR Factorization}

    
One of the main virtues of orthogonal
matrices is that they can be easily inverted---the transpose is the
inverse. This fact, combined with the factorization theorem in this
section, provides a useful way to simplify many matrix calculations (for
    example, in \href{https://ximera.osu.edu/appliedlinearalgebra/c8ChapterEight/learningActivities/m8LearningActivities/svdRevisited/leastSquaresCurves}{least squares approximation}).
    
    
\begin{definition}\label{def:QR-factorization}
Let $A$ be an $m \times n$ matrix with independent columns. A \dfn{QR-factorization} of $A$ expresses it as $A = QR$ where $Q$ is $m \times n$ with orthonormal columns and $R$ is an invertible and upper triangular matrix with positive diagonal entries.
\end{definition}
    
The importance of the factorization
lies in the fact that there are computer algorithms that accomplish it
with good control over round-off error, making it particularly useful in
matrix calculations. The factorization is a matrix version of the Gram-Schmidt process.
    
    
Suppose $A = \left[ \begin{array}{cccc}
|&|& &| \\
\vec{c}_{1} & \vec{c}_{2} & \cdots &  \vec{c}_{n}\\
|&|& &|
\end{array}\right]$ is an $m \times n$ matrix with linearly independent columns $\vec{c}_{1}, \vec{c}_{2}, \dots, \vec{c}_{n}$. The Gram-Schmidt algorithm can be applied to these columns to provide orthogonal columns $\vec{f}_{1}, \vec{f}_{2}, \dots, \vec{f}_{n}$ where $\vec{f}_{1} = \vec{c}_{1}$ and
\begin{equation*}
\vec{f}_{k} = \vec{c}_{k} - \frac{\vec{c}_{k} \dotp \vec{f}_{1}}{\norm{ \vec{f}_{1} }^2}\vec{f}_{1} + \frac{\vec{c}_{k} \dotp \vec{f}_{2}}{\norm{ \vec{f}_{2} }^2}\vec{f}_{2} - \dots - \frac{\vec{c}_{k} \dotp \vec{f}_{k-1}}{\norm{ \vec{f}_{k-1} }^2}\vec{f}_{k-1}
\end{equation*}
for each $k = 2, 3, \dots, n$. Now write $\vec{q}_{k} = \frac{1}{\norm{ \vec{f}_{k} }}\vec{f}_{k}$ for each $k$. Then $\vec{q}_{1}, \vec{q}_{2}, \dots, \vec{q}_{n}$ are orthonormal columns, and the above equation becomes
\begin{equation*}
\norm{ \vec{f}_{k} } \vec{q}_{k} = \vec{c}_{k} - (\vec{c}_{k} \dotp \vec{q}_{1})\vec{q}_{1} - (\vec{c}_{k} \dotp \vec{q}_{2})\vec{q}_{2} - \dots - (\vec{c}_{k} \dotp \vec{q}_{k-1})\vec{q}_{k-1}
\end{equation*}
Using these equations, express each $\vec{c}_{k}$ as a linear combination of the $\vec{q}_{i}$:
\begin{equation*}
\begin{array}{ccl}
\vec{c}_{1} &=& \norm{ \vec{f}_{1} } \vec{q}_{1} \\
\vec{c}_{2} &=& (\vec{c}_{2} \dotp \vec{q}_{1})\vec{q}_{1} + \norm{ \vec{f}_{2} } \vec{q}_{2} \\
\vec{c}_{3} &=& (\vec{c}_{3} \dotp \vec{q}_{1})\vec{q}_{1} + (\vec{c}_{3} \dotp \vec{q}_{2})\vec{q}_{2} + \norm{ \vec{f}_{3} } \vec{q}_{3} \\
\vdots && \vdots \\
\vec{c}_{n} &=& (\vec{c}_{n} \dotp \vec{q}_{1})\vec{q}_{1} + (\vec{c}_{n} \dotp \vec{q}_{2})\vec{q}_{2} + (\vec{c}_{n} \dotp \vec{q}_{3})\vec{q}_{3} + \dots + \norm{ \vec{f}_{n} } \vec{q}_{n}
\end{array}
\end{equation*}
These equations have a matrix form that gives the required factorization:
\begin{align}
A & = \left[ \begin{array}{ccccc}
|&|&|& &| \\
\vec{c}_{1} & \vec{c}_{2} & \vec{c}_{3} &\cdots &  \vec{c}_{n}\\
|&|&|& &|
\end{array}\right] \nonumber \\
&= \left[ \begin{array}{ccccc}
|&|&|& &| \\
\vec{q}_{1} & \vec{q}_{2} & \vec{q}_{3} & \cdots &  \vec{q}_{n}\\
|&|&|& &|
\end{array}\right] \left[ \begin{array}{ccccc}
\norm{ \vec{f}_{1} } & \vec{c}_{2} \dotp \vec{q}_{1} & \vec{c}_{3} \dotp \vec{q}_{1} & \cdots & \vec{c}_{n} \dotp \vec{q}_{1} \\
0 & \norm{ \vec{f}_{2} } & \vec{c}_{3} \dotp \vec{q}_{2} & \cdots & \vec{c}_{n} \dotp \vec{q}_{2} \\
0 & 0 & \norm{ \vec{f}_{3} } & \cdots & \vec{c}_{n} \dotp \vec{q}_{3} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & \norm{ \vec{f}_{n} }
\end{array} \right] \label{matrixFactEq}
\end{align}
Here the first factor $Q = \left[ \begin{array}{ccccc}
|&|&|& &| \\
\vec{q}_{1} & \vec{q}_{2} & \vec{q}_{3} & \cdots &  \vec{q}_{n}\\
|&|&|& &|
\end{array}\right]$ has orthonormal columns, and the second factor is an $n \times n$ upper triangular matrix $R$ with positive diagonal entries (and so is invertible). We record this in the following theorem.


\begin{theorem}[QR-Factorization]\label{th:QR-025133}
Every $m \times n$ matrix $A$ with linearly independent columns has a QR-factorization $A = QR$ where $Q$ has orthonormal columns and $R$ is upper triangular with positive diagonal entries.
\end{theorem}
    
The matrices $Q$ and $R$ in Theorem~\ref{th:QR-025133} are uniquely determined by $A$; we return to this below.
    
\begin{example}\label{ex:QR4x3-025139}
Find the QR-factorization of $A = \left[ \begin{array}{rrr}
1 & 1 & 0 \\
-1 & 0 & 1 \\
0 & 1 & 1 \\
0 & 0 & 1
\end{array}\right]$.
    
\begin{explanation}
    Denote the columns of $A$ as $\vec{c}_{1}$, $\vec{c}_{2}$, and $\vec{c}_{3}$, and observe that $\{\vec{c}_{1}, \vec{c}_{2}, \vec{c}_{3}\}$ is independent. If we apply the Gram-Schmidt algorithm to these columns, the result is:
\begin{equation*}
\vec{f}_{1} = \vec{c}_{1} = \left[ \begin{array}{r}
1  \\
-1  \\
0  \\
0
\end{array}\right], \quad \vec{f}_{2} = \vec{c}_{2} - \frac{1}{2}\vec{f}_{1} = \left[ \def\arraystretch{1.2} \begin{array}{r}
\frac{1}{2}  \\
\frac{1}{2}  \\
1  \\
0
\end{array}\right], \mbox{ and } \quad \vec{f}_{3} = \vec{c}_{3} + \frac{1}{2}\vec{f}_{1} - \vec{f}_{2} = \left[ \begin{array}{r}
0  \\
0  \\
0  \\
1
\end{array}\right].
\end{equation*}
Write $\vec{q}_{j} = \frac{1}{\norm{ \vec{f}_{j} }^2}\vec{f}_{j}$
    for each $j$, so $\{\vec{q}_{1}, \vec{q}_{2}, \vec{q}_{3}\}$ is orthonormal. Then we have $A = QR$ where
\begin{align*}
Q &= \left[ \begin{array}{ccc} | & | & | \\
\vec{q}_{1} & \vec{q}_{2} & \vec{q}_{3} \\
| & | & |
\end{array}\right] = \left[ \def\arraystretch{1.3}\begin{array}{ccc}
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{6}} & 0 \\
\frac{-1}{\sqrt{2}} & \frac{1}{\sqrt{6}} & 0 \\
0 & \frac{2}{\sqrt{6}} & 0 \\
0 & 0 & 1
\end{array}\right]  = \frac{1}{\sqrt{6}} \left[ \begin{array}{ccc}
\sqrt{3} & 1 & 0 \\
-\sqrt{3} & 1 & 0 \\
0 & 2 & 0 \\
0 & 0 & \sqrt{6}
\end{array} \right]
\\
R &= \left[ \begin{array}{ccc}
\norm{ \vec{f}_{1} } & \vec{c}_{2} \dotp \vec{q}_{1} & \vec{c}_{3} \dotp \vec{q}_{1} \\
0 & \norm{ \vec{f}_{2} } & \vec{c}_{3} \dotp \vec{q}_{2} \\
0 & 0 & \norm{ \vec{f}_{3} } \\
\end{array} \right] = \left[ \def\arraystretch{1.5} \begin{array}{ccc}
\sqrt{2} & \frac{1}{\sqrt{2}} & \frac{-1}{\sqrt{2}} \\
0 & \frac{\sqrt{3}}{\sqrt{2}} & \frac{\sqrt{3}}{\sqrt{2}} \\
0 & 0 & 1
\end{array} \right] = \frac{1}{\sqrt{2}}\left[ \begin{array}{ccc}
2 & 1 & -1 \\
0 & \sqrt{3} & \sqrt{3} \\
0 & 0 & \sqrt{2}
\end{array} \right]
\end{align*}
    
The reader can verify that indeed $A = QR$.
\end{explanation}
\end{example}
    
If a matrix $A$ has independent rows and we apply QR-factorization to $A^{T}$, the result is:
    
\begin{corollary}\label{cor:QR-transpose-025162}
If $A$ has independent rows, then $A$ factors uniquely as $A = LP$ where $P$ has orthonormal rows and $L$ is an invertible lower triangular matrix with positive main diagonal entries.
\end{corollary}
    
Since a square matrix with orthonormal columns is orthogonal, we have
    
\begin{theorem}\label{th:025166}
Every square invertible matrix $A$ has factorizations $A = QR$ and $A = LP$ where $Q$ and $P$ are orthogonal, $R$ is upper triangular with positive diagonal entries, and $L$ is lower triangular with positive diagonal entries.
\end{theorem}
    
We now take the time to prove the uniqueness of the QR-factorization.
    
\begin{theorem}\label{th:QR-unique-025187}
Let $A$ be an $m \times n$ matrix with independent columns. If $A = QR$ and $A = Q_{1}R_{1}$ are QR-factorizations of $A$, then $Q_{1} = Q$ and $R_{1} = R$.
\end{theorem}
    
\begin{proof}
Write $Q = \left[ \begin{array}{cccc}
|&|& &| \\
\vec{c}_{1} & \vec{c}_{2} & \cdots &  \vec{c}_{n}\\
|&|& &|
\end{array}\right]$  and $Q_{1} =  \left[ \begin{array}{cccc}
|&|& &| \\
\vec{d}_{1} & \vec{d}_{2} & \cdots &  \vec{d}_{n}\\
|&|& &|
\end{array}\right]$ in terms of their columns, and observe first that $Q^TQ = I_{n} = Q_{1}^TQ_{1}$ because $Q$ and $Q_{1}$ have orthonormal columns. Hence it suffices to show that $Q_{1} = Q$ (then $R_{1} = Q_{1}^TA = Q^TA = R$). Since $Q_{1}^TQ_{1} = I_{n}$, the equation $QR = Q_{1}R_{1}$ gives $Q_{1}^TQ = R_{1}R^{-1}$; for convenience we write this matrix as
\begin{equation*}
Q_{1}^TQ = R_{1}R^{-1} = \left[ \begin{array}{c} t_{ij} \end{array}\right]
\end{equation*}
This matrix is upper triangular with positive diagonal elements (since this is true for $R$ and $R_{1}$), so $t_{ii} > 0$ for each $i$ and $t_{ij} = 0$ if $i > j$. On the other hand, the $(i, j)$-entry of $Q_{1}^TQ$ is $\vec{d}_{i}^T\vec{c}_{j} = \vec{d}_{i} \dotp \vec{c}_{j}$, so we have $\vec{d}_{i} \dotp \vec{c}_{j} = t_{ij}$ for all $i$ and $j$. But each $\vec{c}_{j}$ is in $\mbox{span}\{\vec{d}_{1}, \vec{d}_{2}, \dots, \vec{d}_{n}\}$ because $Q = Q_{1}(R_{1}R^{-1})$. We know how to write a vector as a linear combination of an orthonormal basis:
\begin{equation*}
\vec{c}_{j} = (\vec{d}_{1} \dotp \vec{c}_{j})\vec{d}_{1} + (\vec{d}_{2} \dotp \vec{c}_{j})\vec{d}_{2} + \dots + (\vec{d}_{n} \dotp \vec{c}_{j})\vec{d}_{n} = t_{1j}\vec{d}_{1} + t_{2j}\vec{d}_{2} + \dots + t_{jj}\vec{d}_{i},
\end{equation*}
because $\vec{d}_{i} \dotp \vec{c}_{j} = t_{ij} = 0$ if $i > j$. The first few equations here are
\begin{equation*}
\begin{array}{ccl}
\vec{c}_{1} &=& t_{11}\vec{d}_{1} \\
\vec{c}_{2} &=& t_{12}\vec{d}_{1} + t_{22}\vec{d}_{2} \\
\vec{c}_{3} &=& t_{13}\vec{d}_{1} + t_{23}\vec{d}_{2} + t_{33}\vec{d}_{3} \\
\vec{c}_{4} &=& t_{14}\vec{d}_{1} + t_{24}\vec{d}_{2} + t_{34}\vec{d}_{3} + t_{44}\vec{d}_{4} \\
\vdots && \vdots
\end{array}
\end{equation*}
The first of these equations gives $1 = \norm{ \vec{c}_{1} } = \norm{ t_{11}\vec{d}_{1} } = | t_{11} | \norm{ \vec{d}_{1} } = t_{11}$, whence $\vec{c}_{1} = \vec{d}_{1}$. But then we have $t_{12} = \vec{d}_{1} \dotp \vec{c}_{2} = \vec{c}_{1} \dotp \vec{c}_{2} = 0$, so the second equation becomes $\vec{c}_{2} = t_{22}\vec{d}_{2}$. Now a similar argument gives $\vec{c}_{2} = \vec{d}_{2}$, and then $t_{13} = 0$ and $t_{23} = 0$ follows in the same way. Hence $\vec{c}_{3} = t_{33}\vec{d}_{3}$ and $\vec{c}_{3} = \vec{d}_{3}$. Continue in this way to get $\vec{c}_{i} = \vec{d}_{i}$ for all $i$. This proves that $Q_{1} = Q$.
\end{proof}

    
\section*{QR-Algorithm for approximating eigenvalues}\label{sec:QRalgorithm}
    
There are various ways that algorithms efficiently approximate eigenvalues, one such is the QR-algorithm. The interested reader is referred to
    J. M. Wilkinson, \textit{The Algebraic Eigenvalue Problem} (Oxford, England: Oxford University Press, 1965)\index{\textit{The Algebraic Eigenvalue Problem} (Wilkinson)} or G. W. Stewart, \textit{Introduction to Matrix Computations} (New York: Academic Press, 1973)\index{\textit{Introduction to Matrix Computations} (Stewart)}.
    
The \dfn{QR-algorithm} uses QR-factorization repeatedly to create a sequence of matrices $A_{1} =A, A_{2}, A_{3}, \dots,$ as follows:
    
\begin{enumerate}
\item Define $A_{1} = A$ and factor it as $A_{1} = Q_{1}R_{1}$.
    
\item Define $A_{2} = R_{1}Q_{1}$ and factor it as $A_{2} = Q_{2}R_{2}$.
    
\item Define $A_{3} = R_{2}Q_{2}$ and factor it as $A_{3} = Q_{3}R_{3}$.
\\ \hspace*{4em} $\vdots$
    
    
\end{enumerate}
    
In general, $A_{k}$ is factored as $A_{k} = Q_{k}R_{k}$ and we define $A_{k + 1} = R_{k}Q_{k}$. Then $A_{k + 1}$ is similar to $A_{k}$ [in fact, $A_{k+1} = R_{k}Q_{k} = (Q_{k}^{-1}A_{k})Q_{k}$], and hence each $A_{k}$ has the same eigenvalues as $A$. If the eigenvalues of $A$ are real and have distinct absolute values, the remarkable thing is that the sequence of matrices $A_{1}, A_{2}, A_{3}, \dots$ converges to an upper triangular matrix with these eigenvalues on the main diagonal. [See below for the case of complex eigenvalues.]
    
\begin{example}\label{QR-algortihm-2x2-025425}
If $A = \left[ \begin{array}{rr}
1 & 1 \\
2 & 0
\end{array}\right]$ as in Example~\ref{ex:2x2PowerMethod025326}, use the QR-algorithm to approximate the eigenvalues.
    
\begin{explanation}
    The matrices $A_{1}$, $A_{2}$, and $A_{3}$ are as follows:
\begin{align*}
A_{1} = & \left[ \begin{array}{rr}
1 & 1 \\
2 & 0
\end{array}\right] = Q_{1}R_{1} \quad \mbox{ where } Q_{1} = \frac{1}{\sqrt{5}}\left[ \begin{array}{rr}
1 & 2 \\
2 & -1
\end{array}\right] \mbox{ and } R_{1} =  \frac{1}{\sqrt{5}}\left[ \begin{array}{rr}
5 & 1 \\
0 & 2
\end{array}\right] \\
A_{2} = & \frac{1}{5}\left[ \begin{array}{rr}
7 & 9 \\
4 & -2
\end{array}\right] = \left[ \begin{array}{rr}
1.4 & -1.8 \\
-0.8 & -0.4
\end{array}\right]= Q_{2}R_{2} \\
&\mbox{ where } Q_{2} = \frac{1}{\sqrt{65}}\left[ \begin{array}{rr}
7 & 4 \\
4 & -7
\end{array}\right] \mbox{ and } R_{2} =  \frac{1}{\sqrt{65}}\left[ \begin{array}{rr}
13 & 11 \\
0 & 10
\end{array}\right] \\
A_{3} = &\frac{1}{13}\left[ \begin{array}{rr}
27 & -5 \\
8 & -14
\end{array}\right] = \left[ \begin{array}{rr}
2.08 & -0.38 \\
0.62 & -1.08
\end{array}\right]
\end{align*}
This is converging to $\left[ \begin{array}{rr}
2 & \ast \\
0 & -1
\end{array}\right]$ and so is approximating the eigenvalues $2$ and $-1$ on the main diagonal.
\end{explanation}
\end{example}
    
    
\subsubsection*{Shifting.} Convergence is accelerated if, at stage $k$ of the algorithm, a number $\tau_{k}$ is chosen and $A_{k} - \tau_{k}I$ is factored in the form $Q_{k}R_{k}$ rather than $A_{k}$ itself. Then
\begin{equation*}
Q_{k}^{-1}A_{k}Q_{k} = Q_{k}^{-1}(Q_{k}R_{k} + \tau_{k}I)Q_{k} = R_{k}Q_{k} + \tau_{k}I
\end{equation*}
so we take $A_{k+1} = R_{k}Q_{k} + \tau_{k}I$. If the shifts $\tau_{k}$ are carefully chosen, convergence can be greatly improved.
    
\subsubsection*{Preliminary Preparation.} A matrix such as
\begin{equation*}
\left[ \begin{array}{rrrrr}
\ast  & \ast & \ast & \ast & \ast  \\
\ast  & \ast & \ast & \ast & \ast  \\
0  & \ast & \ast & \ast & \ast  \\
0  & 0 & \ast & \ast & \ast  \\
0  & 0 & 0 & \ast & \ast  \\
\end{array}\right]
\end{equation*}
is said to be in \dfn{upper Hessenberg} form, and the QR-factorizations of such matrices are greatly simplified. Given an $n \times n$ matrix $A$, a series of orthogonal matrices $H_{1}, H_{2}, \dots, H_{m}$ (called \dfn{Householder matrices} can be easily constructed such that
\begin{equation*}
B = H_{m}^T \cdots H_{1}^TAH_{1} \cdots H_{m}
\end{equation*}
is in upper Hessenberg form. Then the QR-algorithm can be efficiently applied to $B$ and, because $B$ is similar to $A$, it produces the eigenvalues of $A$.
    
\subsubsection*{Complex Eigenvalues.} If some of the eigenvalues of a real matrix $A$ are not real, the QR-algorithm converges to a block upper triangular matrix where the diagonal blocks are either $1 \times 1$ (the real eigenvalues) or $2 \times 2$ (each providing a pair of conjugate complex eigenvalues of $A$).

\end{document}